---
title: "Why should I trust you \U0001F916?"
author: ZERO
date: '2018-03-26'
slug: why-should-i-trust-you
categories:
  - machine learning
tags:
  - LIME
thumbnailImagePosition: left
thumbnailImage: https://i.loli.net/2018/02/11/5a7fa56fd7333.jpg
metaAlignment: center
coverMeta: out
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,#show code and output
	message = FALSE,
	warning = FALSE
)
```
ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ä¸»è¦é›†ä¸­åœ¨æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–ä¸Š; æœ€å¥½çš„æ¨¡å‹é€šå¸¸æ˜¯é€šè¿‡åƒç²¾åº¦æˆ–è€…é”™è¯¯è¿™æ ·çš„æ€§èƒ½åº¦é‡æ¥é€‰æ‹©çš„ï¼Œè€Œä¸”å¦‚æœå®ƒé€šè¿‡äº†è¿™äº›æ€§èƒ½æ ‡å‡†çš„æŸäº›é˜ˆå€¼ï¼Œæˆ‘ä»¬å€¾å‘äºå‡è®¾ä¸€ä¸ªæ¨¡å‹æ˜¯è¶³å¤Ÿå¥½çš„ã€‚åœ¨æœºå™¨å­¦ä¹ çš„è®¸å¤šåº”ç”¨ä¸­ï¼Œç”¨æˆ·ä¼šä½¿ç”¨ä¸€ä¸ªæ¨¡å‹æ¥å¸®åŠ©åšå†³å®šï¼Œ ä¾‹å¦‚ï¼šä¸€ä½åŒ»ç”Ÿä¸ä¼šå¯¹ç—…äººè¿›è¡Œæ‰‹æœ¯ï¼Œä»…ä»…å› ä¸ºè¿™ä¸ªæ¨¡å‹è¯´ä¸åº”è¯¥è¿›è¡Œæ‰‹æœ¯ï¼Ÿ 

ç”±äºå¤æ‚çš„æœºå™¨å­¦ä¹ æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯é»‘ç›’å­ï¼Œè€Œä¸”å¤ªå¤æ‚ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹åšå‡ºçš„åˆ†ç±»å†³å®šé€šå¸¸å¾ˆéš¾è¢«äººç±»çš„å¤§è„‘æ‰€ç†è§£ï¼Œä½†æ˜¯èƒ½å¤Ÿç†è§£å’Œè§£é‡Šè¿™äº›æ¨¡å‹å¯¹äºæé«˜æ¨¡å‹è´¨é‡ï¼Œæé«˜ä¿¡ä»»åº¦å’Œé€æ˜åº¦ä»¥åŠå‡å°‘åè§éå¸¸é‡è¦ï¼Œå› ä¸ºäººç±»å¾€å¾€å…·æœ‰è‰¯å¥½çš„ç›´è§‰å’Œå› æœæ¨ç†ï¼Œè¿™äº›éƒ½æ˜¯éš¾ä»¥åœ¨æ•°æ®è¯„ä¼°æŒ‡æ ‡ä¸­æ•è·ã€‚  

å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå½¢è±¡çš„ç†è§£å®ƒä»¬çš„å·¥ä½œåŸç†ï¼šä¸ºä»€ä¹ˆä¸€ä¸ªæ¨¡å‹å°†å…·æœ‰ç‰¹å®šæ ‡ç­¾çš„æ¡ˆä¾‹è¿›è¡Œå‡†ç¡®åˆ†ç±»ã€‚eg: ä¸ºä»€ä¹ˆä¸€ä¸ªä¹³è…ºè‚¿å—æ ·æœ¬è¢«å½’ç±»ä¸ºâ€œæ¶æ€§â€è€Œä¸æ˜¯â€œè‰¯æ€§ï¼Œä»…ä»…å› ä¸ºå®ƒé•¿å¾—ä¸‘å—ï¼Ÿ

> [Local Interpretable Model-Agnostic Explanations (LIME)](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime) is an attempt to make these complex models at least partly understandable. The method has been published in [â€œWhy Should I Trust You?â€](https://arxiv.org/pdf/1602.04938.pdf) 
Explaining the Predictions of Any Classifier. By Marco Tulio Ribeiro, Sameer Singh and Carlos Guestrin from the University of Washington in Seattle


##**How LIME works**

>lime is able to explain all models for which we can obtain prediction probabilities (in R, that is every model that works with predict(type = "prob")). It makes use of the fact that linear models are easy to explain because they are based on linear relationships between features and class labels: The complex model function is approximated by locally fitting linear models to permutations of the original training set.On each permutation, a linear model is being fit and weights are given so that incorrect classification of instances that are more similar to the original data are penalized (positive weights support a decision, negative weights contradict them). This will give an approximation of how much (and in which way) each feature contributed to a decision made by the model

```{r out.width="30%", fig.margin=TRUE, echo=FALSE, eval=TRUE}
knitr::include_graphics("https://i.loli.net/2018/02/11/5a7fa56fd7333.jpg")
```


>We take the image on the left and divide it into interpretable components

```{r out.width="30%", fig.margin=TRUE, echo=FALSE, eval=TRUE}
knitr::include_graphics("https://i.loli.net/2018/02/11/5a7faba0d302a.jpg")
```

>we then generate a data set of perturbed instances by turning some of the interpretable components "oï¬€". For each perturbed instance, we get the probability that a tree frog is in the image according to the model.

```{r out.width="30%", fig.margin=TRUE, echo=FALSE, eval=TRUE}
knitr::include_graphics("https://i.loli.net/2018/02/11/5a7fac3ca8787.jpg")
```


>We then learn a simple (linear) model on this data set, which is locally weightedâ€”that is, we care more about making mistakes in perturbed instances that are more similar to the original image

```{r out.width="30%", fig.margin=TRUE, echo=FALSE, eval=TRUE}
knitr::include_graphics("https://i.loli.net/2018/02/11/5a7fac8382a86.jpg")
```

>the end, we present the superpixels with highest positive weights as an explanation, graying out everything else

```{r out.width="30%", fig.margin=TRUE, echo=FALSE, eval=TRUE}
knitr::include_graphics("https://i.loli.net/2018/02/11/5a7faf16f0320.jpg")
```

ğŸ¤–é¢„æµ‹è¿™å¼ å›¾æ˜¯ä¸ªæ ‘è›™æ˜¯å› ä¸ºè¿™ä¸ªéƒ¨åˆ†,æ‰€ä»¥ğŸ¤–é¢„æµ‹ç»“æœæ˜¯æ¯”è¾ƒå¯ä¿¡çš„ã€‚
```{r echo=FALSE, fig.align = "center"}
knitr::include_graphics("https://i.loli.net/2018/02/11/5a7faebabe34e.jpg")
```

ğŸ¤–é¢„æµ‹è¿™å¼ å›¾æ˜¯å°çƒæ˜¯æ ¹æ®è¿™äº›éƒ¨åˆ†ï¼Œæ‰€ä»¥ğŸ¤–é¢„æµ‹ç»“æœæ˜¯ä¸å¯ä¿¡çš„ã€‚
```{r echo=FALSE, fig.align = "center"}
knitr::include_graphics("https://i.loli.net/2018/02/11/5a7fb61ecb060.jpg")
```

##**Example in R**
###01.Prepare the breast cancer data

This **data** of example comes from the book of **R in action**
```{r}
loc <- "http://archive.ics.uci.edu/ml/machine-learning-databases/"
ds  <- "breast-cancer-wisconsin/breast-cancer-wisconsin.data"
url <- paste(loc, ds, sep="")

breast <- read.table(url, sep=",", header=FALSE, na.strings="?")
names(breast) <- c("ID", "clumpThickness", "sizeUniformity",
                   "shapeUniformity", "maginalAdhesion", 
                   "singleEpithelialCellSize", "bareNuclei", 
                   "blandChromatin", "normalNucleoli", "mitosis", "class")

df <- breast[-1]
df$class <- factor(df$class, levels=c(2,4), 
                   labels=c("benign", "malignant"))

set.seed(1234)
train <- sample(nrow(df), 0.7*nrow(df))
df.train <- df[train,]
df.validate <- df[-train,]
table(df.train$class)
table(df.validate$class)
```

###02 Create decision tree model
```{r}
library(rpart)
set.seed(1234)
dtree <- rpart(class ~ ., data=df.train, method="class",      
               parms=list(split="information"))
dtree$cptable
plotcp(dtree)

dtree.pruned <- prune(dtree, cp=.0125)
```

###03 predict
```{r}
dtree.pred <- predict(dtree.pruned, df.validate, type="class")
(dtree.perf <- table(df.validate$class, dtree.pred, 
                    dnn=c("Actual", "Predicted")))
```

###04 plot decision tree
```{r echo=TRUE}
#plot01
library(rpart.plot)
prp(dtree.pruned, type = 2, extra = 104,  
    fallen.leaves = TRUE, main="Decision Tree")
#plot02
library(partykit)
library(dplyr)
dtree.pruned %>% as.party() %>% plot()
```

###LIME: why should I trust you ğŸ¤–?
```{r}
library(lime)
explainer <- lime(df.train, model = dtree.pruned)

# Explain new observation
#[model_type](https://github.com/thomasp85/lime/blob/master/R/models.R)
model_type.rpart <- function(x, ...) 'classification'#defined model_type method
test.data <- 
  df.validate %>% 
  dplyr::select(-class) %>% 
  head(3)
explanation <- lime::explain(test.data, explainer, n_labels = 1, n_features = 2)

# The output is provided in a consistent tabular format and includes the
# output from the model.
head(explanation)

# And can be visualised directly
plot_features(explanation)
```


##**Links**
- [CKD data set](http://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease)
- [open-source Python code for LIME](https://github.com/marcotcr/lime)
- [R package for LIME](https://github.com/thomasp85/lime)
