<feed xmlns="http://www.w3.org/2005/Atom">
  <title>student zero </title>
  <link href="/index.xml" rel="self"/>
  <link href="/"/>
  <updated>2018-04-03T00:00:00+00:00</updated>
  <id>/</id>
  <author>
    <name>Jixing Liu</name>
  </author>
  <generator>Hugo -- gohugo.io</generator>
  <entry>
    <title type="html"><![CDATA[Run Python From R]]></title>
    <link href="/2018/04/run-python-from-r/"/>
    <id>/2018/04/run-python-from-r/</id>
    <published>2018-04-03T00:00:00+00:00</published>
    <updated>2018-04-03T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>R</strong> is mainly known for data analysis, statistical modeling and visualization. While <strong>python</strong> is popular for deep learning and natural language processing.</p>
<p>Python and R were ranked top 2 tools for data science and machine learning. If you really want to boost your career in data science world, these are the languages you need to focus on.</p>
<div id="how-to-call-or-run-python-from-r" class="section level1">
<h1><strong>How To Call Or Run Python From R?</strong></h1>
<p>RStudio developed a package called <strong>reticulate</strong> which provides a medium to run Python packages and functions from R.</p>
<pre class="r"><code># Load reticulate package
if (!require(&quot;reticulate&quot;)) install.packages(&quot;reticulate&quot;)</code></pre>
</div>
<div id="python-version-configuration" class="section level1">
<h1><a href="https://rstudio.github.io/reticulate/articles/versions.html">Python Version Configuration</a></h1>
<p>If the version of Python you want to use is located on the system PATH then it will be automatically discovered (via Sys.which) and used.</p>
<p>Alternatively, you can use one of the following functions to specify alternate versions of Python:</p>
<p>Function Description</p>
<ul>
<li><p><a href="https://rstudio.github.io/reticulate/reference/use_python.html">use_python</a> Specify the path a specific Python binary.</p></li>
<li><p><a href="https://rstudio.github.io/reticulate/reference/use_python.html">use_virtualenv</a> Specify the directory containing a Python virtualenv.</p></li>
<li><p><a href="https://rstudio.github.io/reticulate/reference/use_python.html">use_condaenv</a> Specify the name of a Conda environment.</p></li>
</ul>
<pre class="r"><code>py_discover_config()</code></pre>
<pre><code>## python:         /usr/bin/python
## libpython:      /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/config/libpython2.7.dylib
## pythonhome:     /System/Library/Frameworks/Python.framework/Versions/2.7:/System/Library/Frameworks/Python.framework/Versions/2.7
## version:        2.7.10 (default, Feb  7 2017, 00:08:15)  [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)]
## numpy:          /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy
## numpy_version:  1.8.1
## 
## python versions found: 
##  /usr/bin/python
##  /usr/local/bin/python3
##  /Users/zero/anaconda/bin/python
##  /Users/zero/anaconda3/bin/python
##  /Users/zero/anaconda3/envs/circRNA/bin/python
##  /Users/zero/anaconda3/envs/lifeifei/bin/python
##  /Users/zero/anaconda3/envs/PDSH/bin/python
##  /Users/zero/anaconda3/envs/py27/bin/python
##  /Users/zero/anaconda3/envs/RvsPython/bin/python
##  /Users/zero/anaconda3/envs/tfdeeplearning/bin/python
##  /Users/zero/miniconda2/bin/python
##  /Users/zero/miniconda3/bin/python</code></pre>
<pre class="r"><code>use_python(&quot;/Users/zero/anaconda3/envs/PDSH/bin/python&quot;)
#use_virtualenv(&quot;/Users/zero/anaconda3/envs&quot;, required = TRUE)
use_condaenv(&quot;PDSH&quot;)</code></pre>
</div>
<div id="import-a-python-module-within-r" class="section level1">
<h1>Import a python module within R</h1>
<pre class="r"><code>os &lt;- import(&quot;os&quot;)
pd &lt;- import(&quot;pandas&quot;)
numpy &lt;- import(&quot;numpy&quot;)
py_module_available(&quot;pandas&quot;)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>os$getcwd()</code></pre>
<pre><code>## [1] &quot;/Users/zero/Documents/_14_tmp/jixingBlogdown/content/post&quot;</code></pre>
</div>
<div id="install-python-package-with-conda-in-terminal" class="section level1">
<h1>Install Python package with conda in terminal</h1>
</div>
<div id="working-with-numpy" class="section level1">
<h1>working with <strong>numpy</strong></h1>
<pre class="r"><code>y &lt;- array(1:4, c(2, 2)) # create a matrix with R
x &lt;- numpy$array(y) # edit it with python
numpy$transpose(y) # transpose the above array</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1    2
## [2,]    3    4</code></pre>
<pre class="r"><code>numpy$linalg$eig(y) # Eigenvalues and eigen vectors</code></pre>
<pre><code>## [[1]]
## [1] -0.3722813  5.3722813
## 
## [[2]]
##            [,1]       [,2]
## [1,] -0.9093767 -0.5657675
## [2,]  0.4159736 -0.8245648</code></pre>
<pre class="r"><code>numpy$sqrt(x)</code></pre>
<pre><code>##          [,1]     [,2]
## [1,] 1.000000 1.732051
## [2,] 1.414214 2.000000</code></pre>
<pre class="r"><code>numpy$exp(x)</code></pre>
<pre><code>##          [,1]     [,2]
## [1,] 2.718282 20.08554
## [2,] 7.389056 54.59815</code></pre>
</div>
<div id="access-objects-created-in-python-from-r" class="section level1">
<h1>‚≠ê<strong>Access objects created in python from R</strong></h1>
<p><strong>note: Don‚Äôt work with Rmd</strong></p>
<pre class="r"><code>repl_python()
#===== python console====

# Load Pandas package
import pandas as pd

# Importing Dataset
travel = pd.read_excel(&quot;data/AIR.xlsx&quot;)

# Number of rows and columns
travel.shape

# Select random no. of rows 
travel.sample(n = 10)

# Group By
travel.groupby(&quot;Year&quot;).AIR.mean()

# Filter
t = travel.loc[(travel.Month &gt;= 6) &amp; (travel.Year &gt;= 1955),:]

# Return to R
exit

#==== R console====
# Access objects created in python from R
summary(py$t)

# Line chart using ggplot2
library(ggplot2)
ggplot(py$t, aes(AIR, Year)) + geom_line()</code></pre>
</div>
<div id="access-objects-created-in-r-from-python" class="section level1">
<h1>‚≠ê<strong>Access objects created in R from Python</strong></h1>
<pre class="r"><code># Let&#39;s create a object in R
#===== R console====
mydata = head(cars, n=15)

#Use the R created object within Python REPL
repl_python()
#===== python console====
import pandas as pd
#Access objects created in R from Python: r.mydata
r.mydata.describe()
pd.isnull(r.mydata.speed)

exit</code></pre>
</div>
<div id="building-logistic-regression-model-using-sklearn-package" class="section level1">
<h1>‚≠êBuilding Logistic Regression Model using sklearn package</h1>
<pre class="r"><code>repl_python()

# Load libraries
from sklearn import datasets
from sklearn.linear_model import LogisticRegression

# load the iris datasets
iris = datasets.load_iris()

# Developing logit model
model = LogisticRegression()
model.fit(iris.data, iris.target)

# Scoring
actual = iris.target
predicted = model.predict(iris.data)

# Performance Metrics
print(metrics.classification_report(actual, predicted))
print(metrics.confusion_matrix(actual, predicted))</code></pre>
<div id="links" class="section level2">
<h2><strong>Links</strong></h2>
<ul>
<li><a href="https://www.diycode.cc/projects/rstudio/reticulate">R Interface to Python</a></li>
</ul>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[9 Ensembling The Predictions]]></title>
    <link href="/2018/04/9-ensembling-the-predictions/"/>
    <id>/2018/04/9-ensembling-the-predictions/</id>
    <published>2018-04-01T00:00:00+00:00</published>
    <updated>2018-04-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code>load(&quot;../../data/craet_8.Rdata&quot;)
library(tidyverse)
library(caret)
#Set Parallel Processing - Decrease computation time
if (!require(&quot;doMC&quot;)) install.packages(&quot;doMC&quot;)
library(doMC)
registerDoMC(cores = 4)</code></pre>
</div>
<div id="train-multiple-models" class="section level1">
<h1>Train Multiple Models</h1>
<p>So now we have predictions from multiple individual models.To do this we had to run the train() function once for each model, store the models and pass it to the res</p>
<pre class="r"><code>library(caretEnsemble)

# Stacking Algorithms - Run multiple algos in one call.
trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, 
                             number=10, 
                             repeats=3,
                             savePredictions=TRUE, 
                             classProbs=TRUE)

algorithmList &lt;- c(&#39;rf&#39;, &#39;adaboost&#39;, &#39;earth&#39;, &#39;svmRadial&#39;)

set.seed(100)
models &lt;- caretList(Purchase ~ ., data=trainData, trControl=trainControl, methodList=algorithmList) 

results &lt;- resamples(models)
summary(results)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = results)
## 
## Models: rf, adaboost, earth, svmRadial 
## Number of resamples: 30 
## 
## Accuracy 
##                Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## rf        0.7011494 0.7764706 0.7965116 0.8033148 0.8250684 0.9058824    0
## adaboost  0.6823529 0.7674419 0.7906977 0.7966532 0.8328659 0.8941176    0
## earth     0.7209302 0.7906977 0.8187415 0.8164175 0.8367305 0.8604651    0
## svmRadial 0.7558140 0.7948276 0.8304378 0.8261842 0.8588235 0.9058824    0
## 
## Kappa 
##                Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## rf        0.3518625 0.5184810 0.5504351 0.5737290 0.6253768 0.8040346    0
## adaboost  0.3349754 0.5046620 0.5686668 0.5711983 0.6423870 0.7831018    0
## earth     0.4102857 0.5609657 0.6148850 0.6095470 0.6580869 0.7147595    0
## svmRadial 0.4685109 0.5645744 0.6326120 0.6285652 0.6993397 0.7996464    0</code></pre>
<pre class="r"><code># Box plots to compare models
scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;))
bwplot(results, scales=scales)</code></pre>
<p><img src="/post/2018-04-01-9-ensembling-the-predictions_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="combine-the-predictions-of-multiple-models-to-form-a-final-prediction" class="section level1">
<h1>Combine The Predictions Of Multiple Models To Form A Final Prediction</h1>
<ul>
<li>One thought: Is it possible to combine these predicted values from multiple models somehow and make a new ensemble that predicts better?</li>
<li>another thought: using the caretStack(). <strong>You just need to make sure you don‚Äôt use the same trainControl you used to build the models</strong></li>
</ul>
<pre class="r"><code># Create the trainControl
set.seed(101)
stackControl &lt;- trainControl(method=&quot;repeatedcv&quot;, 
                             number=10, 
                             repeats=3,
                             savePredictions=TRUE, 
                             classProbs=TRUE)

# Ensemble the predictions of `models` to form a new combined prediction based on glm
# Âú®ÂéüÊúâÊ®°ÂûãÁöÑÂü∫Á°Ä‰∏äÂè†Âä† ‰∏ÄËà¨Á∫øÊÄßÊ®°Âûã ‰Ωú‰∏∫È¢ÑÊµã
stack.glm &lt;- caretStack(models, method=&quot;glm&quot;, metric=&quot;Accuracy&quot;, trControl=stackControl)
print(stack.glm)</code></pre>
<pre><code>## A glm ensemble of 2 base models: rf, adaboost, earth, svmRadial
## 
## Ensemble results:
## Generalized Linear Model 
## 
## 2571 samples
##    4 predictor
##    2 classes: &#39;CH&#39;, &#39;MM&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 2314, 2314, 2314, 2314, 2313, 2313, ... 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.8321128  0.6419638</code></pre>
<pre class="r"><code># Predict on testData
stack_predicteds &lt;- predict(stack.glm, newdata=testData4)
head(stack_predicteds)</code></pre>
<pre><code>## [1] CH CH CH CH CH MM
## Levels: CH MM</code></pre>
<pre class="r"><code>save.image(&quot;../../data/craet_9.Rdata&quot;)</code></pre>
<p><strong>A point to consider: The ensembles tend to perform better if the predictions are less correlated with each other.</strong></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[8 How To Evaluate Performance Of Multiple Machine Learning Algorithms?]]></title>
    <link href="/2018/03/8-how-to-evaluate-performance-of-multiple-machine-learning-algorithms/"/>
    <id>/2018/03/8-how-to-evaluate-performance-of-multiple-machine-learning-algorithms/</id>
    <published>2018-03-31T00:00:00+00:00</published>
    <updated>2018-03-31T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code>load(&quot;../../data/craet_7.Rdata&quot;)
library(tidyverse)
library(caret)
#Set Parallel Processing - Decrease computation time
if (!require(&quot;doMC&quot;)) install.packages(&quot;doMC&quot;)
library(doMC)
registerDoMC(cores = 4)</code></pre>
</div>
<div id="caret-provides-the-resamples-function-where-you-can-provide-multiple-machine-learning-models-and-collectively-evaluate-them" class="section level1">
<h1>Caret provides the resamples() function where you can provide multiple machine learning models and collectively evaluate them</h1>
<div id="define-the-training-control" class="section level2">
<h2>Define the training control</h2>
<pre class="r"><code>fitControl &lt;- trainControl(
    method = &#39;cv&#39;,                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = &#39;final&#39;,       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) </code></pre>
</div>
<div id="train-models" class="section level2">
<h2>train models</h2>
<pre class="r"><code>set.seed(100)

# Training Adaboost using adaboost
model_adaboost = train(Purchase ~ ., data=trainData, method=&#39;adaboost&#39;, tuneLength=2, trControl = fitControl)

# Training Random Forest model using rf
model_rf = train(Purchase ~ ., data=trainData, method=&#39;rf&#39;, tuneLength=5, trControl = fitControl)

# Training xgBoost Dart
#model_xgbDART = train(Purchase ~ ., data=trainData, method=&#39;xgbDART&#39;, tuneLength=5, trControl = fitControl, verbose=F)

# Train SVM using MARS
model_svmRadial = train(Purchase ~ ., data=trainData, method=&#39;svmRadial&#39;, tuneLength=15, trControl = fitControl)</code></pre>
</div>
</div>
<div id="run-resamples-to-compare-the-models" class="section level1">
<h1>Run resamples() to compare the models</h1>
<pre class="r"><code># Compare model performances using resample()
models_compare &lt;- resamples(list(ADABOOST=model_adaboost, RF=model_rf, MARS=model_mars3, SVM=model_svmRadial))

# Summary of the models performances
summary(models_compare)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = models_compare)
## 
## Models: ADABOOST, RF, MARS, SVM 
## Number of resamples: 5 
## 
## ROC 
##               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## ADABOOST 0.8126510 0.8462687 0.8682549 0.8657598 0.8868515 0.9147727    0
## RF       0.8635394 0.8647908 0.8748565 0.8841388 0.9046198 0.9128875    0
## MARS     0.8520967 0.8660981 0.9091561 0.8953757 0.9118590 0.9376688    0
## SVM      0.8769723 0.8839375 0.8902597 0.8895479 0.8948048 0.9017652    0
## 
## Sens 
##               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## ADABOOST 0.7619048 0.7904762 0.7904762 0.8070330 0.8076923 0.8846154    0
## RF       0.7809524 0.8000000 0.8461538 0.8451832 0.8750000 0.9238095    0
## MARS     0.8190476 0.8476190 0.8857143 0.8739377 0.8942308 0.9230769    0
## SVM      0.8750000 0.8761905 0.8761905 0.8891209 0.9047619 0.9134615    0
## 
## Spec 
##               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## ADABOOST 0.7014925 0.7462687 0.7727273 0.7635007 0.7761194 0.8208955    0
## RF       0.6119403 0.6363636 0.7462687 0.7332429 0.8208955 0.8507463    0
## MARS     0.6567164 0.7164179 0.7313433 0.7454998 0.7424242 0.8805970    0
## SVM      0.6969697 0.7164179 0.7313433 0.7364089 0.7611940 0.7761194    0</code></pre>
<pre class="r"><code># Draw box plots to compare models
scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;))
bwplot(models_compare, scales=scales)</code></pre>
<p><img src="/post/2018-03-31-8-how-to-evaluate-performance-of-multiple-machine-learning-algorithms_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>save.image(&quot;../../data/craet_8.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[7 How To Do Hyperparameter Tuning ]]></title>
    <link href="/2018/03/7-how-to-do-hyperparameter-tuning/"/>
    <id>/2018/03/7-how-to-do-hyperparameter-tuning/</id>
    <published>2018-03-30T00:00:00+00:00</published>
    <updated>2018-03-30T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code>load(&quot;../../data/craet_6.Rdata&quot;)
library(tidyverse)
library(caret)
# Set Parallel Processing - Decrease computation time
if (!require(&quot;doMC&quot;)) install.packages(&quot;doMC&quot;)
library(doMC)
registerDoMC(cores = 4)</code></pre>
<div id="hyper-parameter-tuning-using-tunegrid" class="section level2">
<h2>Hyper parameter tuning using tuneGrid</h2>
<ol style="list-style-type: decimal">
<li><p>Model Tuning Parameter Set</p></li>
<li><p>Cross Validation Set</p>
<strong>Cross validation¬†<code>method</code>¬†can be one amongst</strong>:
<ul>
<li>‚Äòboot‚Äô: Bootstrap sampling</li>
<li>‚Äòboot632‚Äô: Bootstrap sampling with 63.2% bias correction applied</li>
<li>‚Äòoptimism_boot‚Äô: The optimism bootstrap estimator</li>
<li>‚Äòboot_all‚Äô: All boot methods.</li>
<li>‚Äòcv‚Äô: k-Fold cross validation</li>
<li>‚Äòrepeatedcv‚Äô: Repeated k-Fold cross validation</li>
<li>‚Äòoob‚Äô: Out of Bag cross validation</li>
<li>‚ÄòLOOCV‚Äô: Leave one out cross validation</li>
<li>‚ÄòLGOCV‚Äô: Leave group out cross validation</li>
</ul></li>
<li><p>Training And Tuning</p></li>
<li><p>Predict</p></li>
<li><p>Confusion Matrix</p></li>
</ol>
<pre class="r"><code># Step 1: Define the tuneGrid
marsGrid &lt;-  expand.grid(nprune = c(2, 4, 6, 8, 10), 
                        degree = c(1, 2, 3))

# Step 2: Define the training control
fitControl &lt;- trainControl(
    method = &#39;cv&#39;,                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = &#39;final&#39;,       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 

# Step 3: Training and Tuning hyper parameters by setting tuneGrid
set.seed(100)
model_mars3 = train(Purchase ~ ., data=trainData, method=&#39;earth&#39;, metric=&#39;ROC&#39;, tuneGrid = marsGrid, trControl = fitControl)
model_mars3</code></pre>
<pre><code>## Multivariate Adaptive Regression Spline 
## 
## 857 samples
##  18 predictor
##   2 classes: &#39;CH&#39;, &#39;MM&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 685, 685, 687, 686, 685 
## Resampling results across tuning parameters:
## 
##   degree  nprune  ROC        Sens       Spec     
##   1        2      0.8745398  0.8700916  0.7006784
##   1        4      0.8924657  0.8662454  0.7394844
##   1        6      0.8912361  0.8719414  0.7334238
##   1        8      0.8886974  0.8661722  0.7334238
##   1       10      0.8879988  0.8623626  0.7423790
##   2        2      0.8745398  0.8700916  0.7006784
##   2        4      0.8953757  0.8739377  0.7454998
##   2        6      0.8917824  0.8681868  0.7515152
##   2        8      0.8904559  0.8624359  0.7574401
##   2       10      0.8932377  0.8547436  0.7784261
##   3        2      0.8582783  0.8777106  0.6618725
##   3        4      0.8914544  0.8662454  0.7544550
##   3        6      0.8910605  0.8586264  0.7665310
##   3        8      0.8838647  0.8452015  0.7456355
##   3       10      0.8827056  0.8471062  0.7426504
## 
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were nprune = 4 and degree = 2.</code></pre>
<pre class="r"><code># Step 4: Predict on testData 
predicted3 &lt;- predict(model_mars3, testData4)

# Step 5: Compute the confusion matrix
confusionMatrix(reference = testData$Purchase, data = predicted3, mode=&#39;everything&#39;, positive=&#39;MM&#39;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  CH  MM
##         CH 117  21
##         MM  13  62
##                                           
##                Accuracy : 0.8404          
##                  95% CI : (0.7841, 0.8869)
##     No Information Rate : 0.6103          
##     P-Value [Acc &gt; NIR] : 2.164e-13       
##                                           
##                   Kappa : 0.6585          
##  Mcnemar&#39;s Test P-Value : 0.2299          
##                                           
##             Sensitivity : 0.7470          
##             Specificity : 0.9000          
##          Pos Pred Value : 0.8267          
##          Neg Pred Value : 0.8478          
##               Precision : 0.8267          
##                  Recall : 0.7470          
##                      F1 : 0.7848          
##              Prevalence : 0.3897          
##          Detection Rate : 0.2911          
##    Detection Prevalence : 0.3521          
##       Balanced Accuracy : 0.8235          
##                                           
##        &#39;Positive&#39; Class : MM              
## </code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  methods   stats     graphics  grDevices utils     datasets 
## [8] base     
## 
## other attached packages:
##  [1] earth_4.6.1        plotmo_3.3.5       TeachingDemos_2.10
##  [4] plotrix_3.7        doMC_1.3.5         iterators_1.0.9   
##  [7] foreach_1.4.4      caret_6.0-78       lattice_0.20-35   
## [10] forcats_0.3.0      stringr_1.3.0      dplyr_0.7.4       
## [13] purrr_0.2.4        readr_1.1.1        tidyr_0.8.0       
## [16] tibble_1.4.2       ggplot2_2.2.1      tidyverse_1.2.1   
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-131.1     lubridate_1.7.3    dimRed_0.1.0      
##  [4] httr_1.3.1         rprojroot_1.3-2    tools_3.4.3       
##  [7] backports_1.1.2    R6_2.2.2           rpart_4.1-13      
## [10] lazyeval_0.2.1     colorspace_1.3-2   nnet_7.3-12       
## [13] withr_2.1.1.9000   tidyselect_0.2.4   mnormt_1.5-5      
## [16] compiler_3.4.3     cli_1.0.0          rvest_0.3.2       
## [19] xml2_1.2.0         bookdown_0.7       scales_0.5.0.9000 
## [22] sfsmisc_1.1-2      DEoptimR_1.0-8     psych_1.7.8       
## [25] robustbase_0.92-8  digest_0.6.15      foreign_0.8-69    
## [28] rmarkdown_1.9      pkgconfig_2.0.1    htmltools_0.3.6   
## [31] rlang_0.2.0.9000   readxl_1.0.0       ddalpha_1.3.1.1   
## [34] bindr_0.1.1        jsonlite_1.5       ModelMetrics_1.1.0
## [37] magrittr_1.5       Matrix_1.2-12      Rcpp_0.12.16      
## [40] munsell_0.4.3      stringi_1.1.7      yaml_2.1.18       
## [43] MASS_7.3-49        plyr_1.8.4         recipes_0.1.2     
## [46] grid_3.4.3         crayon_1.3.4       haven_1.1.1       
## [49] splines_3.4.3      hms_0.4.2          knitr_1.20        
## [52] pillar_1.2.1       reshape2_1.4.3     codetools_0.2-15  
## [55] stats4_3.4.3       CVST_0.2-1         glue_1.2.0        
## [58] evaluate_0.10.1    blogdown_0.5       modelr_0.1.1      
## [61] cellranger_1.1.0   gtable_0.2.0       kernlab_0.9-25    
## [64] assertthat_0.2.0   DRR_0.0.3          xfun_0.1          
## [67] gower_0.1.2        prodlim_1.6.1      broom_0.4.3       
## [70] e1071_1.6-8        class_7.3-14       survival_2.41-3   
## [73] timeDate_3043.102  RcppRoll_0.2.2     bindrcpp_0.2      
## [76] lava_1.6           ipred_0.9-6</code></pre>
<pre class="r"><code>save.image(file = &quot;../../data/craet_7.Rdata&quot;)</code></pre>
</div>
<div id="links" class="section level2">
<h2><strong>Links</strong></h2>
<ul>
<li><a href="https://www.machinelearningplus.com/caret-package/#7howtodohyperparametertuningtooptimizethemodelforbetterperformance?">How to do hyperparameter tuning to optimize the model for better performance?</a></li>
</ul>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[6 Training and Tuning the model]]></title>
    <link href="/2018/03/6-training-and-tuning-the-model/"/>
    <id>/2018/03/6-training-and-tuning-the-model/</id>
    <published>2018-03-29T00:00:00+00:00</published>
    <updated>2018-03-29T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
</div>
<div id="training" class="section level1">
<h1>Training</h1>
<div id="how-to-train-the-model-and-interpret-the-results" class="section level2">
<h2>1. How to train the model and interpret the results?</h2>
<p>Once you have chosen an algorithm, building the model is fairly easy using the train() function</p>
<p><code>train()</code> does multiple other things like:</p>
<ol style="list-style-type: decimal">
<li><em>Cross validating the model</em></li>
<li><em>Tune the hyper parameters for optimal model performance</em></li>
<li><em>Choose the optimal model based on a given evaluation metric</em></li>
<li><em>Preprocess the predictors (what we did so far using preProcess())</em></li>
</ol>
</div>
<div id="how-to-compute-variable-importance" class="section level2">
<h2>2. How to compute variable importance?</h2>
<p>Which variables came out to be useful?</p>
</div>
</div>
<div id="tuning" class="section level1">
<h1>Tuning</h1>
<div id="preprocess-the-test-dataset-and-predict" class="section level2">
<h2>1. Preprocess the test dataset and predict</h2>
<p>The pre-processing in the following sequence:</p>
<p><strong>Missing Value imputation ‚Äì&gt; One-Hot Encoding ‚Äì&gt; Range Normalization</strong></p>
<p><strong>All the information required for pre-processing is stored in the respective preProcess model and dummyVar model.</strong></p>
<p>pass the testData through these models in the same sequence:</p>
<p><strong>preProcess_missingdata_model ‚Äì&gt; dummies_model ‚Äì&gt; preProcess_range_model</strong></p>
</div>
<div id="predict-on-testdata-and-confusion-matrix" class="section level2">
<h2>2. Predict on testData and Confusion Matrix</h2>
</div>
<div id="reference" class="section level2">
<h2><strong>Reference</strong></h2>
<p><a href="https://www.machinelearningplus.com/caret-package/">Traing and Tuning model</a></p>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[5 How to do feature selection using recursive feature elimination]]></title>
    <link href="/2018/03/5-how-to-do-feature-selection-using-recursive-feature-elimination/"/>
    <id>/2018/03/5-how-to-do-feature-selection-using-recursive-feature-elimination/</id>
    <published>2018-03-28T00:00:00+00:00</published>
    <updated>2018-03-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>You might need <em>a rigorous way to determine the important variables</em> first before feeding them to the ML algorithm. This is important.</p>
<p>A good choice of selecting the important features is the <em>recursive feature elimination (RFE)</em></p>
<p>RFE works in 3 broad steps:</p>
<p>Step 1: Build a ML model on a training dataset and estimate the feature importances on the test dataset.ÔºàÂú®Á°ÆÂÆöËá™Áî±Â∫¶ÁöÑÊÉÖÂÜµ‰∏ãÔºåËØÑ‰ª∑ÂèòÈáèÂú®ÊµãËØïÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºâ</p>
<p>Step 2: Keeping priority to the most important variables, iterate through by building models of given sizes. Ranking of the predictors is recalculated in each iteration.ÔºàÊääÂàöÊâçÁöÑËøáÁ®ãÂú®‰∏çÂêåÁöÑËá™Áî±Â∫¶‰∏ãËø≠‰ª£ÊâßË°åÔºâ</p>
<p>Step 3: The model performances are compared across different subset sizes to arrive at the optimal number and list of final predictors.ÔºàÊØîËæÉ‰∏çÂêåËá™Áî±Â∫¶ÁöÑÊµãËØïÈîôËØØÁéáÔºåÁªôÂá∫ÊúÄ‰Ω≥Ëá™Áî±Â∫¶Ê®°ÂûãÈÄâÊã©Ôºâ</p>
<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code># Load Package And Data
load(&quot;../../data/craet_4.Rdata&quot;)
library(tidyverse)
library(caret)
#Set Parallel Processing - Decrease computation time
if (!require(&quot;doMC&quot;)) install.packages(&quot;doMC&quot;)
library(doMC)
registerDoMC(cores = 4)</code></pre>
</div>
<div id="feature-select" class="section level1">
<h1>Feature select</h1>
<pre class="r"><code>set.seed(100)
options(warn=-1)

subsets &lt;- c(1:5, 10, 15, 18)

#Step 1: Build a ML model on a training dataset and estimate the feature importances on the test dataset.ÔºàÂú®Á°ÆÂÆöËá™Áî±Â∫¶ÁöÑÊÉÖÂÜµ‰∏ãÔºåËØÑ‰ª∑ÂèòÈáèÂú®ÊµãËØïÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºâ
ctrl &lt;- rfeControl(functions = rfFuncs,
                   method = &quot;repeatedcv&quot;,#repeated K-fold cross-validation
                   number = 10,#10-fold cross-validations
                   repeats = 5, #five separate 10-fold cross-validations are used
                   verbose = FALSE)
#Step 2: Keeping priority to the most important variables, iterate through by building models of given sizes. Ranking of the predictors is recalculated in each iteration.ÔºàÊääÂàöÊâçÁöÑËøáÁ®ãÂú®‰∏çÂêåÁöÑËá™Áî±Â∫¶‰∏ãËø≠‰ª£ÊâßË°å
lmProfile &lt;- rfe(x=trainData[, 1:18], y=trainData$Purchase,
                 sizes = subsets,
                 rfeControl = ctrl)

#Step 3: The model performances are compared across different subset sizes to arrive at the optimal number and list of final predictors.ÔºàÊØîËæÉ‰∏çÂêåËá™Áî±Â∫¶ÁöÑÊµãËØïÈîôËØØÁéáÔºåÁªôÂá∫ÊúÄ‰Ω≥Ëá™Áî±Â∫¶Ê®°ÂûãÈÄâÊã©
lmProfile</code></pre>
<pre><code>## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (10 fold, repeated 5 times) 
## 
## Resampling performance over subset size:
## 
##  Variables Accuracy  Kappa AccuracySD KappaSD Selected
##          1   0.7442 0.4569    0.04125 0.08753         
##          2   0.8124 0.6031    0.04002 0.08505         
##          3   0.8182 0.6136    0.04170 0.08790        *
##          4   0.8047 0.5879    0.04314 0.08993         
##          5   0.8000 0.5770    0.04215 0.08861         
##         10   0.8035 0.5826    0.04112 0.08815         
##         15   0.8089 0.5918    0.04209 0.09076         
##         18   0.8084 0.5918    0.04118 0.08894         
## 
## The top 3 variables (out of 3):
##    LoyalCH, PriceDiff, StoreID</code></pre>
<div id="input" class="section level2">
<h2>input</h2>
<ul>
<li><p>Size: sizes determines what all model sizes (the number of most important features) the rfe should consider</p></li>
<li>rfeControl():
<ul>
<li>functions: what type of algorithm should be used <strong>rfFuncs:: random forest based</strong>
<ul>
<li>methods: repeated K-fold cross-validation</li>
<li>number: 10-fold cross-validations</li>
<li>repeats: five separate 10-fold cross-validations are used</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="output" class="section level2">
<h2>output</h2>
<p>The Output Shows: - accuracy<br />
- kappa (and their standard deviation) for the different model sizes we provided - The final selected model subset size is marked with a * in the rightmost Selected column.</p>
<pre class="r"><code>save.image(&quot;../../data/craet_5.Rdata&quot;)
sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  methods   stats     graphics  grDevices utils     datasets 
## [8] base     
## 
## other attached packages:
##  [1] doMC_1.3.5      iterators_1.0.9 foreach_1.4.4   caret_6.0-78   
##  [5] lattice_0.20-35 forcats_0.3.0   stringr_1.3.0   dplyr_0.7.4    
##  [9] purrr_0.2.4     readr_1.1.1     tidyr_0.8.0     tibble_1.4.2   
## [13] ggplot2_2.2.1   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##  [1] httr_1.3.1          ddalpha_1.3.1.1     sfsmisc_1.1-2      
##  [4] jsonlite_1.5        splines_3.4.3       prodlim_1.6.1      
##  [7] modelr_0.1.1        assertthat_0.2.0    stats4_3.4.3       
## [10] DRR_0.0.3           cellranger_1.1.0    yaml_2.1.18        
## [13] robustbase_0.92-8   ipred_0.9-6         pillar_1.2.1       
## [16] backports_1.1.2     glue_1.2.0          digest_0.6.15      
## [19] randomForest_4.6-12 rvest_0.3.2         colorspace_1.3-2   
## [22] recipes_0.1.2       htmltools_0.3.6     Matrix_1.2-12      
## [25] plyr_1.8.4          psych_1.7.8         timeDate_3043.102  
## [28] pkgconfig_2.0.1     CVST_0.2-1          broom_0.4.3        
## [31] haven_1.1.1         bookdown_0.7        scales_0.5.0.9000  
## [34] gower_0.1.2         lava_1.6            withr_2.1.1.9000   
## [37] nnet_7.3-12         lazyeval_0.2.1      cli_1.0.0          
## [40] mnormt_1.5-5        survival_2.41-3     magrittr_1.5       
## [43] crayon_1.3.4        readxl_1.0.0        evaluate_0.10.1    
## [46] nlme_3.1-131.1      MASS_7.3-49         xml2_1.2.0         
## [49] dimRed_0.1.0        foreign_0.8-69      class_7.3-14       
## [52] blogdown_0.5        tools_3.4.3         hms_0.4.2          
## [55] kernlab_0.9-25      munsell_0.4.3       bindrcpp_0.2       
## [58] e1071_1.6-8         compiler_3.4.3      RcppRoll_0.2.2     
## [61] rlang_0.2.0.9000    grid_3.4.3          rmarkdown_1.9      
## [64] gtable_0.2.0        ModelMetrics_1.1.0  codetools_0.2-15   
## [67] reshape2_1.4.3      R6_2.2.2            lubridate_1.7.3    
## [70] knitr_1.20          bindr_0.1.1         rprojroot_1.3-2    
## [73] stringi_1.1.7       Rcpp_0.12.16        rpart_4.1-13       
## [76] tidyselect_0.2.4    DEoptimR_1.0-8      xfun_0.1</code></pre>
</div>
<div id="reference" class="section level2">
<h2><strong>Reference</strong></h2>
<ul>
<li><a href="https://www.machinelearningplus.com/caret-package/">How to do feature selection using recursive feature elimination (rfe)?</a></li>
</ul>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[4 How To Visualize The Importance Of Variables Using featurePlot]]></title>
    <link href="/2018/03/4-how-to-visualize-the-importance-of-variables-using-featureplot/"/>
    <id>/2018/03/4-how-to-visualize-the-importance-of-variables-using-featureplot/</id>
    <published>2018-03-27T00:00:00+00:00</published>
    <updated>2018-03-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code>load(&quot;../../data/craet_3-3.Rdata&quot;)
library(tidyverse)
library(caret)</code></pre>
</div>
<div id="q-how-the-predictors-influence-the-y" class="section level1">
<h1>Q: How The Predictors Influence The Y</h1>
<p>ÈÄâÊã©ÈáçË¶ÅÁöÑÂèòÈáè: ÈÄöËøáËßÇÂØüÂú®YÁöÑÂàÜÁªÑ‰∏ãÂêÑ‰∏™ÂèòÈáèÁöÑÂàÜÂ∏ÉÊÉÖÂÜµ</p>
<p>‰∏ÄËà¨Êúâ ÁÆ±Á∫øÂõæ Âíå ÂØÜÂ∫¶Âõæ</p>
</div>
<div id="box-plot" class="section level1">
<h1>box-plot</h1>
<pre class="r"><code>featurePlot(x = trainData[, 1:18], 
            y = trainData$Purchase, 
            plot = &quot;box&quot;,#&quot;density&quot;
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation=&quot;free&quot;), 
                          y = list(relation=&quot;free&quot;)))</code></pre>
<p><img src="/post/2018-03-27-4-how-to-visualize-the-importance-of-variables-using-featureplot_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="density" class="section level1">
<h1>Density</h1>
<pre class="r"><code>featurePlot(x = trainData[, 1:18], 
            y = trainData$Purchase, 
            plot = &quot;density&quot;,
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation=&quot;free&quot;), 
                          y = list(relation=&quot;free&quot;)))</code></pre>
<p><img src="/post/2018-03-27-4-how-to-visualize-the-importance-of-variables-using-featureplot_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>save.image(&quot;../../data/craet_4.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Scraping and Wranging Tables from Research Articles]]></title>
    <link href="/2018/03/scraping-and-wranging-tables-from-research-articles/"/>
    <id>/2018/03/scraping-and-wranging-tables-from-research-articles/</id>
    <published>2018-03-27T00:00:00+00:00</published>
    <updated>2018-03-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>What do you do when you want to use results from the literature to anchor your own analysis? we‚Äôll go through a practical scenario on scraping an html table from a Nature Genetics article into R and wrangling the data into a useful format.</p>
<div id="scraping-a-html-table-from-a-webpage" class="section level1">
<h1>01. Scraping a html table from a webpage</h1>
<pre class="r"><code>#load packages
library(&quot;rvest&quot;)
library(&quot;knitr&quot;)
library(tidyverse)
#scraping web page
url &lt;- &quot;https://www.nature.com/articles/ng.2802/tables/2&quot;

#====üî•find where is the table lives on this webpage====
table_path=&#39;//*[@id=&quot;content&quot;]/div/div/figure/div[1]/div/div[1]/table&#39;
#get the table
nature_genetics_table2 &lt;- url %&gt;%
  read_html() %&gt;%
  html_nodes(xpath=table_path) %&gt;%
  html_table(fill=T) %&gt;% .[[1]]
#the first few lines of table
kable(nature_genetics_table2[1:4,])</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">SNPa</th>
<th align="left">Chr.</th>
<th align="left">Positionb</th>
<th align="left">Closest genec</th>
<th align="left">Major/minor alleles</th>
<th align="left">MAFd</th>
<th align="left">Stage 1</th>
<th align="left">Stage 1</th>
<th align="left">Stage 2</th>
<th align="left">Stage 2</th>
<th align="left">Overall</th>
<th align="left">Overall</th>
<th align="left">Overall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">SNPa</td>
<td align="left">Chr.</td>
<td align="left">Positionb</td>
<td align="left">Closest genec</td>
<td align="left">Major/minor alleles</td>
<td align="left">MAFd</td>
<td align="left">OR (95% CI)e</td>
<td align="left">Meta P value</td>
<td align="left">OR (95% CI)e</td>
<td align="left">Meta P value</td>
<td align="left">OR (95% CI)e</td>
<td align="left">Meta P value</td>
<td align="left">I2 (%), P valuef</td>
</tr>
<tr class="even">
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="odd">
<td align="left">rs6656401</td>
<td align="left">1</td>
<td align="left">207692049</td>
<td align="left">CR1</td>
<td align="left">G/A</td>
<td align="left">0.197</td>
<td align="left">1.17 (1.12‚Äì1.22)</td>
<td align="left">7.7 √ó 10‚àí15</td>
<td align="left">1.21 (1.14‚Äì1.28)</td>
<td align="left">7.9 √ó 10‚àí11</td>
<td align="left">1.18 (1.14‚Äì1.22)</td>
<td align="left">5.7 √ó 10‚àí24</td>
<td align="left">0, 7.8 √ó 10‚àí1</td>
</tr>
<tr class="even">
<td align="left">rs6733839</td>
<td align="left">2</td>
<td align="left">127892810</td>
<td align="left">BIN1</td>
<td align="left">C/T</td>
<td align="left">0.409</td>
<td align="left">1.21 (1.17‚Äì1.25)</td>
<td align="left">1.7 √ó 10‚àí26</td>
<td align="left">1.24 (1.18‚Äì1.29)</td>
<td align="left">3.4 √ó 10‚àí19</td>
<td align="left">1.22 (1.18‚Äì1.25)</td>
<td align="left">6.9 √ó 10‚àí44</td>
<td align="left">28, 6.1 √ó 10‚àí2</td>
</tr>
</tbody>
</table>
</div>
<div id="making-messy-data-useful" class="section level1">
<h1>02 Making messy data useful</h1>
<div id="cleaning-up-the-rows" class="section level2">
<h2>Cleaning up the rows</h2>
<p>All The Elements Of These Rows Contain The Exact Same Text</p>
<pre class="r"><code>v=which(apply(nature_genetics_table2,1, function(x) length(unique(unlist(x))) )==1)
v</code></pre>
<pre><code>## [1]  2 12 18</code></pre>
</div>
<div id="split-table" class="section level2">
<h2>split table</h2>
<pre class="r"><code>nature_genetics_table2_list = split(nature_genetics_table2, cumsum(1:nrow(nature_genetics_table2) %in% v))
nature_genetics_table2_list = lapply(nature_genetics_table2_list[2:4], function(y) {
y$Description = unique(as.character(y[1, ]))
y[-1, ]
})

#rbind three table
nature_genetics_table2_clean = do.call(&quot;rbind&quot;, nature_genetics_table2_list)

kable(nature_genetics_table2_clean[1:3,])</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">SNPa</th>
<th align="left">Chr.</th>
<th align="left">Positionb</th>
<th align="left">Closest genec</th>
<th align="left">Major/minor alleles</th>
<th align="left">MAFd</th>
<th align="left">Stage 1</th>
<th align="left">Stage 1</th>
<th align="left">Stage 2</th>
<th align="left">Stage 2</th>
<th align="left">Overall</th>
<th align="left">Overall</th>
<th align="left">Overall</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1.3</td>
<td align="left">rs6656401</td>
<td align="left">1</td>
<td align="left">207692049</td>
<td align="left">CR1</td>
<td align="left">G/A</td>
<td align="left">0.197</td>
<td align="left">1.17 (1.12‚Äì1.22)</td>
<td align="left">7.7 √ó 10‚àí15</td>
<td align="left">1.21 (1.14‚Äì1.28)</td>
<td align="left">7.9 √ó 10‚àí11</td>
<td align="left">1.18 (1.14‚Äì1.22)</td>
<td align="left">5.7 √ó 10‚àí24</td>
<td align="left">0, 7.8 √ó 10‚àí1</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="even">
<td>1.4</td>
<td align="left">rs6733839</td>
<td align="left">2</td>
<td align="left">127892810</td>
<td align="left">BIN1</td>
<td align="left">C/T</td>
<td align="left">0.409</td>
<td align="left">1.21 (1.17‚Äì1.25)</td>
<td align="left">1.7 √ó 10‚àí26</td>
<td align="left">1.24 (1.18‚Äì1.29)</td>
<td align="left">3.4 √ó 10‚àí19</td>
<td align="left">1.22 (1.18‚Äì1.25)</td>
<td align="left">6.9 √ó 10‚àí44</td>
<td align="left">28, 6.1 √ó 10‚àí2</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="odd">
<td>1.5</td>
<td align="left">rs10948363</td>
<td align="left">6</td>
<td align="left">47487762</td>
<td align="left">CD2AP</td>
<td align="left">A/G</td>
<td align="left">0.266</td>
<td align="left">1.10 (1.07‚Äì1.14)</td>
<td align="left">3.1 √ó 10‚àí8</td>
<td align="left">1.09 (1.04‚Äì1.15)</td>
<td align="left">4.1 √ó 10‚àí4</td>
<td align="left">1.10 (1.07‚Äì1.13)</td>
<td align="left">5.2 √ó 10‚àí11</td>
<td align="left">0, 9 √ó 10‚àí1</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="fixing-column-names" class="section level1">
<h1>03. Fixing column names</h1>
<pre class="r"><code>colnames(nature_genetics_table2_clean) &lt;- c(&quot;SNP&quot;, &quot;Chr&quot;, &quot;Position&quot;, &quot;Closest gene&quot;, &quot;Major/minor alleles&quot;, &quot;MAF&quot;, &quot;Stage1_OR&quot;, &quot;Stage1_MetaP&quot;, &quot;Stage2_OR&quot;,&quot;Stage2_MetaP&quot;,    &quot;Overall_OR&quot;, &quot;Overall_MetaP&quot;, &quot;I2_Percent/P&quot;,&quot;Description&quot;)
colnames(nature_genetics_table2_clean)</code></pre>
<pre><code>##  [1] &quot;SNP&quot;                 &quot;Chr&quot;                 &quot;Position&quot;           
##  [4] &quot;Closest gene&quot;        &quot;Major/minor alleles&quot; &quot;MAF&quot;                
##  [7] &quot;Stage1_OR&quot;           &quot;Stage1_MetaP&quot;        &quot;Stage2_OR&quot;          
## [10] &quot;Stage2_MetaP&quot;        &quot;Overall_OR&quot;          &quot;Overall_MetaP&quot;      
## [13] &quot;I2_Percent/P&quot;        &quot;Description&quot;</code></pre>
</div>
<div id="making-a-character-variable-into-a-numeric-variable" class="section level1">
<h1>04. Making a character variable into a numeric variable</h1>
<pre class="r"><code># &quot; √ó 10-&quot; -&gt; &quot;e-&quot;
nature_genetics_table2_clean$Stage1_MetaP &lt;- 
str_replace(nature_genetics_table2_clean$Stage1_MetaP,&quot; √ó 10‚àí&quot;,&quot;e-&quot;) %&gt;% as.numeric()
kable(nature_genetics_table2_clean[1:3,])</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">SNP</th>
<th align="left">Chr</th>
<th align="left">Position</th>
<th align="left">Closest gene</th>
<th align="left">Major/minor alleles</th>
<th align="left">MAF</th>
<th align="left">Stage1_OR</th>
<th align="right">Stage1_MetaP</th>
<th align="left">Stage2_OR</th>
<th align="left">Stage2_MetaP</th>
<th align="left">Overall_OR</th>
<th align="left">Overall_MetaP</th>
<th align="left">I2_Percent/P</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1.3</td>
<td align="left">rs6656401</td>
<td align="left">1</td>
<td align="left">207692049</td>
<td align="left">CR1</td>
<td align="left">G/A</td>
<td align="left">0.197</td>
<td align="left">1.17 (1.12‚Äì1.22)</td>
<td align="right">0</td>
<td align="left">1.21 (1.14‚Äì1.28)</td>
<td align="left">7.9 √ó 10‚àí11</td>
<td align="left">1.18 (1.14‚Äì1.22)</td>
<td align="left">5.7 √ó 10‚àí24</td>
<td align="left">0, 7.8 √ó 10‚àí1</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="even">
<td>1.4</td>
<td align="left">rs6733839</td>
<td align="left">2</td>
<td align="left">127892810</td>
<td align="left">BIN1</td>
<td align="left">C/T</td>
<td align="left">0.409</td>
<td align="left">1.21 (1.17‚Äì1.25)</td>
<td align="right">0</td>
<td align="left">1.24 (1.18‚Äì1.29)</td>
<td align="left">3.4 √ó 10‚àí19</td>
<td align="left">1.22 (1.18‚Äì1.25)</td>
<td align="left">6.9 √ó 10‚àí44</td>
<td align="left">28, 6.1 √ó 10‚àí2</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="odd">
<td>1.5</td>
<td align="left">rs10948363</td>
<td align="left">6</td>
<td align="left">47487762</td>
<td align="left">CD2AP</td>
<td align="left">A/G</td>
<td align="left">0.266</td>
<td align="left">1.10 (1.07‚Äì1.14)</td>
<td align="right">0</td>
<td align="left">1.09 (1.04‚Äì1.15)</td>
<td align="left">4.1 √ó 10‚àí4</td>
<td align="left">1.10 (1.07‚Äì1.13)</td>
<td align="left">5.2 √ó 10‚àí11</td>
<td align="left">0, 9 √ó 10‚àí1</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
</tbody>
</table>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
##  [1] forcats_0.3.0   stringr_1.3.0   dplyr_0.7.4     purrr_0.2.4    
##  [5] readr_1.1.1     tidyr_0.8.0     tibble_1.4.2    ggplot2_2.2.1  
##  [9] tidyverse_1.2.1 knitr_1.20      rvest_0.3.2     xml2_1.2.0     
## 
## loaded via a namespace (and not attached):
##  [1] xfun_0.1          reshape2_1.4.3    haven_1.1.1      
##  [4] lattice_0.20-35   colorspace_1.3-2  htmltools_0.3.6  
##  [7] yaml_2.1.18       rlang_0.2.0.9000  pillar_1.2.1     
## [10] foreign_0.8-69    glue_1.2.0        selectr_0.3-2    
## [13] modelr_0.1.1      readxl_1.0.0      bindrcpp_0.2     
## [16] bindr_0.1.1       plyr_1.8.4        munsell_0.4.3    
## [19] blogdown_0.5      gtable_0.2.0      cellranger_1.1.0 
## [22] psych_1.7.8       evaluate_0.10.1   parallel_3.4.3   
## [25] curl_3.1          highr_0.6         broom_0.4.3      
## [28] Rcpp_0.12.16      backports_1.1.2   scales_0.5.0.9000
## [31] jsonlite_1.5      mnormt_1.5-5      hms_0.4.2        
## [34] digest_0.6.15     stringi_1.1.7     bookdown_0.7     
## [37] grid_3.4.3        rprojroot_1.3-2   cli_1.0.0        
## [40] tools_3.4.3       magrittr_1.5      lazyeval_0.2.1   
## [43] crayon_1.3.4      pkgconfig_2.0.1   lubridate_1.7.3  
## [46] assertthat_0.2.0  rmarkdown_1.9     httr_1.3.1       
## [49] R6_2.2.2          nlme_3.1-131.1    compiler_3.4.3</code></pre>
</div>
<div id="reference" class="section level1">
<h1><strong>Reference</strong></h1>
<ul>
<li><a href="http://research.libd.org/rstatsclub/2018/03/19/introduction-to-scraping-and-wranging-tables-from-research-articles/">Introduction to Scraping and Wranging Tables from Research Articles</a></li>
</ul>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[3.3 How To Create Dummy Variables And Normalization]]></title>
    <link href="/2018/03/how-to-create-dummy-variables-and-normalization/"/>
    <id>/2018/03/how-to-create-dummy-variables-and-normalization/</id>
    <published>2018-03-26T00:00:00+00:00</published>
    <updated>2018-03-26T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code>load(&quot;../../data/craet_3-2.Rdata&quot;)
library(tidyverse)
library(caret)</code></pre>
</div>
<div id="why-dummy-variables" class="section level1">
<h1>Why Dummy Variables</h1>
<p>ÂØπ‰∫éÂ≠óÁ¨¶ÂûãÁöÑÂõ†Â≠êÂèòÈáèÔºåÊàë‰ª¨ÈúÄË¶ÅÊääÂÆÉËΩ¨Âèò‰∏∫ÊúâÂ∫èÁöÑÊï∞ÂÄºÔºå‰∏ÄËà¨ËΩ¨‰∏∫ 0Ôºå1 ÁöÑ‰∫åÂèòÈáèÔºå ËøôÊ†∑0 Â∞±‰ª£Ë°®Âü∫Á°ÄÊ∞¥Âπ≥Ôºå 1‰ª£Ë°®ÊØîËæÉÁªÑ</p>
<p><img src="https://i.loli.net/2018/03/26/5ab847e351487.jpg" width="30%" style="display: block; margin: auto;" /></p>
</div>
<div id="how" class="section level1">
<h1>How</h1>
<pre class="r"><code># One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model &lt;- dummyVars(Purchase ~ ., data=trainData)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat &lt;- predict(dummies_model, newdata = trainData)

# # Convert to dataframe
trainData &lt;- data.frame(trainData_mat)

# # See the structure of the new dataset
str(trainData)</code></pre>
<pre><code>## &#39;data.frame&#39;:    857 obs. of  18 variables:
##  $ WeekofPurchase: num  -1.1 -1.74 -1.68 -1.29 -1.04 ...
##  $ StoreID       : num  -1.29 -1.29 1.33 1.33 1.33 ...
##  $ PriceCH       : num  -1.14 -1.73 -1.73 -1.14 -1.14 ...
##  $ PriceMM       : num  -0.688 -2.898 -2.898 -0.688 -0.688 ...
##  $ DiscCH        : num  -0.452 -0.452 -0.452 -0.452 -0.452 ...
##  $ DiscMM        : num  -0.582 -0.582 -0.582 1.341 1.341 ...
##  $ SpecialCH     : num  -0.429 -0.429 -0.429 2.329 -0.429 ...
##  $ SpecialMM     : num  -0.42 -0.42 -0.42 -0.42 -0.42 ...
##  $ LoyalCH       : num  -0.205 -0.525 1.256 1.324 1.35 ...
##  $ SalePriceMM   : num  0.113 -1.101 -1.101 -1.506 -1.506 ...
##  $ SalePriceCH   : num  -0.431 -0.844 -0.844 -0.431 -0.431 ...
##  $ PriceDiff     : num  0.341 -0.563 -0.563 -1.165 -1.165 ...
##  $ Store7.No     : num  1 1 0 0 0 0 0 0 0 1 ...
##  $ Store7.Yes    : num  0 0 1 1 1 1 1 1 1 0 ...
##  $ PctDiscMM     : num  -0.588 -0.588 -0.588 1.447 1.447 ...
##  $ PctDiscCH     : num  -0.448 -0.448 -0.448 -0.448 -0.448 ...
##  $ ListPriceDiff : num  0.211 -1.988 -1.988 0.211 0.211 ...
##  $ STORE         : num  -0.457 -0.457 -1.15 -1.15 -1.15 ...</code></pre>
</div>
<div id="why-normalization" class="section level1">
<h1>Why Normalization</h1>
<p>‰∏∫‰∫ÜÊ∂àÈô§‰∏çÂêåÂèòÈáèÁî±‰∫éÂçï‰ΩçÈÄ†ÊàêÁöÑÊùÉÈáçÂΩ±ÂìçÔºåÊàë‰ª¨ÂØπÊï∞ÊçÆËøõË°åÊï∞ÊçÆÊ†áÂáÜÂåñ</p>
</div>
<div id="how-1" class="section level1">
<h1>How</h1>
<ol style="list-style-type: decimal">
<li><strong>range:</strong>¬†Normalize values so it ranges between 0 and 1</li>
<li><strong>center:</strong>¬†Subtract Mean</li>
<li><strong>scale:</strong>¬†Divide by standard deviation</li>
<li><strong>BoxCox:</strong>¬†Remove skewness leading to normality. Values must be &gt; 0</li>
<li><strong>YeoJohnson:</strong>¬†Like BoxCox, but works for negative values.</li>
<li><strong>expoTrans:</strong>¬†Exponential transformation, works for negative values.</li>
<li><strong>pca:</strong>¬†Replace with principal components</li>
<li><strong>ica:</strong>¬†Replace with independent components</li>
<li><strong>spatialSign:</strong>¬†Project the data to a unit circle</li>
</ol>
<pre class="r"><code>preProcess_range_model &lt;- preProcess(trainData, method=&#39;range&#39;)
trainData &lt;- predict(preProcess_range_model, newdata = trainData)

# Append the Y variable
trainData$Purchase &lt;- y

apply(trainData[, 1:10], 2, FUN=function(x){c(&#39;min&#39;=min(x), &#39;max&#39;=max(x))})</code></pre>
<pre><code>##     WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH
## min              0       0       0       0      0      0         0
## max              1       1       1       1      1      1         1
##     SpecialMM LoyalCH SalePriceMM
## min         0       0           0
## max         1       1           1</code></pre>
<pre class="r"><code>save.image(file = &quot;../../data/craet_3-3.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Why should I trust you ü§ñ?]]></title>
    <link href="/2018/03/why-should-i-trust-you/"/>
    <id>/2018/03/why-should-i-trust-you/</id>
    <published>2018-03-26T00:00:00+00:00</published>
    <updated>2018-03-26T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>‰º†ÁªüÁöÑÊú∫Âô®Â≠¶‰π†Â∑•‰ΩúÊµÅÁ®ã‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ê®°ÂûãËÆ≠ÁªÉÂíå‰ºòÂåñ‰∏ä; ÊúÄÂ•ΩÁöÑÊ®°ÂûãÈÄöÂ∏∏ÊòØÈÄöËøáÂÉèÁ≤æÂ∫¶ÊàñËÄÖÈîôËØØËøôÊ†∑ÁöÑÊÄßËÉΩÂ∫¶ÈáèÊù•ÈÄâÊã©ÁöÑÔºåËÄå‰∏îÂ¶ÇÊûúÂÆÉÈÄöËøá‰∫ÜËøô‰∫õÊÄßËÉΩÊ†áÂáÜÁöÑÊüê‰∫õÈòàÂÄºÔºåÊàë‰ª¨ÂÄæÂêë‰∫éÂÅáËÆæ‰∏Ä‰∏™Ê®°ÂûãÊòØË∂≥Â§üÂ•ΩÁöÑ„ÄÇÂú®Êú∫Âô®Â≠¶‰π†ÁöÑËÆ∏Â§öÂ∫îÁî®‰∏≠ÔºåÁî®Êà∑‰ºö‰ΩøÁî®‰∏Ä‰∏™Ê®°ÂûãÊù•Â∏ÆÂä©ÂÅöÂÜ≥ÂÆöÔºå ‰æãÂ¶ÇÔºö‰∏Ä‰ΩçÂåªÁîü‰∏ç‰ºöÂØπÁóÖ‰∫∫ËøõË°åÊâãÊúØÔºå‰ªÖ‰ªÖÂõ†‰∏∫Ëøô‰∏™Ê®°ÂûãËØ¥‰∏çÂ∫îËØ•ËøõË°åÊâãÊúØÔºü</p>
<p>Áî±‰∫éÂ§çÊùÇÁöÑÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÊú¨Ë¥®‰∏äÊòØÈªëÁõíÂ≠êÔºåËÄå‰∏îÂ§™Â§çÊùÇÔºåÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÂÅöÂá∫ÁöÑÂàÜÁ±ªÂÜ≥ÂÆöÈÄöÂ∏∏ÂæàÈöæË¢´‰∫∫Á±ªÁöÑÂ§ßËÑëÊâÄÁêÜËß£Ôºå‰ΩÜÊòØËÉΩÂ§üÁêÜËß£ÂíåËß£ÈáäËøô‰∫õÊ®°ÂûãÂØπ‰∫éÊèêÈ´òÊ®°ÂûãË¥®ÈáèÔºåÊèêÈ´ò‰ø°‰ªªÂ∫¶ÂíåÈÄèÊòéÂ∫¶‰ª•ÂèäÂáèÂ∞ëÂÅèËßÅÈùûÂ∏∏ÈáçË¶ÅÔºåÂõ†‰∏∫‰∫∫Á±ªÂæÄÂæÄÂÖ∑ÊúâËâØÂ•ΩÁöÑÁõ¥ËßâÂíåÂõ†ÊûúÊé®ÁêÜÔºåËøô‰∫õÈÉΩÊòØÈöæ‰ª•Âú®Êï∞ÊçÆËØÑ‰º∞ÊåáÊ†á‰∏≠ÊçïËé∑„ÄÇ</p>
<p>Âõ†Ê≠§ÔºåÊàë‰ª¨Â∏åÊúõËÉΩÂ§üÂΩ¢Ë±°ÁöÑÁêÜËß£ÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÂéüÁêÜÔºö‰∏∫‰ªÄ‰πà‰∏Ä‰∏™Ê®°ÂûãÂ∞ÜÂÖ∑ÊúâÁâπÂÆöÊ†áÁ≠æÁöÑÊ°à‰æãËøõË°åÂáÜÁ°ÆÂàÜÁ±ª„ÄÇeg: ‰∏∫‰ªÄ‰πà‰∏Ä‰∏™‰π≥ËÖ∫ËÇøÂùóÊ†∑Êú¨Ë¢´ÂΩíÁ±ª‰∏∫‚ÄúÊÅ∂ÊÄß‚ÄùËÄå‰∏çÊòØ‚ÄúËâØÊÄßÔºå‰ªÖ‰ªÖÂõ†‰∏∫ÂÆÉÈïøÂæó‰∏ëÂêóÔºü</p>
<blockquote>
<p><a href="https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime">Local Interpretable Model-Agnostic Explanations (LIME)</a> is an attempt to make these complex models at least partly understandable. The method has been published in <a href="https://arxiv.org/pdf/1602.04938.pdf">‚ÄúWhy Should I Trust You?‚Äù</a> Explaining the Predictions of Any Classifier. By Marco Tulio Ribeiro, Sameer Singh and Carlos Guestrin from the University of Washington in Seattle</p>
</blockquote>
<div id="how-lime-works" class="section level2">
<h2><strong>How LIME works</strong></h2>
<blockquote>
<p>lime is able to explain all models for which we can obtain prediction probabilities (in R, that is every model that works with predict(type = ‚Äúprob‚Äù)). It makes use of the fact that linear models are easy to explain because they are based on linear relationships between features and class labels: The complex model function is approximated by locally fitting linear models to permutations of the original training set.On each permutation, a linear model is being fit and weights are given so that incorrect classification of instances that are more similar to the original data are penalized (positive weights support a decision, negative weights contradict them). This will give an approximation of how much (and in which way) each feature contributed to a decision made by the model</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7fa56fd7333.jpg" width="30%" /></p>
<blockquote>
<p>We take the image on the left and divide it into interpretable components</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7faba0d302a.jpg" width="30%" /></p>
<blockquote>
<p>we then generate a data set of perturbed instances by turning some of the interpretable components ‚ÄúoÔ¨Ä‚Äù. For each perturbed instance, we get the probability that a tree frog is in the image according to the model.</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7fac3ca8787.jpg" width="30%" /></p>
<blockquote>
<p>We then learn a simple (linear) model on this data set, which is locally weighted‚Äîthat is, we care more about making mistakes in perturbed instances that are more similar to the original image</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7fac8382a86.jpg" width="30%" /></p>
<blockquote>
<p>the end, we present the superpixels with highest positive weights as an explanation, graying out everything else</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7faf16f0320.jpg" width="30%" /></p>
<p>ü§ñÈ¢ÑÊµãËøôÂº†ÂõæÊòØ‰∏™Ê†ëËõôÊòØÂõ†‰∏∫Ëøô‰∏™ÈÉ®ÂàÜ,ÊâÄ‰ª•ü§ñÈ¢ÑÊµãÁªìÊûúÊòØÊØîËæÉÂèØ‰ø°ÁöÑ„ÄÇ <img src="https://i.loli.net/2018/02/11/5a7faebabe34e.jpg" style="display: block; margin: auto;" /></p>
<p>ü§ñÈ¢ÑÊµãËøôÂº†ÂõæÊòØÂè∞ÁêÉÊòØÊ†πÊçÆËøô‰∫õÈÉ®ÂàÜÔºåÊâÄ‰ª•ü§ñÈ¢ÑÊµãÁªìÊûúÊòØ‰∏çÂèØ‰ø°ÁöÑ„ÄÇ <img src="https://i.loli.net/2018/02/11/5a7fb61ecb060.jpg" style="display: block; margin: auto;" /></p>
</div>
<div id="example-in-r" class="section level2">
<h2><strong>Example in R</strong></h2>
<div id="prepare-the-breast-cancer-data" class="section level3">
<h3>01.Prepare the breast cancer data</h3>
<p>This <strong>data</strong> of example comes from the book of <strong>R in action</strong></p>
<pre class="r"><code>loc &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/&quot;
ds  &lt;- &quot;breast-cancer-wisconsin/breast-cancer-wisconsin.data&quot;
url &lt;- paste(loc, ds, sep=&quot;&quot;)

breast &lt;- read.table(url, sep=&quot;,&quot;, header=FALSE, na.strings=&quot;?&quot;)
names(breast) &lt;- c(&quot;ID&quot;, &quot;clumpThickness&quot;, &quot;sizeUniformity&quot;,
                   &quot;shapeUniformity&quot;, &quot;maginalAdhesion&quot;, 
                   &quot;singleEpithelialCellSize&quot;, &quot;bareNuclei&quot;, 
                   &quot;blandChromatin&quot;, &quot;normalNucleoli&quot;, &quot;mitosis&quot;, &quot;class&quot;)

df &lt;- breast[-1]
df$class &lt;- factor(df$class, levels=c(2,4), 
                   labels=c(&quot;benign&quot;, &quot;malignant&quot;))

set.seed(1234)
train &lt;- sample(nrow(df), 0.7*nrow(df))
df.train &lt;- df[train,]
df.validate &lt;- df[-train,]
table(df.train$class)</code></pre>
<pre><code>## 
##    benign malignant 
##       329       160</code></pre>
<pre class="r"><code>table(df.validate$class)</code></pre>
<pre><code>## 
##    benign malignant 
##       129        81</code></pre>
</div>
<div id="create-decision-tree-model" class="section level3">
<h3>02 Create decision tree model</h3>
<pre class="r"><code>library(rpart)
set.seed(1234)
dtree &lt;- rpart(class ~ ., data=df.train, method=&quot;class&quot;,      
               parms=list(split=&quot;information&quot;))
dtree$cptable</code></pre>
<pre><code>##         CP nsplit rel error  xerror       xstd
## 1 0.800000      0   1.00000 1.00000 0.06484605
## 2 0.046875      1   0.20000 0.30625 0.04150018
## 3 0.012500      3   0.10625 0.20625 0.03467089
## 4 0.010000      4   0.09375 0.18125 0.03264401</code></pre>
<pre class="r"><code>plotcp(dtree)</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>dtree.pruned &lt;- prune(dtree, cp=.0125)</code></pre>
</div>
<div id="predict" class="section level3">
<h3>03 predict</h3>
<pre class="r"><code>dtree.pred &lt;- predict(dtree.pruned, df.validate, type=&quot;class&quot;)
(dtree.perf &lt;- table(df.validate$class, dtree.pred, 
                    dnn=c(&quot;Actual&quot;, &quot;Predicted&quot;)))</code></pre>
<pre><code>##            Predicted
## Actual      benign malignant
##   benign       122         7
##   malignant      2        79</code></pre>
</div>
<div id="plot-decision-tree" class="section level3">
<h3>04 plot decision tree</h3>
<pre class="r"><code>#plot01
library(rpart.plot)
prp(dtree.pruned, type = 2, extra = 104,  
    fallen.leaves = TRUE, main=&quot;Decision Tree&quot;)
#plot02
library(partykit)</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>library(dplyr)
dtree.pruned %&gt;% as.party() %&gt;% plot()</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
</div>
<div id="lime-why-should-i-trust-you" class="section level3">
<h3>LIME: why should I trust you ü§ñ?</h3>
<pre class="r"><code>library(lime)
explainer &lt;- lime(df.train, model = dtree.pruned)

# Explain new observation
#[model_type](https://github.com/thomasp85/lime/blob/master/R/models.R)
model_type.rpart &lt;- function(x, ...) &#39;classification&#39;#defined model_type method
test.data &lt;- 
  df.validate %&gt;% 
  dplyr::select(-class) %&gt;% 
  head(3)
explanation &lt;- lime::explain(test.data, explainer, n_labels = 1, n_features = 2)

# The output is provided in a consistent tabular format and includes the
# output from the model.
head(explanation)</code></pre>
<pre><code>##       model_type case     label label_prob  model_r2 model_intercept
## 1 classification    3    benign  0.9933993 0.1809444      0.05042991
## 2 classification    3    benign  0.9933993 0.1809444      0.05042991
## 3 classification    4 malignant  0.9507042 0.1918642      0.54743276
## 4 classification    4 malignant  0.9507042 0.1918642      0.54743276
## 5 classification    5    benign  0.9933993 0.1924691      0.05378321
## 6 classification    5    benign  0.9933993 0.1924691      0.05378321
##   model_prediction                  feature feature_value feature_weight
## 1        0.4637659           blandChromatin             3   -0.001977357
## 2        0.4637659           sizeUniformity             1    0.415313316
## 3        0.9524157 singleEpithelialCellSize             3    0.002885248
## 4        0.9524157           sizeUniformity             8    0.402097681
## 5        0.4712710          maginalAdhesion             3   -0.004533032
## 6        0.4712710           sizeUniformity             1    0.422020822
##                        feature_desc                      data
## 1           2 &lt; blandChromatin &lt;= 3 3, 1, 1, 1, 2, 2, 3, 1, 1
## 2               sizeUniformity &lt;= 4 3, 1, 1, 1, 2, 2, 3, 1, 1
## 3 2 &lt; singleEpithelialCellSize &lt;= 4 6, 8, 8, 1, 3, 4, 3, 7, 1
## 4                4 &lt; sizeUniformity 6, 8, 8, 1, 3, 4, 3, 7, 1
## 5              maginalAdhesion &lt;= 3 4, 1, 1, 3, 2, 1, 3, 1, 1
## 6               sizeUniformity &lt;= 4 4, 1, 1, 3, 2, 1, 3, 1, 1
##               prediction
## 1 0.99339934, 0.00660066
## 2 0.99339934, 0.00660066
## 3 0.04929577, 0.95070423
## 4 0.04929577, 0.95070423
## 5 0.99339934, 0.00660066
## 6 0.99339934, 0.00660066</code></pre>
<pre class="r"><code># And can be visualised directly
plot_features(explanation)</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
</div>
<div id="links" class="section level2">
<h2><strong>Links</strong></h2>
<ul>
<li><a href="http://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease">CKD data set</a></li>
<li><a href="https://github.com/marcotcr/lime">open-source Python code for LIME</a></li>
<li><a href="https://github.com/thomasp85/lime">R package for LIME</a></li>
</ul>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[3.2 statistic description and impute missing value]]></title>
    <link href="/2018/03/3-2-statistic-description-and-impute-missing-value/"/>
    <id>/2018/03/3-2-statistic-description-and-impute-missing-value/</id>
    <published>2018-03-24T00:00:00+00:00</published>
    <updated>2018-03-24T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="section level1">
<h1>Âä†ËΩΩÊï∞ÊçÆÂíåÂåÖ</h1>
<pre class="r"><code>load(&quot;../../data/caret.Rdata&quot;)
library(tidyverse)
library(caret)</code></pre>
<p>Âú®ËøõË°åÊï∞ÊçÆÊï¥ÁêÜ‰πãÂâç Êàë‰ª¨ÂÖàÁúãÁúãËÆ≠ÁªÉÊï∞ÊçÆÁöÑÁªüËÆ°ÊèèËø∞</p>
<p><code>skimr</code>ÂåÖÂØπÂàóÁöÑÁªüËÆ°Êèê‰æõ‰∫ÜÊñπ‰æøÁöÑÂáΩÊï∞</p>
<p><code>skimr::skim_to_wide()</code> ËæìÂá∫‰∏Ä‰∏™ÂåÖÂê´ÂàóÁªüËÆ°ÊèèËø∞ÁöÑÊï∞ÊçÆÊ°Ü</p>
<pre class="r"><code>library(skimr)
skimmed &lt;- skim_to_wide(trainData)
skimmed[, c(1:5, 9:11, 13, 15:16)]</code></pre>
<pre><code>## # A tibble: 18 x 11
##    type   variable  missing complete n     mean   sd    p0    median p100 
##    &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;
##  1 factor Purchase  0       857      857   &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt; 
##  2 factor Store7    0       857      857   &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt; 
##  3 integ‚Ä¶ SpecialCH 2       855      857   &quot;  0.‚Ä¶ &quot; 0.‚Ä¶ 0     0      1    
##  4 integ‚Ä¶ SpecialMM 4       853      857   &quot;  0.‚Ä¶ &quot; 0.‚Ä¶ 0     0      1    
##  5 integ‚Ä¶ STORE     2       855      857   &quot;  1.‚Ä¶ &quot; 1.‚Ä¶ 0     2      4    
##  6 integ‚Ä¶ StoreID   1       856      857   &quot;  3.‚Ä¶ &quot; 2.‚Ä¶ 1     3      7    
##  7 integ‚Ä¶ WeekofPu‚Ä¶ 0       857      857   254.17 15.59 227   257    278  
##  8 numer‚Ä¶ DiscCH    2       855      857   0.054  &quot;0.1‚Ä¶ &quot; 0 ‚Ä¶ &quot;0   &quot; &quot;0.5‚Ä¶
##  9 numer‚Ä¶ DiscMM    3       854      857   &quot;0.12‚Ä¶ &quot;0.2‚Ä¶ &quot; 0 ‚Ä¶ &quot;0   &quot; &quot;0.8‚Ä¶
## 10 numer‚Ä¶ ListPric‚Ä¶ 0       857      857   &quot;0.22‚Ä¶ &quot;0.1‚Ä¶ &quot; 0 ‚Ä¶ 0.24   0.44 
## 11 numer‚Ä¶ LoyalCH   5       852      857   &quot;0.56‚Ä¶ &quot;0.3‚Ä¶ &quot; 1.‚Ä¶ &quot;0.6 &quot; &quot;1  ‚Ä¶
## 12 numer‚Ä¶ PctDiscCH 2       855      857   0.028  0.063 &quot; 0 ‚Ä¶ &quot;0   &quot; 0.25 
## 13 numer‚Ä¶ PctDiscMM 2       855      857   0.058  0.099 &quot; 0 ‚Ä¶ &quot;0   &quot; &quot;0.4‚Ä¶
## 14 numer‚Ä¶ PriceCH   1       856      857   &quot;1.87‚Ä¶ &quot;0.1‚Ä¶ &quot; 1.‚Ä¶ 1.86   2.09 
## 15 numer‚Ä¶ PriceDiff 1       856      857   &quot;0.15‚Ä¶ &quot;0.2‚Ä¶ &quot;-0.‚Ä¶ 0.23   0.64 
## 16 numer‚Ä¶ PriceMM   1       856      857   &quot;2.08‚Ä¶ &quot;0.1‚Ä¶ &quot; 1.‚Ä¶ 2.09   2.29 
## 17 numer‚Ä¶ SalePric‚Ä¶ 1       856      857   &quot;1.81‚Ä¶ &quot;0.1‚Ä¶ &quot; 1.‚Ä¶ 1.86   2.09 
## 18 numer‚Ä¶ SalePric‚Ä¶ 3       854      857   &quot;1.96‚Ä¶ &quot;0.2‚Ä¶ &quot; 1.‚Ä¶ 2.09   2.29 
## # ... with 1 more variable: hist &lt;chr&gt;</code></pre>
</div>
<div class="section level1">
<h1>ÊèíÂÖ•Êï∞ÊçÆ</h1>
<p>Caret Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÂæàÊñπ‰æøÁöÑÂáΩÊï∞ <code>preProcess()</code></p>
<ul>
<li>ËÆæÁΩÆ <code>method=knnImpute</code> ÁîüÊàê‰∏Ä‰∏™Ê®°Âûã</li>
<li>‰ΩøÁî® <code>predict()</code> ÂØπÊï∞ÊçÆËøõË°åÊèíÂÖ•</li>
</ul>
<pre class="r"><code># Create the knn imputation model on the training data
preProcess_missingdata_model &lt;- preProcess(trainData, method=&#39;knnImpute&#39;)
preProcess_missingdata_model</code></pre>
<pre><code>## Created from 828 samples and 18 variables
## 
## Pre-processing:
##   - centered (16)
##   - ignored (2)
##   - 5 nearest neighbor imputation (16)
##   - scaled (16)</code></pre>
<pre class="r"><code># Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData &lt;- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)</code></pre>
<pre><code>## [1] FALSE</code></pre>
<pre class="r"><code>save.image(file = &quot;../../data/craet_3-2.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[3.1 How to split the dataset into training and validation?]]></title>
    <link href="/2018/03/3-1-how-to-split-the-dataset-into-training-and-validation/"/>
    <id>/2018/03/3-1-how-to-split-the-dataset-into-training-and-validation/</id>
    <published>2018-03-23T00:00:00+00:00</published>
    <updated>2018-03-23T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Êï∞ÊçÆÂáÜÂ§áÂ•Ω‰∫Ü‰πãÂêéÁöÑÁ¨¨‰∏ÄÊ≠•Â∞±ÊòØÊãÜÂàÜÊï∞ÊçÆÈõÜ‰∏∫ËÆ≠ÁªÉÊï∞ÊçÆÂíåÊµãËØïÊï∞ÊçÆÔºå‰∏ÄËà¨ÊòØ 8:2 ÁöÑÊØî‰æã„ÄÇ</p>
<p>‰∏∫‰ªÄ‰πàÊãÜÂàÜÊï∞ÊçÆÂë¢Ôºü</p>
<p>ÂΩìÊàë‰ª¨Âú®ÊûÑÂª∫‰∏Ä‰∏™Êú∫Âô®Â≠¶‰π†Ê®°Âûã‰∏äÊó∂ÔºåÁúüÊ≠£ÁöÑÁõÆÁöÑÊòØ‰∏∫‰∫ÜÈ¢ÑÊµãÁúüÊòØ‰∏ñÁïåÁöÑÊï∞ÊçÆÔºåËÄåÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÊòØ‰æùÈù†ÁÆóÊ≥ïÂ≠¶‰π†ËÆ≠ÁªÉÊï∞ÊçÆÂ≠¶‰π†Y ‰∏é X ÁöÑÂÖ≥Á≥ªÔºåËøôÁßçÁöÑÂÖ≥Á≥ªÁöÑÂ≠¶‰π†Â•ΩÂùèÁöÑËØÑÂà§ÊòØË¶Å‰æùÈù†Ê≤°ÊúâÂèÇ‰∏éÂ≠¶‰π†Ê®°ÂûãÁöÑÊï∞ÊçÆ‰∏éÈ¢ÑÊµãÊï∞ÊçÆ‰πãÈó¥ÁöÑÂ∑ÆË∑ùÊù•ËØÑÂà§ÁöÑ„ÄÇ</p>
<pre class="r"><code># Load the caret package
library(caret)

# Import dataset
orange &lt;- read.csv(&#39;../../data/orange_juice_withmissing.csv&#39;)
# Create the training and test datasets
set.seed(100)

# Step 1: Get row numbers for the training data
trainRowNumbers &lt;- createDataPartition(orange$Purchase, p=0.8, list=FALSE)

# Step 2: Create the training  dataset
trainData &lt;- orange[trainRowNumbers,]

# Step 3: Create the test dataset
testData &lt;- orange[-trainRowNumbers,]

# Store X and Y for later use.
x = trainData[, 2:18]
y = trainData$Purchase</code></pre>
<p><code>createDataPartition</code>ÔºöËæìÂÖ• Y Âíå P ÊØîÁéáÔºàËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊØîÁéáÔºâ ËæìÂá∫ ËÆ≠ÁªÉÊï∞ÊçÆÁöÑË°åÁ¥¢Âºï„ÄÇ</p>
<div id="save-the-image-for-next-blog" class="section level1">
<h1>save the image for next blog</h1>
<pre class="r"><code>save.image(file = &quot;../../data/caret.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[2 machine learning: load dataset]]></title>
    <link href="/2018/03/machine-learning-2-load-dataset/"/>
    <id>/2018/03/machine-learning-2-load-dataset/</id>
    <published>2018-03-22T00:00:00+00:00</published>
    <updated>2018-03-22T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-the-package-and-dataset" class="section level1">
<h1>Load the package and dataset</h1>
<p>Êàë‰ª¨Â∞Ü‰ΩøÁî® ISLR ÂåÖ‰∏≠ÁöÑ <a href="&#39;https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv&#39;">Orange Juice Data</a>.</p>
<p>ÁõÆÊ†áÔºö È¢ÑÊµãÈ°æÂÆ¢‰ºöË¥≠‰π∞Âì™‰∏§ÁßçüçäÊ±Å</p>
<p>Êï∞ÊçÆÈõÜ‰∏çÊòØÂæàÂ§ßÔºåÊàë‰ª¨ÁöÑÈáçÁÇπÊòØÊûÑÂª∫Ê®°ÂûãÁöÑËøáÁ®ãÔºåËÄåÈùûÁúüÊ≠£Êê≠Âª∫‰∏Ä‰∏™ÊúâÁî®ÁöÑÊ®°Âûã„ÄÇ</p>
<p>üëåÔºålet‚Äôgo.</p>
<pre class="r"><code># Load the caret package
library(caret)

# Import dataset
orange &lt;- read.csv(&#39;../../data/orange_juice_withmissing.csv&#39;)

# Structure of the dataframe
str(orange)</code></pre>
<pre><code>## &#39;data.frame&#39;:    1070 obs. of  18 variables:
##  $ Purchase      : Factor w/ 2 levels &quot;CH&quot;,&quot;MM&quot;: 1 1 1 2 1 1 1 1 1 1 ...
##  $ WeekofPurchase: int  237 239 245 227 228 230 232 234 235 238 ...
##  $ StoreID       : int  1 1 1 1 7 7 7 7 7 7 ...
##  $ PriceCH       : num  1.75 1.75 1.86 1.69 1.69 1.69 1.69 1.75 1.75 1.75 ...
##  $ PriceMM       : num  1.99 1.99 2.09 1.69 1.69 1.99 1.99 1.99 1.99 1.99 ...
##  $ DiscCH        : num  0 0 0.17 0 0 0 0 0 0 0 ...
##  $ DiscMM        : num  0 0.3 0 0 0 0 0.4 0.4 0.4 0.4 ...
##  $ SpecialCH     : int  0 0 0 0 0 0 1 1 0 0 ...
##  $ SpecialMM     : int  0 1 0 0 0 1 1 0 0 0 ...
##  $ LoyalCH       : num  0.5 0.6 0.68 0.4 0.957 ...
##  $ SalePriceMM   : num  1.99 1.69 2.09 1.69 1.69 1.99 1.59 1.59 1.59 1.59 ...
##  $ SalePriceCH   : num  1.75 1.75 1.69 1.69 1.69 1.69 1.69 1.75 1.75 1.75 ...
##  $ PriceDiff     : num  0.24 -0.06 0.4 0 0 0.3 -0.1 -0.16 -0.16 -0.16 ...
##  $ Store7        : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 2 2 2 2 2 2 ...
##  $ PctDiscMM     : num  0 0.151 0 0 0 ...
##  $ PctDiscCH     : num  0 0 0.0914 0 0 ...
##  $ ListPriceDiff : num  0.24 0.24 0.23 0 0 0.3 0.3 0.24 0.24 0.24 ...
##  $ STORE         : int  1 1 1 1 0 0 0 0 0 0 ...</code></pre>
<pre class="r"><code># See top 6 rows and 10 columns
head(orange[, 1:10])</code></pre>
<pre><code>##   Purchase WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH
## 1       CH            237       1    1.75    1.99   0.00    0.0         0
## 2       CH            239       1    1.75    1.99   0.00    0.3         0
## 3       CH            245       1    1.86    2.09   0.17    0.0         0
## 4       MM            227       1    1.69    1.69   0.00    0.0         0
## 5       CH            228       7    1.69    1.69   0.00    0.0         0
## 6       CH            230       7    1.69    1.99   0.00    0.0         0
##   SpecialMM  LoyalCH
## 1         0 0.500000
## 2         1 0.600000
## 3         0 0.680000
## 4         0 0.400000
## 5         0 0.956535
## 6         1 0.965228</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Machine Learning in R 1Ôºöintrodution]]></title>
    <link href="/2018/03/machine-learning-in-r-introdution/"/>
    <id>/2018/03/machine-learning-in-r-introdution/</id>
    <published>2018-03-21T00:00:00+00:00</published>
    <updated>2018-03-21T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Caret PackageÊòØ‰∏Ä‰∏™Áî®‰∫éÂú®R‰∏≠ÊûÑÂª∫Êú∫Âô®Â≠¶‰π†Ê®°ÂûãÁöÑÁªºÂêàÊ°ÜÊû∂„ÄÇÂú®Ê≠§ÂçöÊñá‰∏≠ÔºåÊàë‰ºöËß£Èáä caret ÂåÖÁöÑÂá†‰πéÊâÄÊúâÊ†∏ÂøÉÂäüËÉΩÔºåÂπ∂ÂºïÂØºÂÆåÊàêÊûÑÂª∫È¢ÑÊµãÊ®°ÂûãÁöÑÂàÜÊ≠•ËøáÁ®ã„ÄÇ</p>
<p>Caret ÊòØ Classification And REgression Training ÁöÑÁÆÄÁß∞„ÄÇ</p>
<p>Caret ÂåÖÂæàÂ•ΩÂú∞ËÆ©ÊâÄÊúâ‰∏éÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÂºÄÂèëÁõ∏ÂÖ≥ÁöÑÊ≠•È™§ÈõÜÊàêÂà∞‰∏Ä‰∏™ÁÆÄÂåñÁöÑÂ∑•‰ΩúÊµÅÁ®ã‰∏≠ÔºåÂá†‰πéÊØè‰∏™‰∏ªÊµÅÁöÑMLÁÆóÊ≥ïÈÉΩÂèØ‰ª•Âú®R‰∏≠ÂÆûÁé∞„ÄÇ</p>
<p>Áî±‰∫éRÂÖ∑ÊúâÂ¶ÇÊ≠§Â§öÁöÑÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÂÆûÁé∞ÂåÖÔºåÂõ†Ê≠§ÈÄâÂàôÂú®Âì™‰∏™ÂåÖ‰∏≠ÂÆûÁé∞ÊüêÁßçÁÆóÊ≥ïÊòØÈùûÂ∏∏Â§¥ÁñºÁöÑÈóÆÈ¢ò„ÄÇÂ§öÊï∞Êó∂ÂÄôÔºåÂÆûÁé∞ÁÆóÊ≥ïÁöÑËØ≠Ê≥ïÂíåÊñπÊ≥ïÂú®‰∏çÂêåÂåÖ‰∏≠ÊúâÊâÄ‰∏çÂêå„ÄÇ ÁªìÂêàÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÔºåÊü•ÈòÖË∂ÖÂèÇÊï∞ÁöÑÂ∏ÆÂä©È°µÈù¢ÔºàÂÆö‰πâÁÆóÊ≥ïÂ¶Ç‰ΩïÂ≠¶‰π†ÁöÑÂèÇÊï∞ÔºâÂπ∂Âä™ÂäõÂØªÊâæÊúÄ‰Ω≥Ê®°ÂûãÔºåÂèØ‰ª•‰ΩøÊûÑÂª∫È¢ÑÊµãÊ®°ÂûãÊàê‰∏∫‰∏ÄÈ°πÁõ∏ÂÖ≥‰ªªÂä°„ÄÇ</p>
<p>Âú®Êú¨ÊïôÁ®ãÂêéÈù¢ÁöÑÈÉ®ÂàÜÔºåÊàëÂ∞Ü‰ªãÁªçÂ¶Ç‰ΩïÊü•ÁúãÊâÄÊúâcaret ÊîØÊåÅÁöÑMLÁÆóÊ≥ïÔºàËøôÊòØ‰∏Ä‰∏™ÂæàÈïøÁöÑÂàóË°®Ôºâ‰ª•ÂèäÂèØ‰ª•Ë∞ÉÊï¥Âì™‰∫õË∂ÖÂèÇÊï∞„ÄÇ</p>
<p>Ê≠§Â§ñÔºåÊàë‰ª¨‰∏ç‰ºöÊ≠¢Ê≠•‰∫é caret ÂåÖÔºåÊàë‰ª¨Â∞ÜÁúãÁúãÂ¶Ç‰ΩïÂ∑ßÂ¶ôÂú∞ÈõÜÊàêÊù•Ëá™Â§ö‰∏™ÊúÄ‰Ω≥Ê®°ÂûãÁöÑÈ¢ÑÊµãÔºåÂπ∂ÂèØËÉΩ‰ΩøÁî® <code>caretEnsemble</code> Êù•‰∫ßÁîüÊõ¥Â•ΩÁöÑÈ¢ÑÊµã„ÄÇ</p>
<p>Ëøô‰∏™ÊïôÁ®ãÊÄªÂÖ±ÂåÖÊã¨5ÈÉ®ÂàÜÔºåÂàÜÂà´ÊòØÔºö 1. Êï∞ÊçÆÂáÜÂ§áÂíåÊ∏ÖÁêÜ 2. ÂèØËßÜÂåñÈáçË¶ÅÂèòÈáè 3. ÁâπÂæÅÈÄâÊã© 4. ËÆ≠ÁªÉÊ®°ÂûãÂíåË∞ÉËäÇÊ®°Âûã 5. È¢ÑÊµã</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[HOW TO USE THE NCBI&#39;S NEW API KEYS]]></title>
    <link href="/2018/03/how-to-use-the-ncbis-new-api-keys/"/>
    <id>/2018/03/how-to-use-the-ncbis-new-api-keys/</id>
    <published>2018-03-19T00:00:00+00:00</published>
    <updated>2018-03-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><a href="https://www.ncbi.nlm.nih.gov/">The NCBI</a> is one of the most important sources of biological data. The centre provides access to information on 28 million scholarly articles through PubMed and 250 million DNA sequences through GenBank. More importantly, records in the [50 public databases] (<a href="https://www.ncbi.nlm.nih.gov/guide/all/#databases" class="uri">https://www.ncbi.nlm.nih.gov/guide/all/#databases</a>) maintained by the NCBI are strongly cross-referenced. As a result, it is possible to pinpoint searches using almost 2 million taxonomic names or a <a href="https://www.nlm.nih.gov/mesh/">controlled vocabulary with 270,000 terms</a>.</p>
<p><strong>Rentrez has been designed to make it easy to search for and download NCBI records and download them from within an R session.</strong></p>
<p>I though it might be fun to use this post to find out where papers describing R packages are published these days</p>
<p>Here we use the <code>entrez_search</code> and <code>entrez_summary</code> functions to get some information on all of the papers published in 2017 with the term ‚ÄòR package‚Äô in their title:</p>
<pre class="r"><code>if (!require(&quot;rentrez&quot;)) install.packages(&quot;rentrez&quot;)
library(rentrez)

pkg_search &lt;- entrez_search(db=&quot;pubmed&quot;, 
                            term=&quot;(R Package[TITLE]) AND (2018[PDAT])&quot;, 
                            use_history=TRUE)
pkg_summs &lt;- entrez_summary(db=&quot;pubmed&quot;, web_history=pkg_search$web_history)
pkg_summs</code></pre>
<pre><code>## List of  31 esummary records. First record:
## 
##  $`29554216`
## esummary result with 42 items:
##  [1] uid               pubdate           epubdate         
##  [4] source            authors           lastauthor       
##  [7] title             sorttitle         volume           
## [10] issue             pages             lang             
## [13] nlmuniqueid       issn              essn             
## [16] pubtype           recordstatus      pubstatus        
## [19] articleids        history           references       
## [22] attributes        pmcrefcount       fulljournalname  
## [25] elocationid       doctype           srccontriblist   
## [28] booktitle         medium            edition          
## [31] publisherlocation publishername     srcdate          
## [34] reportnumber      availablefromurl  locationlabel    
## [37] doccontriblist    docdate           bookname         
## [40] chapter           sortpubdate       sortfirstauthor</code></pre>
<p>we are interested in the journals in which these papers appear. We can use the helper function¬†<code>extract_from_esummary</code>¬†to isolate the <em>source</em> of each paper, then use¬†<code>table</code>¬†to count up the frequency of each journal.</p>
<pre class="r"><code>library(ggplot2)
library(ggpomological)
#scales::show_col(ggpomological:::pomological_palette)

journals &lt;- extract_from_esummary(pkg_summs, &quot;source&quot;)
journal_freq &lt;- as.data.frame(table(journals, dnn=&quot;journal&quot;), responseName=&quot;n.papers&quot;)
pkg_journal &lt;- ggplot(journal_freq, aes(reorder(journal, n.papers), n.papers)) + 
    geom_point(size=2) + 
    coord_flip() + 
    scale_y_continuous(&quot;Number of papers&quot;) +
    scale_x_discrete(&quot;Journal&quot;) +
    theme_bw() +
    ggtitle(&quot;Venues for papers describing R Packages in 2018&quot;)

pkg_journal + ggpomological::theme_pomological()</code></pre>
<p><img src="/post/2018-03-19-how_to_use_NCBI_API_keys_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>So, it looks like <em>Bioinformatics</em>, <em>Plos One</em> and <em>Comput Methods Progams Biomed</em> Resources are popular destinations for papers describing R packages, but these appear in journals all the way across the biological sciences.</p>
<p>The NCBI now gives users the opportunity to¬†<a href="https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/">register for an access key</a>¬†that will allow them to make up to 10 requests per second (non-registered users are limited to 3 requests per second per IP address).For one-off cases, this is as simple as adding the api_key argument to a given function call.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[A function for ggplot2: input strings for aes()]]></title>
    <link href="/2018/03/a-function-for-ggplot2-input-strings-for-aes/"/>
    <id>/2018/03/a-function-for-ggplot2-input-strings-for-aes/</id>
    <published>2018-03-18T00:00:00+00:00</published>
    <updated>2018-03-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Âú®ÊàëÂÜôÊñáÁ´†ÁîªÂõæÊó∂ÁªèÂ∏∏ÈÅáÂà∞ÁöÑ‰∏Ä‰∏™ÈóÆÈ¢òÊòØÔºöggplot2 ÂùêÊ†áËΩ¥ÁöÑËæìÂÖ•‰∏çÊîØÊåÅËæìÂÖ•Êï∞ÊçÆÊ°ÜÁöÑÂèòÈáèÂêçÔºåÈÄöÂ∏∏‰ºöÊä•ÈîôÊâæ‰∏çÂà∞ÂØπË±°</p>
<div class="section level1">
<h1>üå∞ÔºöÈóÆÈ¢òÊèèËø∞</h1>
<p>data: <a href="https://github.com/fivethirtyeight/data/tree/master/early-senate-polls">early senate poll</a></p>
<pre class="r"><code>library(tidyverse) # general tasks
library(broom) # tidy model output
library(ggthemes) # style the plots

poll_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/early-senate-polls/early-senate-polls.csv&quot;)

glimpse(poll_data)</code></pre>
<pre><code>## Observations: 107
## Variables: 4
## $ year                  &lt;int&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006...
## $ election_result       &lt;int&gt; -39, -10, -9, -16, 40, 10, -2, -41, -31,...
## $ presidential_approval &lt;int&gt; 46, 33, 32, 33, 53, 44, 37, 39, 42, 33, ...
## $ poll_average          &lt;int&gt; -28, -10, -1, -15, 39, 14, 2, -22, -27, ...</code></pre>
<p>background: <strong>there is a strong correlation between polling numbers and the ultimate result of an election</strong></p>
<div id="-" class="section level2">
<h2>ÊûÑÂª∫Ê®°ÂûãÔºö Á∫øÊÄßÊ®°Âûã</h2>
<pre class="r"><code>poll_lm &lt;- lm(election_result ~ poll_average, data = poll_data)

summary(poll_lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = election_result ~ poll_average, data = poll_data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -29.4281  -5.0197   0.5601   6.1364  17.9357 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -0.89110    0.76969  -1.158     0.25    
## poll_average  1.04460    0.03777  27.659   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.93 on 105 degrees of freedom
## Multiple R-squared:  0.8793, Adjusted R-squared:  0.8782 
## F-statistic:   765 on 1 and 105 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="section level2">
<h2>ÂÜô‰∏™ÂáΩÊï∞ÁîªÂá∫Âõ†ÂèòÈáèÂíåËá™ÂèòÈáèÁöÑÂÖ≥Á≥ª</h2>
<p>ÁªìÊûúÂá∫Áé∞‰∫Ü‰∏Ä‰∏™‰ª§ÊàëË¥πËß£ÁöÑÊä•Èîô</p>
<blockquote>
<p><strong>Error in FUN(X[[i]], ‚Ä¶) : object ‚Äòpoll_average‚Äô not found</strong></p>
</blockquote>
<p>Êàë‰∏çÊñ≠Âú∞Ê£ÄÊü•ÊàëÁöÑÊãºÂÜôÔºåÁõ¥Âà∞ÊàëÂºÄÂßãÊÄÄÁñë‰∫∫Áîü</p>
</div>
</div>
<div id="define-aesthetic-mappings-programatically" class="section level1">
<h1>Ëß£ÂÜ≥ÂäûÊ≥ïÔºö<a href="http://ggplot2.tidyverse.org/reference/aes_.html"><strong>Define aesthetic mappings programatically</strong></a></h1>
<pre class="r"><code>plot_model &lt;- function(mod, explanatory, response, .fitted = &quot;.fitted&quot;) {
  augment(mod) %&gt;%
  ggplot() +
    geom_point(aes_string(x = explanatory, y = response), color = &quot;#2CA58D&quot;) +
    geom_line(aes_string(x = explanatory, y = .fitted), color = &quot;#033F63&quot;) +
    theme_solarized() +
    theme(axis.title = element_text()) +
    labs(x = &quot;Poll average&quot;, y = &quot;Election results&quot;)
}

plot_model(poll_lm, &quot;poll_average&quot;, &quot;election_result&quot;)</code></pre>
<p><img src="/post/2018-03-18-a-function-for-ggplot2-input-strings-for-aes_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hello world]]></title>
    <link href="/2018/03/hello-world/"/>
    <id>/2018/03/hello-world/</id>
    <published>2018-03-15T00:00:00+00:00</published>
    <updated>2018-03-15T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Just say hello world!</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Conferences, webinars, podcasts and the likes]]></title>
    <link href="/page/conferences_podcasts_webinars/"/>
    <id>/page/conferences_podcasts_webinars/</id>
    <published>2018-02-01T16:06:06+02:00</published>
    <updated>2018-02-01T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p>Here, you can find a list of all the talks I gave at conferences, webinars, podcasts, workshops, and all the other places you can and could hear me talk. :-)</p>

<h2 id="workshops-i-am-giving">Workshops I am giving</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/11/deep_learning_keras_tensorflow/">Workshop on Deep Learning with Keras and TensorFlow in R</a></li>
</ul>

<blockquote>
<p>I offer a workshop on deep learning with Keras and TensorFlow using R.
Date and place depend on who and how many people are interested, so please contact me either directly or via the workshop page: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/</a> (the description is in German but I also offer to give the workshop in English).</p>
</blockquote>

<h2 id="upcoming-talks-webinars-podcasts-etc">Upcoming talks, webinars, podcasts, etc.</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/m3_2018/">Announcing my talk about explainability of machine learning models at Minds Mastering Machines Conference</a></li>
</ul>

<blockquote>
<p>On Wednesday, April 25th 2018 I am going to talk about explainability of machine learning models at the Minds Mastering Machines conference in Cologne.</p>
</blockquote>

<ul>
<li><a href="JAX 2018 talk announcement: Deep Learning - a Primer">JAX 2018 talk announcement: Deep Learning - a Primer</a></li>
</ul>

<blockquote>
<p>Deep Learning is one of the &ldquo;hot&rdquo; topics in the AI area ‚Äì a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become &ldquo;Software 2.0&rdquo;, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>

<h2 id="past-talks-webinars-podcasts-etc">Past talks, webinars, podcasts, etc.</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/herr_mies_wills_wissen/">I talk about machine learning with Daniel Mies (Podcast in German, though)</a></li>
</ul>

<blockquote>
<p>In January 2018 I was interviewed for a tech podcast where I talked about machine learning, neural nets, why I love R and Rstudio and how I became a Data Scientist.</p>
</blockquote>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/">Explaining Predictions of Machine Learning Models with LIME - M√ºnster Data Science Meetup</a></li>
</ul>

<blockquote>
<p>In December 2017 I talked about Explaining Predictions of Machine Learning Models with LIME at the M√ºnster Data Science Meetup.</p>
</blockquote>

<ul>
<li><a href="https://shiring.github.io/blogging/2017/09/20/webinar_biology_to_data_science">From Biology to Industry. A Blogger‚Äôs Journey to Data Science</a></li>
</ul>

<blockquote>
<p>In September 2017 I gave a webinar for the Applied Epidemiology Didactic of the University of Wisconsin - Madison titled ‚ÄúFrom Biology to Industry. A Blogger‚Äôs Journey to Data Science.‚Äù
I talked about how blogging about R and Data Science helped me become a Data Scientist. I also gave a short introduction to Machine Learning, Big Data and Neural Networks.</p>
</blockquote>

<ul>
<li><a href="https://shiring.github.io/machine_learning/2017/03/31/webinar_code">Building meaningful machine learning models for disease prediction</a></li>
</ul>

<blockquote>
<p>In March 2017 I gave a webinar for the ISDS R Group about my work on building machine-learning models to predict the course of different diseases. I went over building a model, evaluating its performance, and answering or addressing different disease related questions using machine learning. My talk covered the theory of machine learning as it is applied using R.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Welcome to my page!]]></title>
    <link href="/page/about/"/>
    <id>/page/about/</id>
    <published>2017-09-12T16:06:06+02:00</published>
    <updated>2017-09-12T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p><img src="/img/Bewerbungsfoto_klein.jpg" alt="" /></p>

<p>I&rsquo;m Shirin, a biologist turned bioinformatician turned data scientist.</p>

<p>I&rsquo;m especially interested in machine learning and data visualization. While I am using R most every day at work, I wanted to have an incentive to regularly explore other types of analyses and other types of data that I don&rsquo;t normally work with. I have also very often benefited from other people&rsquo;s published code in that it gave me ideas for my own work; and I hope that sharing my own analyses will inspire others as much as I often am by what can be be done with data.  It&rsquo;s amazing to me what can be learned from analyzing and visualizing data!</p>

<p>My tool of choice for data analysis so far has been R. I also organize the <a href="https://shiring.github.io/r_users_group/2017/05/20/muenster_r_user_group">M√ºnsteR R-users group on meetup.com</a>.</p>

<p><img src="http://res.cloudinary.com/shiring/image/upload/v1511852499/my_story_wml3zm.png" alt="My journey to Data Science" /></p>

<p>I love dancing and used to do competitive ballroom and latin dancing. Even though I don&rsquo;t have time for that anymore, I still enjoy teaching &ldquo;social dances&rdquo; once a week with the Hochschulsport (university sports courses).</p>

<p>I created the R package <a href="https://github.com/ShirinG/exprAnalysis">exprAnalysis</a>, designed to streamline my RNA-seq data analysis pipeline. It is available via Github. Instructions for installation and usage can be found <a href="https://shiring.github.io/rna-seq/microarray/2016/09/28/exprAnalysis">here</a>.</p>

<p>This blog will showcase some of the analyses I have been doing with different data sets (all freely available). I will also host teaching materials for students to access in conjunction with R courses I am giving.</p>

<hr />

<h2 id="contact-me">Contact me:</h2>

<ul>
<li><a href="https://www.codecentric.de/team/shirin-glander/">Codecentric AG</a></li>
<li><a href="mailto:shirin.glander@gmail.com">Email</a></li>
<li><a href="http://www.xing.com/profile/Shirin_Glander">Xing</a></li>
<li><a href="http://de.linkedin.com/in/shirin-glander-01120881">Linkedin</a></li>
<li><a href="http://twitter.com/ShirinGlander">Twitter</a></li>
</ul>

<hr />

<p>Also check out <a href="http://www.R-bloggers.com">R-bloggers</a> for lots of cool R stuff!</p>
]]></content>
  </entry>
</feed>