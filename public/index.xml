<feed xmlns="http://www.w3.org/2005/Atom">
  <title>student zero </title>
  <link href="/index.xml" rel="self"/>
  <link href="/"/>
  <updated>2018-03-31T00:00:00+00:00</updated>
  <id>/</id>
  <author>
    <name>Jixing Liu</name>
  </author>
  <generator>Hugo -- gohugo.io</generator>
  <entry>
    <title type="html"><![CDATA[8 How To Evaluate Performance Of Multiple Machine Learning Algorithms?]]></title>
    <link href="/2018/03/8-how-to-evaluate-performance-of-multiple-machine-learning-algorithms/"/>
    <id>/2018/03/8-how-to-evaluate-performance-of-multiple-machine-learning-algorithms/</id>
    <published>2018-03-31T00:00:00+00:00</published>
    <updated>2018-03-31T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code>load(&quot;../../data/craet_7.Rdata&quot;)
library(tidyverse)
library(caret)
#Set Parallel Processing - Decrease computation time
if (!require(&quot;doMC&quot;)) install.packages(&quot;doMC&quot;)
library(doMC)
registerDoMC(cores = 4)</code></pre>
</div>
<div id="caret-provides-the-resamples-function-where-you-can-provide-multiple-machine-learning-models-and-collectively-evaluate-them" class="section level1">
<h1>Caret provides the resamples() function where you can provide multiple machine learning models and collectively evaluate them</h1>
<div id="define-the-training-control" class="section level2">
<h2>Define the training control</h2>
<pre class="r"><code>fitControl &lt;- trainControl(
    method = &#39;cv&#39;,                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = &#39;final&#39;,       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) </code></pre>
</div>
<div id="train-models" class="section level2">
<h2>train models</h2>
<pre class="r"><code>set.seed(100)

# Training Adaboost using adaboost
model_adaboost = train(Purchase ~ ., data=trainData, method=&#39;adaboost&#39;, tuneLength=2, trControl = fitControl)

# Training Random Forest model using rf
model_rf = train(Purchase ~ ., data=trainData, method=&#39;rf&#39;, tuneLength=5, trControl = fitControl)

# Training xgBoost Dart
#model_xgbDART = train(Purchase ~ ., data=trainData, method=&#39;xgbDART&#39;, tuneLength=5, trControl = fitControl, verbose=F)

# Train SVM using MARS
model_svmRadial = train(Purchase ~ ., data=trainData, method=&#39;svmRadial&#39;, tuneLength=15, trControl = fitControl)</code></pre>
</div>
</div>
<div id="run-resamples-to-compare-the-models" class="section level1">
<h1>Run resamples() to compare the models</h1>
<pre class="r"><code># Compare model performances using resample()
models_compare &lt;- resamples(list(ADABOOST=model_adaboost, RF=model_rf, MARS=model_mars3, SVM=model_svmRadial))

# Summary of the models performances
summary(models_compare)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = models_compare)
## 
## Models: ADABOOST, RF, MARS, SVM 
## Number of resamples: 5 
## 
## ROC 
##               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## ADABOOST 0.8126510 0.8462687 0.8682549 0.8657598 0.8868515 0.9147727    0
## RF       0.8635394 0.8647908 0.8748565 0.8841388 0.9046198 0.9128875    0
## MARS     0.8520967 0.8660981 0.9091561 0.8953757 0.9118590 0.9376688    0
## SVM      0.8769723 0.8839375 0.8902597 0.8895479 0.8948048 0.9017652    0
## 
## Sens 
##               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## ADABOOST 0.7619048 0.7904762 0.7904762 0.8070330 0.8076923 0.8846154    0
## RF       0.7809524 0.8000000 0.8461538 0.8451832 0.8750000 0.9238095    0
## MARS     0.8190476 0.8476190 0.8857143 0.8739377 0.8942308 0.9230769    0
## SVM      0.8750000 0.8761905 0.8761905 0.8891209 0.9047619 0.9134615    0
## 
## Spec 
##               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## ADABOOST 0.7014925 0.7462687 0.7727273 0.7635007 0.7761194 0.8208955    0
## RF       0.6119403 0.6363636 0.7462687 0.7332429 0.8208955 0.8507463    0
## MARS     0.6567164 0.7164179 0.7313433 0.7454998 0.7424242 0.8805970    0
## SVM      0.6969697 0.7164179 0.7313433 0.7364089 0.7611940 0.7761194    0</code></pre>
<pre class="r"><code># Draw box plots to compare models
scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;))
bwplot(models_compare, scales=scales)</code></pre>
<p><img src="/post/2018-03-31-8-how-to-evaluate-performance-of-multiple-machine-learning-algorithms_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>save.image(&quot;../../data/craet_8.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[7 How To Do Hyperparameter Tuning ]]></title>
    <link href="/2018/03/7-how-to-do-hyperparameter-tuning/"/>
    <id>/2018/03/7-how-to-do-hyperparameter-tuning/</id>
    <published>2018-03-30T00:00:00+00:00</published>
    <updated>2018-03-30T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code>load(&quot;../../data/craet_6.Rdata&quot;)
library(tidyverse)
library(caret)
# Set Parallel Processing - Decrease computation time
if (!require(&quot;doMC&quot;)) install.packages(&quot;doMC&quot;)
library(doMC)
registerDoMC(cores = 4)</code></pre>
<div id="hyper-parameter-tuning-using-tunegrid" class="section level2">
<h2>Hyper parameter tuning using tuneGrid</h2>
<ol style="list-style-type: decimal">
<li><p>Model Tuning Parameter Set</p></li>
<li><p>Cross Validation Set</p>
<strong>Cross validation <code>method</code> can be one amongst</strong>:
<ul>
<li>‘boot’: Bootstrap sampling</li>
<li>‘boot632’: Bootstrap sampling with 63.2% bias correction applied</li>
<li>‘optimism_boot’: The optimism bootstrap estimator</li>
<li>‘boot_all’: All boot methods.</li>
<li>‘cv’: k-Fold cross validation</li>
<li>‘repeatedcv’: Repeated k-Fold cross validation</li>
<li>‘oob’: Out of Bag cross validation</li>
<li>‘LOOCV’: Leave one out cross validation</li>
<li>‘LGOCV’: Leave group out cross validation</li>
</ul></li>
<li><p>Training And Tuning</p></li>
<li><p>Predict</p></li>
<li><p>Confusion Matrix</p></li>
</ol>
<pre class="r"><code># Step 1: Define the tuneGrid
marsGrid &lt;-  expand.grid(nprune = c(2, 4, 6, 8, 10), 
                        degree = c(1, 2, 3))

# Step 2: Define the training control
fitControl &lt;- trainControl(
    method = &#39;cv&#39;,                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = &#39;final&#39;,       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 

# Step 3: Training and Tuning hyper parameters by setting tuneGrid
set.seed(100)
model_mars3 = train(Purchase ~ ., data=trainData, method=&#39;earth&#39;, metric=&#39;ROC&#39;, tuneGrid = marsGrid, trControl = fitControl)
model_mars3</code></pre>
<pre><code>## Multivariate Adaptive Regression Spline 
## 
## 857 samples
##  18 predictor
##   2 classes: &#39;CH&#39;, &#39;MM&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 685, 685, 687, 686, 685 
## Resampling results across tuning parameters:
## 
##   degree  nprune  ROC        Sens       Spec     
##   1        2      0.8745398  0.8700916  0.7006784
##   1        4      0.8924657  0.8662454  0.7394844
##   1        6      0.8912361  0.8719414  0.7334238
##   1        8      0.8886974  0.8661722  0.7334238
##   1       10      0.8879988  0.8623626  0.7423790
##   2        2      0.8745398  0.8700916  0.7006784
##   2        4      0.8953757  0.8739377  0.7454998
##   2        6      0.8917824  0.8681868  0.7515152
##   2        8      0.8904559  0.8624359  0.7574401
##   2       10      0.8932377  0.8547436  0.7784261
##   3        2      0.8582783  0.8777106  0.6618725
##   3        4      0.8914544  0.8662454  0.7544550
##   3        6      0.8910605  0.8586264  0.7665310
##   3        8      0.8838647  0.8452015  0.7456355
##   3       10      0.8827056  0.8471062  0.7426504
## 
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were nprune = 4 and degree = 2.</code></pre>
<pre class="r"><code># Step 4: Predict on testData 
predicted3 &lt;- predict(model_mars3, testData4)

# Step 5: Compute the confusion matrix
confusionMatrix(reference = testData$Purchase, data = predicted3, mode=&#39;everything&#39;, positive=&#39;MM&#39;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  CH  MM
##         CH 117  21
##         MM  13  62
##                                           
##                Accuracy : 0.8404          
##                  95% CI : (0.7841, 0.8869)
##     No Information Rate : 0.6103          
##     P-Value [Acc &gt; NIR] : 2.164e-13       
##                                           
##                   Kappa : 0.6585          
##  Mcnemar&#39;s Test P-Value : 0.2299          
##                                           
##             Sensitivity : 0.7470          
##             Specificity : 0.9000          
##          Pos Pred Value : 0.8267          
##          Neg Pred Value : 0.8478          
##               Precision : 0.8267          
##                  Recall : 0.7470          
##                      F1 : 0.7848          
##              Prevalence : 0.3897          
##          Detection Rate : 0.2911          
##    Detection Prevalence : 0.3521          
##       Balanced Accuracy : 0.8235          
##                                           
##        &#39;Positive&#39; Class : MM              
## </code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  methods   stats     graphics  grDevices utils     datasets 
## [8] base     
## 
## other attached packages:
##  [1] earth_4.6.1        plotmo_3.3.5       TeachingDemos_2.10
##  [4] plotrix_3.7        doMC_1.3.5         iterators_1.0.9   
##  [7] foreach_1.4.4      caret_6.0-78       lattice_0.20-35   
## [10] forcats_0.3.0      stringr_1.3.0      dplyr_0.7.4       
## [13] purrr_0.2.4        readr_1.1.1        tidyr_0.8.0       
## [16] tibble_1.4.2       ggplot2_2.2.1      tidyverse_1.2.1   
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-131.1     lubridate_1.7.3    dimRed_0.1.0      
##  [4] httr_1.3.1         rprojroot_1.3-2    tools_3.4.3       
##  [7] backports_1.1.2    R6_2.2.2           rpart_4.1-13      
## [10] lazyeval_0.2.1     colorspace_1.3-2   nnet_7.3-12       
## [13] withr_2.1.1.9000   tidyselect_0.2.4   mnormt_1.5-5      
## [16] compiler_3.4.3     cli_1.0.0          rvest_0.3.2       
## [19] xml2_1.2.0         bookdown_0.7       scales_0.5.0.9000 
## [22] sfsmisc_1.1-2      DEoptimR_1.0-8     psych_1.7.8       
## [25] robustbase_0.92-8  digest_0.6.15      foreign_0.8-69    
## [28] rmarkdown_1.9      pkgconfig_2.0.1    htmltools_0.3.6   
## [31] rlang_0.2.0.9000   readxl_1.0.0       ddalpha_1.3.1.1   
## [34] bindr_0.1.1        jsonlite_1.5       ModelMetrics_1.1.0
## [37] magrittr_1.5       Matrix_1.2-12      Rcpp_0.12.16      
## [40] munsell_0.4.3      stringi_1.1.7      yaml_2.1.18       
## [43] MASS_7.3-49        plyr_1.8.4         recipes_0.1.2     
## [46] grid_3.4.3         crayon_1.3.4       haven_1.1.1       
## [49] splines_3.4.3      hms_0.4.2          knitr_1.20        
## [52] pillar_1.2.1       reshape2_1.4.3     codetools_0.2-15  
## [55] stats4_3.4.3       CVST_0.2-1         glue_1.2.0        
## [58] evaluate_0.10.1    blogdown_0.5       modelr_0.1.1      
## [61] cellranger_1.1.0   gtable_0.2.0       kernlab_0.9-25    
## [64] assertthat_0.2.0   DRR_0.0.3          xfun_0.1          
## [67] gower_0.1.2        prodlim_1.6.1      broom_0.4.3       
## [70] e1071_1.6-8        class_7.3-14       survival_2.41-3   
## [73] timeDate_3043.102  RcppRoll_0.2.2     bindrcpp_0.2      
## [76] lava_1.6           ipred_0.9-6</code></pre>
<pre class="r"><code>save.image(file = &quot;../../data/craet_7.Rdata&quot;)</code></pre>
</div>
<div id="links" class="section level2">
<h2><strong>Links</strong></h2>
<ul>
<li><a href="https://www.machinelearningplus.com/caret-package/#7howtodohyperparametertuningtooptimizethemodelforbetterperformance?">How to do hyperparameter tuning to optimize the model for better performance?</a></li>
</ul>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[6 Training and Tuning the model]]></title>
    <link href="/2018/03/6-training-and-tuning-the-model/"/>
    <id>/2018/03/6-training-and-tuning-the-model/</id>
    <published>2018-03-29T00:00:00+00:00</published>
    <updated>2018-03-29T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
</div>
<div id="training" class="section level1">
<h1>Training</h1>
<div id="how-to-train-the-model-and-interpret-the-results" class="section level2">
<h2>1. How to train the model and interpret the results?</h2>
<p>Once you have chosen an algorithm, building the model is fairly easy using the train() function</p>
<p><code>train()</code> does multiple other things like:</p>
<ol style="list-style-type: decimal">
<li><em>Cross validating the model</em></li>
<li><em>Tune the hyper parameters for optimal model performance</em></li>
<li><em>Choose the optimal model based on a given evaluation metric</em></li>
<li><em>Preprocess the predictors (what we did so far using preProcess())</em></li>
</ol>
</div>
<div id="how-to-compute-variable-importance" class="section level2">
<h2>2. How to compute variable importance?</h2>
<p>Which variables came out to be useful?</p>
</div>
</div>
<div id="tuning" class="section level1">
<h1>Tuning</h1>
<div id="preprocess-the-test-dataset-and-predict" class="section level2">
<h2>1. Preprocess the test dataset and predict</h2>
<p>The pre-processing in the following sequence:</p>
<p><strong>Missing Value imputation –&gt; One-Hot Encoding –&gt; Range Normalization</strong></p>
<p><strong>All the information required for pre-processing is stored in the respective preProcess model and dummyVar model.</strong></p>
<p>pass the testData through these models in the same sequence:</p>
<p><strong>preProcess_missingdata_model –&gt; dummies_model –&gt; preProcess_range_model</strong></p>
</div>
<div id="predict-on-testdata-and-confusion-matrix" class="section level2">
<h2>2. Predict on testData and Confusion Matrix</h2>
</div>
<div id="reference" class="section level2">
<h2><strong>Reference</strong></h2>
<p><a href="https://www.machinelearningplus.com/caret-package/">Traing and Tuning model</a></p>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[5 How to do feature selection using recursive feature elimination]]></title>
    <link href="/2018/03/5-how-to-do-feature-selection-using-recursive-feature-elimination/"/>
    <id>/2018/03/5-how-to-do-feature-selection-using-recursive-feature-elimination/</id>
    <published>2018-03-28T00:00:00+00:00</published>
    <updated>2018-03-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>You might need <em>a rigorous way to determine the important variables</em> first before feeding them to the ML algorithm. This is important.</p>
<p>A good choice of selecting the important features is the <em>recursive feature elimination (RFE)</em></p>
<p>RFE works in 3 broad steps:</p>
<p>Step 1: Build a ML model on a training dataset and estimate the feature importances on the test dataset.（在确定自由度的情况下，评价变量在测试数据集中的重要性）</p>
<p>Step 2: Keeping priority to the most important variables, iterate through by building models of given sizes. Ranking of the predictors is recalculated in each iteration.（把刚才的过程在不同的自由度下迭代执行）</p>
<p>Step 3: The model performances are compared across different subset sizes to arrive at the optimal number and list of final predictors.（比较不同自由度的测试错误率，给出最佳自由度模型选择）</p>
<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code># Load Package And Data
load(&quot;../../data/craet_4.Rdata&quot;)
library(tidyverse)
library(caret)
#Set Parallel Processing - Decrease computation time
if (!require(&quot;doMC&quot;)) install.packages(&quot;doMC&quot;)
library(doMC)
registerDoMC(cores = 4)</code></pre>
</div>
<div id="feature-select" class="section level1">
<h1>Feature select</h1>
<pre class="r"><code>set.seed(100)
options(warn=-1)

subsets &lt;- c(1:5, 10, 15, 18)

#Step 1: Build a ML model on a training dataset and estimate the feature importances on the test dataset.（在确定自由度的情况下，评价变量在测试数据集中的重要性）
ctrl &lt;- rfeControl(functions = rfFuncs,
                   method = &quot;repeatedcv&quot;,#repeated K-fold cross-validation
                   number = 10,#10-fold cross-validations
                   repeats = 5, #five separate 10-fold cross-validations are used
                   verbose = FALSE)
#Step 2: Keeping priority to the most important variables, iterate through by building models of given sizes. Ranking of the predictors is recalculated in each iteration.（把刚才的过程在不同的自由度下迭代执行
lmProfile &lt;- rfe(x=trainData[, 1:18], y=trainData$Purchase,
                 sizes = subsets,
                 rfeControl = ctrl)

#Step 3: The model performances are compared across different subset sizes to arrive at the optimal number and list of final predictors.（比较不同自由度的测试错误率，给出最佳自由度模型选择
lmProfile</code></pre>
<pre><code>## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (10 fold, repeated 5 times) 
## 
## Resampling performance over subset size:
## 
##  Variables Accuracy  Kappa AccuracySD KappaSD Selected
##          1   0.7442 0.4569    0.04125 0.08753         
##          2   0.8124 0.6031    0.04002 0.08505         
##          3   0.8182 0.6136    0.04170 0.08790        *
##          4   0.8047 0.5879    0.04314 0.08993         
##          5   0.8000 0.5770    0.04215 0.08861         
##         10   0.8035 0.5826    0.04112 0.08815         
##         15   0.8089 0.5918    0.04209 0.09076         
##         18   0.8084 0.5918    0.04118 0.08894         
## 
## The top 3 variables (out of 3):
##    LoyalCH, PriceDiff, StoreID</code></pre>
<div id="input" class="section level2">
<h2>input</h2>
<ul>
<li><p>Size: sizes determines what all model sizes (the number of most important features) the rfe should consider</p></li>
<li>rfeControl():
<ul>
<li>functions: what type of algorithm should be used <strong>rfFuncs:: random forest based</strong>
<ul>
<li>methods: repeated K-fold cross-validation</li>
<li>number: 10-fold cross-validations</li>
<li>repeats: five separate 10-fold cross-validations are used</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="output" class="section level2">
<h2>output</h2>
<p>The Output Shows: - accuracy<br />
- kappa (and their standard deviation) for the different model sizes we provided - The final selected model subset size is marked with a * in the rightmost Selected column.</p>
<pre class="r"><code>save.image(&quot;../../data/craet_5.Rdata&quot;)
sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  methods   stats     graphics  grDevices utils     datasets 
## [8] base     
## 
## other attached packages:
##  [1] doMC_1.3.5      iterators_1.0.9 foreach_1.4.4   caret_6.0-78   
##  [5] lattice_0.20-35 forcats_0.3.0   stringr_1.3.0   dplyr_0.7.4    
##  [9] purrr_0.2.4     readr_1.1.1     tidyr_0.8.0     tibble_1.4.2   
## [13] ggplot2_2.2.1   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##  [1] httr_1.3.1          ddalpha_1.3.1.1     sfsmisc_1.1-2      
##  [4] jsonlite_1.5        splines_3.4.3       prodlim_1.6.1      
##  [7] modelr_0.1.1        assertthat_0.2.0    stats4_3.4.3       
## [10] DRR_0.0.3           cellranger_1.1.0    yaml_2.1.18        
## [13] robustbase_0.92-8   ipred_0.9-6         pillar_1.2.1       
## [16] backports_1.1.2     glue_1.2.0          digest_0.6.15      
## [19] randomForest_4.6-12 rvest_0.3.2         colorspace_1.3-2   
## [22] recipes_0.1.2       htmltools_0.3.6     Matrix_1.2-12      
## [25] plyr_1.8.4          psych_1.7.8         timeDate_3043.102  
## [28] pkgconfig_2.0.1     CVST_0.2-1          broom_0.4.3        
## [31] haven_1.1.1         bookdown_0.7        scales_0.5.0.9000  
## [34] gower_0.1.2         lava_1.6            withr_2.1.1.9000   
## [37] nnet_7.3-12         lazyeval_0.2.1      cli_1.0.0          
## [40] mnormt_1.5-5        survival_2.41-3     magrittr_1.5       
## [43] crayon_1.3.4        readxl_1.0.0        evaluate_0.10.1    
## [46] nlme_3.1-131.1      MASS_7.3-49         xml2_1.2.0         
## [49] dimRed_0.1.0        foreign_0.8-69      class_7.3-14       
## [52] blogdown_0.5        tools_3.4.3         hms_0.4.2          
## [55] kernlab_0.9-25      munsell_0.4.3       bindrcpp_0.2       
## [58] e1071_1.6-8         compiler_3.4.3      RcppRoll_0.2.2     
## [61] rlang_0.2.0.9000    grid_3.4.3          rmarkdown_1.9      
## [64] gtable_0.2.0        ModelMetrics_1.1.0  codetools_0.2-15   
## [67] reshape2_1.4.3      R6_2.2.2            lubridate_1.7.3    
## [70] knitr_1.20          bindr_0.1.1         rprojroot_1.3-2    
## [73] stringi_1.1.7       Rcpp_0.12.16        rpart_4.1-13       
## [76] tidyselect_0.2.4    DEoptimR_1.0-8      xfun_0.1</code></pre>
</div>
<div id="reference" class="section level2">
<h2><strong>Reference</strong></h2>
<ul>
<li><a href="https://www.machinelearningplus.com/caret-package/">How to do feature selection using recursive feature elimination (rfe)?</a></li>
</ul>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[4 How To Visualize The Importance Of Variables Using featurePlot]]></title>
    <link href="/2018/03/4-how-to-visualize-the-importance-of-variables-using-featureplot/"/>
    <id>/2018/03/4-how-to-visualize-the-importance-of-variables-using-featureplot/</id>
    <published>2018-03-27T00:00:00+00:00</published>
    <updated>2018-03-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code>load(&quot;../../data/craet_3-3.Rdata&quot;)
library(tidyverse)
library(caret)</code></pre>
</div>
<div id="q-how-the-predictors-influence-the-y" class="section level1">
<h1>Q: How The Predictors Influence The Y</h1>
<p>选择重要的变量: 通过观察在Y的分组下各个变量的分布情况</p>
<p>一般有 箱线图 和 密度图</p>
</div>
<div id="box-plot" class="section level1">
<h1>box-plot</h1>
<pre class="r"><code>featurePlot(x = trainData[, 1:18], 
            y = trainData$Purchase, 
            plot = &quot;box&quot;,#&quot;density&quot;
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation=&quot;free&quot;), 
                          y = list(relation=&quot;free&quot;)))</code></pre>
<p><img src="/post/2018-03-27-4-how-to-visualize-the-importance-of-variables-using-featureplot_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="density" class="section level1">
<h1>Density</h1>
<pre class="r"><code>featurePlot(x = trainData[, 1:18], 
            y = trainData$Purchase, 
            plot = &quot;density&quot;,
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation=&quot;free&quot;), 
                          y = list(relation=&quot;free&quot;)))</code></pre>
<p><img src="/post/2018-03-27-4-how-to-visualize-the-importance-of-variables-using-featureplot_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>save.image(&quot;../../data/craet_4.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Scraping and Wranging Tables from Research Articles]]></title>
    <link href="/2018/03/scraping-and-wranging-tables-from-research-articles/"/>
    <id>/2018/03/scraping-and-wranging-tables-from-research-articles/</id>
    <published>2018-03-27T00:00:00+00:00</published>
    <updated>2018-03-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>What do you do when you want to use results from the literature to anchor your own analysis? we’ll go through a practical scenario on scraping an html table from a Nature Genetics article into R and wrangling the data into a useful format.</p>
<div id="scraping-a-html-table-from-a-webpage" class="section level1">
<h1>01. Scraping a html table from a webpage</h1>
<pre class="r"><code>#load packages
library(&quot;rvest&quot;)
library(&quot;knitr&quot;)
library(tidyverse)
#scraping web page
url &lt;- &quot;https://www.nature.com/articles/ng.2802/tables/2&quot;

#====🔥find where is the table lives on this webpage====
table_path=&#39;//*[@id=&quot;content&quot;]/div/div/figure/div[1]/div/div[1]/table&#39;
#get the table
nature_genetics_table2 &lt;- url %&gt;%
  read_html() %&gt;%
  html_nodes(xpath=table_path) %&gt;%
  html_table(fill=T) %&gt;% .[[1]]
#the first few lines of table
kable(nature_genetics_table2[1:4,])</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">SNPa</th>
<th align="left">Chr.</th>
<th align="left">Positionb</th>
<th align="left">Closest genec</th>
<th align="left">Major/minor alleles</th>
<th align="left">MAFd</th>
<th align="left">Stage 1</th>
<th align="left">Stage 1</th>
<th align="left">Stage 2</th>
<th align="left">Stage 2</th>
<th align="left">Overall</th>
<th align="left">Overall</th>
<th align="left">Overall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">SNPa</td>
<td align="left">Chr.</td>
<td align="left">Positionb</td>
<td align="left">Closest genec</td>
<td align="left">Major/minor alleles</td>
<td align="left">MAFd</td>
<td align="left">OR (95% CI)e</td>
<td align="left">Meta P value</td>
<td align="left">OR (95% CI)e</td>
<td align="left">Meta P value</td>
<td align="left">OR (95% CI)e</td>
<td align="left">Meta P value</td>
<td align="left">I2 (%), P valuef</td>
</tr>
<tr class="even">
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="odd">
<td align="left">rs6656401</td>
<td align="left">1</td>
<td align="left">207692049</td>
<td align="left">CR1</td>
<td align="left">G/A</td>
<td align="left">0.197</td>
<td align="left">1.17 (1.12–1.22)</td>
<td align="left">7.7 × 10−15</td>
<td align="left">1.21 (1.14–1.28)</td>
<td align="left">7.9 × 10−11</td>
<td align="left">1.18 (1.14–1.22)</td>
<td align="left">5.7 × 10−24</td>
<td align="left">0, 7.8 × 10−1</td>
</tr>
<tr class="even">
<td align="left">rs6733839</td>
<td align="left">2</td>
<td align="left">127892810</td>
<td align="left">BIN1</td>
<td align="left">C/T</td>
<td align="left">0.409</td>
<td align="left">1.21 (1.17–1.25)</td>
<td align="left">1.7 × 10−26</td>
<td align="left">1.24 (1.18–1.29)</td>
<td align="left">3.4 × 10−19</td>
<td align="left">1.22 (1.18–1.25)</td>
<td align="left">6.9 × 10−44</td>
<td align="left">28, 6.1 × 10−2</td>
</tr>
</tbody>
</table>
</div>
<div id="making-messy-data-useful" class="section level1">
<h1>02 Making messy data useful</h1>
<div id="cleaning-up-the-rows" class="section level2">
<h2>Cleaning up the rows</h2>
<p>All The Elements Of These Rows Contain The Exact Same Text</p>
<pre class="r"><code>v=which(apply(nature_genetics_table2,1, function(x) length(unique(unlist(x))) )==1)
v</code></pre>
<pre><code>## [1]  2 12 18</code></pre>
</div>
<div id="split-table" class="section level2">
<h2>split table</h2>
<pre class="r"><code>nature_genetics_table2_list = split(nature_genetics_table2, cumsum(1:nrow(nature_genetics_table2) %in% v))
nature_genetics_table2_list = lapply(nature_genetics_table2_list[2:4], function(y) {
y$Description = unique(as.character(y[1, ]))
y[-1, ]
})

#rbind three table
nature_genetics_table2_clean = do.call(&quot;rbind&quot;, nature_genetics_table2_list)

kable(nature_genetics_table2_clean[1:3,])</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">SNPa</th>
<th align="left">Chr.</th>
<th align="left">Positionb</th>
<th align="left">Closest genec</th>
<th align="left">Major/minor alleles</th>
<th align="left">MAFd</th>
<th align="left">Stage 1</th>
<th align="left">Stage 1</th>
<th align="left">Stage 2</th>
<th align="left">Stage 2</th>
<th align="left">Overall</th>
<th align="left">Overall</th>
<th align="left">Overall</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1.3</td>
<td align="left">rs6656401</td>
<td align="left">1</td>
<td align="left">207692049</td>
<td align="left">CR1</td>
<td align="left">G/A</td>
<td align="left">0.197</td>
<td align="left">1.17 (1.12–1.22)</td>
<td align="left">7.7 × 10−15</td>
<td align="left">1.21 (1.14–1.28)</td>
<td align="left">7.9 × 10−11</td>
<td align="left">1.18 (1.14–1.22)</td>
<td align="left">5.7 × 10−24</td>
<td align="left">0, 7.8 × 10−1</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="even">
<td>1.4</td>
<td align="left">rs6733839</td>
<td align="left">2</td>
<td align="left">127892810</td>
<td align="left">BIN1</td>
<td align="left">C/T</td>
<td align="left">0.409</td>
<td align="left">1.21 (1.17–1.25)</td>
<td align="left">1.7 × 10−26</td>
<td align="left">1.24 (1.18–1.29)</td>
<td align="left">3.4 × 10−19</td>
<td align="left">1.22 (1.18–1.25)</td>
<td align="left">6.9 × 10−44</td>
<td align="left">28, 6.1 × 10−2</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="odd">
<td>1.5</td>
<td align="left">rs10948363</td>
<td align="left">6</td>
<td align="left">47487762</td>
<td align="left">CD2AP</td>
<td align="left">A/G</td>
<td align="left">0.266</td>
<td align="left">1.10 (1.07–1.14)</td>
<td align="left">3.1 × 10−8</td>
<td align="left">1.09 (1.04–1.15)</td>
<td align="left">4.1 × 10−4</td>
<td align="left">1.10 (1.07–1.13)</td>
<td align="left">5.2 × 10−11</td>
<td align="left">0, 9 × 10−1</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="fixing-column-names" class="section level1">
<h1>03. Fixing column names</h1>
<pre class="r"><code>colnames(nature_genetics_table2_clean) &lt;- c(&quot;SNP&quot;, &quot;Chr&quot;, &quot;Position&quot;, &quot;Closest gene&quot;, &quot;Major/minor alleles&quot;, &quot;MAF&quot;, &quot;Stage1_OR&quot;, &quot;Stage1_MetaP&quot;, &quot;Stage2_OR&quot;,&quot;Stage2_MetaP&quot;,    &quot;Overall_OR&quot;, &quot;Overall_MetaP&quot;, &quot;I2_Percent/P&quot;,&quot;Description&quot;)
colnames(nature_genetics_table2_clean)</code></pre>
<pre><code>##  [1] &quot;SNP&quot;                 &quot;Chr&quot;                 &quot;Position&quot;           
##  [4] &quot;Closest gene&quot;        &quot;Major/minor alleles&quot; &quot;MAF&quot;                
##  [7] &quot;Stage1_OR&quot;           &quot;Stage1_MetaP&quot;        &quot;Stage2_OR&quot;          
## [10] &quot;Stage2_MetaP&quot;        &quot;Overall_OR&quot;          &quot;Overall_MetaP&quot;      
## [13] &quot;I2_Percent/P&quot;        &quot;Description&quot;</code></pre>
</div>
<div id="making-a-character-variable-into-a-numeric-variable" class="section level1">
<h1>04. Making a character variable into a numeric variable</h1>
<pre class="r"><code># &quot; × 10-&quot; -&gt; &quot;e-&quot;
nature_genetics_table2_clean$Stage1_MetaP &lt;- 
str_replace(nature_genetics_table2_clean$Stage1_MetaP,&quot; × 10−&quot;,&quot;e-&quot;) %&gt;% as.numeric()
kable(nature_genetics_table2_clean[1:3,])</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">SNP</th>
<th align="left">Chr</th>
<th align="left">Position</th>
<th align="left">Closest gene</th>
<th align="left">Major/minor alleles</th>
<th align="left">MAF</th>
<th align="left">Stage1_OR</th>
<th align="right">Stage1_MetaP</th>
<th align="left">Stage2_OR</th>
<th align="left">Stage2_MetaP</th>
<th align="left">Overall_OR</th>
<th align="left">Overall_MetaP</th>
<th align="left">I2_Percent/P</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1.3</td>
<td align="left">rs6656401</td>
<td align="left">1</td>
<td align="left">207692049</td>
<td align="left">CR1</td>
<td align="left">G/A</td>
<td align="left">0.197</td>
<td align="left">1.17 (1.12–1.22)</td>
<td align="right">0</td>
<td align="left">1.21 (1.14–1.28)</td>
<td align="left">7.9 × 10−11</td>
<td align="left">1.18 (1.14–1.22)</td>
<td align="left">5.7 × 10−24</td>
<td align="left">0, 7.8 × 10−1</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="even">
<td>1.4</td>
<td align="left">rs6733839</td>
<td align="left">2</td>
<td align="left">127892810</td>
<td align="left">BIN1</td>
<td align="left">C/T</td>
<td align="left">0.409</td>
<td align="left">1.21 (1.17–1.25)</td>
<td align="right">0</td>
<td align="left">1.24 (1.18–1.29)</td>
<td align="left">3.4 × 10−19</td>
<td align="left">1.22 (1.18–1.25)</td>
<td align="left">6.9 × 10−44</td>
<td align="left">28, 6.1 × 10−2</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="odd">
<td>1.5</td>
<td align="left">rs10948363</td>
<td align="left">6</td>
<td align="left">47487762</td>
<td align="left">CD2AP</td>
<td align="left">A/G</td>
<td align="left">0.266</td>
<td align="left">1.10 (1.07–1.14)</td>
<td align="right">0</td>
<td align="left">1.09 (1.04–1.15)</td>
<td align="left">4.1 × 10−4</td>
<td align="left">1.10 (1.07–1.13)</td>
<td align="left">5.2 × 10−11</td>
<td align="left">0, 9 × 10−1</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
</tbody>
</table>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
##  [1] forcats_0.3.0   stringr_1.3.0   dplyr_0.7.4     purrr_0.2.4    
##  [5] readr_1.1.1     tidyr_0.8.0     tibble_1.4.2    ggplot2_2.2.1  
##  [9] tidyverse_1.2.1 knitr_1.20      rvest_0.3.2     xml2_1.2.0     
## 
## loaded via a namespace (and not attached):
##  [1] xfun_0.1          reshape2_1.4.3    haven_1.1.1      
##  [4] lattice_0.20-35   colorspace_1.3-2  htmltools_0.3.6  
##  [7] yaml_2.1.18       rlang_0.2.0.9000  pillar_1.2.1     
## [10] foreign_0.8-69    glue_1.2.0        selectr_0.3-2    
## [13] modelr_0.1.1      readxl_1.0.0      bindrcpp_0.2     
## [16] bindr_0.1.1       plyr_1.8.4        munsell_0.4.3    
## [19] blogdown_0.5      gtable_0.2.0      cellranger_1.1.0 
## [22] psych_1.7.8       evaluate_0.10.1   parallel_3.4.3   
## [25] curl_3.1          highr_0.6         broom_0.4.3      
## [28] Rcpp_0.12.16      backports_1.1.2   scales_0.5.0.9000
## [31] jsonlite_1.5      mnormt_1.5-5      hms_0.4.2        
## [34] digest_0.6.15     stringi_1.1.7     bookdown_0.7     
## [37] grid_3.4.3        rprojroot_1.3-2   cli_1.0.0        
## [40] tools_3.4.3       magrittr_1.5      lazyeval_0.2.1   
## [43] crayon_1.3.4      pkgconfig_2.0.1   lubridate_1.7.3  
## [46] assertthat_0.2.0  rmarkdown_1.9     httr_1.3.1       
## [49] R6_2.2.2          nlme_3.1-131.1    compiler_3.4.3</code></pre>
</div>
<div id="reference" class="section level1">
<h1><strong>Reference</strong></h1>
<ul>
<li><a href="http://research.libd.org/rstatsclub/2018/03/19/introduction-to-scraping-and-wranging-tables-from-research-articles/">Introduction to Scraping and Wranging Tables from Research Articles</a></li>
</ul>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[3.3 How To Create Dummy Variables And Normalization]]></title>
    <link href="/2018/03/how-to-create-dummy-variables-and-normalization/"/>
    <id>/2018/03/how-to-create-dummy-variables-and-normalization/</id>
    <published>2018-03-26T00:00:00+00:00</published>
    <updated>2018-03-26T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code>load(&quot;../../data/craet_3-2.Rdata&quot;)
library(tidyverse)
library(caret)</code></pre>
</div>
<div id="why-dummy-variables" class="section level1">
<h1>Why Dummy Variables</h1>
<p>对于字符型的因子变量，我们需要把它转变为有序的数值，一般转为 0，1 的二变量， 这样0 就代表基础水平， 1代表比较组</p>
<p><img src="https://i.loli.net/2018/03/26/5ab847e351487.jpg" width="30%" style="display: block; margin: auto;" /></p>
</div>
<div id="how" class="section level1">
<h1>How</h1>
<pre class="r"><code># One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model &lt;- dummyVars(Purchase ~ ., data=trainData)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat &lt;- predict(dummies_model, newdata = trainData)

# # Convert to dataframe
trainData &lt;- data.frame(trainData_mat)

# # See the structure of the new dataset
str(trainData)</code></pre>
<pre><code>## &#39;data.frame&#39;:    857 obs. of  18 variables:
##  $ WeekofPurchase: num  -1.1 -1.74 -1.68 -1.29 -1.04 ...
##  $ StoreID       : num  -1.29 -1.29 1.33 1.33 1.33 ...
##  $ PriceCH       : num  -1.14 -1.73 -1.73 -1.14 -1.14 ...
##  $ PriceMM       : num  -0.688 -2.898 -2.898 -0.688 -0.688 ...
##  $ DiscCH        : num  -0.452 -0.452 -0.452 -0.452 -0.452 ...
##  $ DiscMM        : num  -0.582 -0.582 -0.582 1.341 1.341 ...
##  $ SpecialCH     : num  -0.429 -0.429 -0.429 2.329 -0.429 ...
##  $ SpecialMM     : num  -0.42 -0.42 -0.42 -0.42 -0.42 ...
##  $ LoyalCH       : num  -0.205 -0.525 1.256 1.324 1.35 ...
##  $ SalePriceMM   : num  0.113 -1.101 -1.101 -1.506 -1.506 ...
##  $ SalePriceCH   : num  -0.431 -0.844 -0.844 -0.431 -0.431 ...
##  $ PriceDiff     : num  0.341 -0.563 -0.563 -1.165 -1.165 ...
##  $ Store7.No     : num  1 1 0 0 0 0 0 0 0 1 ...
##  $ Store7.Yes    : num  0 0 1 1 1 1 1 1 1 0 ...
##  $ PctDiscMM     : num  -0.588 -0.588 -0.588 1.447 1.447 ...
##  $ PctDiscCH     : num  -0.448 -0.448 -0.448 -0.448 -0.448 ...
##  $ ListPriceDiff : num  0.211 -1.988 -1.988 0.211 0.211 ...
##  $ STORE         : num  -0.457 -0.457 -1.15 -1.15 -1.15 ...</code></pre>
</div>
<div id="why-normalization" class="section level1">
<h1>Why Normalization</h1>
<p>为了消除不同变量由于单位造成的权重影响，我们对数据进行数据标准化</p>
</div>
<div id="how-1" class="section level1">
<h1>How</h1>
<ol style="list-style-type: decimal">
<li><strong>range:</strong> Normalize values so it ranges between 0 and 1</li>
<li><strong>center:</strong> Subtract Mean</li>
<li><strong>scale:</strong> Divide by standard deviation</li>
<li><strong>BoxCox:</strong> Remove skewness leading to normality. Values must be &gt; 0</li>
<li><strong>YeoJohnson:</strong> Like BoxCox, but works for negative values.</li>
<li><strong>expoTrans:</strong> Exponential transformation, works for negative values.</li>
<li><strong>pca:</strong> Replace with principal components</li>
<li><strong>ica:</strong> Replace with independent components</li>
<li><strong>spatialSign:</strong> Project the data to a unit circle</li>
</ol>
<pre class="r"><code>preProcess_range_model &lt;- preProcess(trainData, method=&#39;range&#39;)
trainData &lt;- predict(preProcess_range_model, newdata = trainData)

# Append the Y variable
trainData$Purchase &lt;- y

apply(trainData[, 1:10], 2, FUN=function(x){c(&#39;min&#39;=min(x), &#39;max&#39;=max(x))})</code></pre>
<pre><code>##     WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH
## min              0       0       0       0      0      0         0
## max              1       1       1       1      1      1         1
##     SpecialMM LoyalCH SalePriceMM
## min         0       0           0
## max         1       1           1</code></pre>
<pre class="r"><code>save.image(file = &quot;../../data/craet_3-3.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Why should I trust you 🤖?]]></title>
    <link href="/2018/03/why-should-i-trust-you/"/>
    <id>/2018/03/why-should-i-trust-you/</id>
    <published>2018-03-26T00:00:00+00:00</published>
    <updated>2018-03-26T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>传统的机器学习工作流程主要集中在模型训练和优化上; 最好的模型通常是通过像精度或者错误这样的性能度量来选择的，而且如果它通过了这些性能标准的某些阈值，我们倾向于假设一个模型是足够好的。在机器学习的许多应用中，用户会使用一个模型来帮助做决定， 例如：一位医生不会对病人进行手术，仅仅因为这个模型说不应该进行手术？</p>
<p>由于复杂的机器学习模型本质上是黑盒子，而且太复杂，机器学习模型做出的分类决定通常很难被人类的大脑所理解，但是能够理解和解释这些模型对于提高模型质量，提高信任度和透明度以及减少偏见非常重要，因为人类往往具有良好的直觉和因果推理，这些都是难以在数据评估指标中捕获。</p>
<p>因此，我们希望能够形象的理解它们的工作原理：为什么一个模型将具有特定标签的案例进行准确分类。eg: 为什么一个乳腺肿块样本被归类为“恶性”而不是“良性，仅仅因为它长得丑吗？</p>
<blockquote>
<p><a href="https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime">Local Interpretable Model-Agnostic Explanations (LIME)</a> is an attempt to make these complex models at least partly understandable. The method has been published in <a href="https://arxiv.org/pdf/1602.04938.pdf">“Why Should I Trust You?”</a> Explaining the Predictions of Any Classifier. By Marco Tulio Ribeiro, Sameer Singh and Carlos Guestrin from the University of Washington in Seattle</p>
</blockquote>
<div id="how-lime-works" class="section level2">
<h2><strong>How LIME works</strong></h2>
<blockquote>
<p>lime is able to explain all models for which we can obtain prediction probabilities (in R, that is every model that works with predict(type = “prob”)). It makes use of the fact that linear models are easy to explain because they are based on linear relationships between features and class labels: The complex model function is approximated by locally fitting linear models to permutations of the original training set.On each permutation, a linear model is being fit and weights are given so that incorrect classification of instances that are more similar to the original data are penalized (positive weights support a decision, negative weights contradict them). This will give an approximation of how much (and in which way) each feature contributed to a decision made by the model</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7fa56fd7333.jpg" width="30%" /></p>
<blockquote>
<p>We take the image on the left and divide it into interpretable components</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7faba0d302a.jpg" width="30%" /></p>
<blockquote>
<p>we then generate a data set of perturbed instances by turning some of the interpretable components “oﬀ”. For each perturbed instance, we get the probability that a tree frog is in the image according to the model.</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7fac3ca8787.jpg" width="30%" /></p>
<blockquote>
<p>We then learn a simple (linear) model on this data set, which is locally weighted—that is, we care more about making mistakes in perturbed instances that are more similar to the original image</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7fac8382a86.jpg" width="30%" /></p>
<blockquote>
<p>the end, we present the superpixels with highest positive weights as an explanation, graying out everything else</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7faf16f0320.jpg" width="30%" /></p>
<p>🤖预测这张图是个树蛙是因为这个部分,所以🤖预测结果是比较可信的。 <img src="https://i.loli.net/2018/02/11/5a7faebabe34e.jpg" style="display: block; margin: auto;" /></p>
<p>🤖预测这张图是台球是根据这些部分，所以🤖预测结果是不可信的。 <img src="https://i.loli.net/2018/02/11/5a7fb61ecb060.jpg" style="display: block; margin: auto;" /></p>
</div>
<div id="example-in-r" class="section level2">
<h2><strong>Example in R</strong></h2>
<div id="prepare-the-breast-cancer-data" class="section level3">
<h3>01.Prepare the breast cancer data</h3>
<p>This <strong>data</strong> of example comes from the book of <strong>R in action</strong></p>
<pre class="r"><code>loc &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/&quot;
ds  &lt;- &quot;breast-cancer-wisconsin/breast-cancer-wisconsin.data&quot;
url &lt;- paste(loc, ds, sep=&quot;&quot;)

breast &lt;- read.table(url, sep=&quot;,&quot;, header=FALSE, na.strings=&quot;?&quot;)
names(breast) &lt;- c(&quot;ID&quot;, &quot;clumpThickness&quot;, &quot;sizeUniformity&quot;,
                   &quot;shapeUniformity&quot;, &quot;maginalAdhesion&quot;, 
                   &quot;singleEpithelialCellSize&quot;, &quot;bareNuclei&quot;, 
                   &quot;blandChromatin&quot;, &quot;normalNucleoli&quot;, &quot;mitosis&quot;, &quot;class&quot;)

df &lt;- breast[-1]
df$class &lt;- factor(df$class, levels=c(2,4), 
                   labels=c(&quot;benign&quot;, &quot;malignant&quot;))

set.seed(1234)
train &lt;- sample(nrow(df), 0.7*nrow(df))
df.train &lt;- df[train,]
df.validate &lt;- df[-train,]
table(df.train$class)</code></pre>
<pre><code>## 
##    benign malignant 
##       329       160</code></pre>
<pre class="r"><code>table(df.validate$class)</code></pre>
<pre><code>## 
##    benign malignant 
##       129        81</code></pre>
</div>
<div id="create-decision-tree-model" class="section level3">
<h3>02 Create decision tree model</h3>
<pre class="r"><code>library(rpart)
set.seed(1234)
dtree &lt;- rpart(class ~ ., data=df.train, method=&quot;class&quot;,      
               parms=list(split=&quot;information&quot;))
dtree$cptable</code></pre>
<pre><code>##         CP nsplit rel error  xerror       xstd
## 1 0.800000      0   1.00000 1.00000 0.06484605
## 2 0.046875      1   0.20000 0.30625 0.04150018
## 3 0.012500      3   0.10625 0.20625 0.03467089
## 4 0.010000      4   0.09375 0.18125 0.03264401</code></pre>
<pre class="r"><code>plotcp(dtree)</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>dtree.pruned &lt;- prune(dtree, cp=.0125)</code></pre>
</div>
<div id="predict" class="section level3">
<h3>03 predict</h3>
<pre class="r"><code>dtree.pred &lt;- predict(dtree.pruned, df.validate, type=&quot;class&quot;)
(dtree.perf &lt;- table(df.validate$class, dtree.pred, 
                    dnn=c(&quot;Actual&quot;, &quot;Predicted&quot;)))</code></pre>
<pre><code>##            Predicted
## Actual      benign malignant
##   benign       122         7
##   malignant      2        79</code></pre>
</div>
<div id="plot-decision-tree" class="section level3">
<h3>04 plot decision tree</h3>
<pre class="r"><code>#plot01
library(rpart.plot)
prp(dtree.pruned, type = 2, extra = 104,  
    fallen.leaves = TRUE, main=&quot;Decision Tree&quot;)
#plot02
library(partykit)</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>library(dplyr)
dtree.pruned %&gt;% as.party() %&gt;% plot()</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
</div>
<div id="lime-why-should-i-trust-you" class="section level3">
<h3>LIME: why should I trust you 🤖?</h3>
<pre class="r"><code>library(lime)
explainer &lt;- lime(df.train, model = dtree.pruned)

# Explain new observation
#[model_type](https://github.com/thomasp85/lime/blob/master/R/models.R)
model_type.rpart &lt;- function(x, ...) &#39;classification&#39;#defined model_type method
test.data &lt;- 
  df.validate %&gt;% 
  dplyr::select(-class) %&gt;% 
  head(3)
explanation &lt;- lime::explain(test.data, explainer, n_labels = 1, n_features = 2)

# The output is provided in a consistent tabular format and includes the
# output from the model.
head(explanation)</code></pre>
<pre><code>##       model_type case     label label_prob  model_r2 model_intercept
## 1 classification    3    benign  0.9933993 0.1809444      0.05042991
## 2 classification    3    benign  0.9933993 0.1809444      0.05042991
## 3 classification    4 malignant  0.9507042 0.1918642      0.54743276
## 4 classification    4 malignant  0.9507042 0.1918642      0.54743276
## 5 classification    5    benign  0.9933993 0.1924691      0.05378321
## 6 classification    5    benign  0.9933993 0.1924691      0.05378321
##   model_prediction                  feature feature_value feature_weight
## 1        0.4637659           blandChromatin             3   -0.001977357
## 2        0.4637659           sizeUniformity             1    0.415313316
## 3        0.9524157 singleEpithelialCellSize             3    0.002885248
## 4        0.9524157           sizeUniformity             8    0.402097681
## 5        0.4712710          maginalAdhesion             3   -0.004533032
## 6        0.4712710           sizeUniformity             1    0.422020822
##                        feature_desc                      data
## 1           2 &lt; blandChromatin &lt;= 3 3, 1, 1, 1, 2, 2, 3, 1, 1
## 2               sizeUniformity &lt;= 4 3, 1, 1, 1, 2, 2, 3, 1, 1
## 3 2 &lt; singleEpithelialCellSize &lt;= 4 6, 8, 8, 1, 3, 4, 3, 7, 1
## 4                4 &lt; sizeUniformity 6, 8, 8, 1, 3, 4, 3, 7, 1
## 5              maginalAdhesion &lt;= 3 4, 1, 1, 3, 2, 1, 3, 1, 1
## 6               sizeUniformity &lt;= 4 4, 1, 1, 3, 2, 1, 3, 1, 1
##               prediction
## 1 0.99339934, 0.00660066
## 2 0.99339934, 0.00660066
## 3 0.04929577, 0.95070423
## 4 0.04929577, 0.95070423
## 5 0.99339934, 0.00660066
## 6 0.99339934, 0.00660066</code></pre>
<pre class="r"><code># And can be visualised directly
plot_features(explanation)</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
</div>
<div id="links" class="section level2">
<h2><strong>Links</strong></h2>
<ul>
<li><a href="http://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease">CKD data set</a></li>
<li><a href="https://github.com/marcotcr/lime">open-source Python code for LIME</a></li>
<li><a href="https://github.com/thomasp85/lime">R package for LIME</a></li>
</ul>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[3.2 statistic description and impute missing value]]></title>
    <link href="/2018/03/3-2-statistic-description-and-impute-missing-value/"/>
    <id>/2018/03/3-2-statistic-description-and-impute-missing-value/</id>
    <published>2018-03-24T00:00:00+00:00</published>
    <updated>2018-03-24T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="section level1">
<h1>加载数据和包</h1>
<pre class="r"><code>load(&quot;../../data/caret.Rdata&quot;)
library(tidyverse)
library(caret)</code></pre>
<p>在进行数据整理之前 我们先看看训练数据的统计描述</p>
<p><code>skimr</code>包对列的统计提供了方便的函数</p>
<p><code>skimr::skim_to_wide()</code> 输出一个包含列统计描述的数据框</p>
<pre class="r"><code>library(skimr)
skimmed &lt;- skim_to_wide(trainData)
skimmed[, c(1:5, 9:11, 13, 15:16)]</code></pre>
<pre><code>## # A tibble: 18 x 11
##    type   variable  missing complete n     mean   sd    p0    median p100 
##    &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;
##  1 factor Purchase  0       857      857   &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt; 
##  2 factor Store7    0       857      857   &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt; 
##  3 integ… SpecialCH 2       855      857   &quot;  0.… &quot; 0.… 0     0      1    
##  4 integ… SpecialMM 4       853      857   &quot;  0.… &quot; 0.… 0     0      1    
##  5 integ… STORE     2       855      857   &quot;  1.… &quot; 1.… 0     2      4    
##  6 integ… StoreID   1       856      857   &quot;  3.… &quot; 2.… 1     3      7    
##  7 integ… WeekofPu… 0       857      857   254.17 15.59 227   257    278  
##  8 numer… DiscCH    2       855      857   0.054  &quot;0.1… &quot; 0 … &quot;0   &quot; &quot;0.5…
##  9 numer… DiscMM    3       854      857   &quot;0.12… &quot;0.2… &quot; 0 … &quot;0   &quot; &quot;0.8…
## 10 numer… ListPric… 0       857      857   &quot;0.22… &quot;0.1… &quot; 0 … 0.24   0.44 
## 11 numer… LoyalCH   5       852      857   &quot;0.56… &quot;0.3… &quot; 1.… &quot;0.6 &quot; &quot;1  …
## 12 numer… PctDiscCH 2       855      857   0.028  0.063 &quot; 0 … &quot;0   &quot; 0.25 
## 13 numer… PctDiscMM 2       855      857   0.058  0.099 &quot; 0 … &quot;0   &quot; &quot;0.4…
## 14 numer… PriceCH   1       856      857   &quot;1.87… &quot;0.1… &quot; 1.… 1.86   2.09 
## 15 numer… PriceDiff 1       856      857   &quot;0.15… &quot;0.2… &quot;-0.… 0.23   0.64 
## 16 numer… PriceMM   1       856      857   &quot;2.08… &quot;0.1… &quot; 1.… 2.09   2.29 
## 17 numer… SalePric… 1       856      857   &quot;1.81… &quot;0.1… &quot; 1.… 1.86   2.09 
## 18 numer… SalePric… 3       854      857   &quot;1.96… &quot;0.2… &quot; 1.… 2.09   2.29 
## # ... with 1 more variable: hist &lt;chr&gt;</code></pre>
</div>
<div class="section level1">
<h1>插入数据</h1>
<p>Caret 提供了一个很方便的函数 <code>preProcess()</code></p>
<ul>
<li>设置 <code>method=knnImpute</code> 生成一个模型</li>
<li>使用 <code>predict()</code> 对数据进行插入</li>
</ul>
<pre class="r"><code># Create the knn imputation model on the training data
preProcess_missingdata_model &lt;- preProcess(trainData, method=&#39;knnImpute&#39;)
preProcess_missingdata_model</code></pre>
<pre><code>## Created from 828 samples and 18 variables
## 
## Pre-processing:
##   - centered (16)
##   - ignored (2)
##   - 5 nearest neighbor imputation (16)
##   - scaled (16)</code></pre>
<pre class="r"><code># Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData &lt;- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)</code></pre>
<pre><code>## [1] FALSE</code></pre>
<pre class="r"><code>save.image(file = &quot;../../data/craet_3-2.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[3.1 How to split the dataset into training and validation?]]></title>
    <link href="/2018/03/3-1-how-to-split-the-dataset-into-training-and-validation/"/>
    <id>/2018/03/3-1-how-to-split-the-dataset-into-training-and-validation/</id>
    <published>2018-03-23T00:00:00+00:00</published>
    <updated>2018-03-23T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>数据准备好了之后的第一步就是拆分数据集为训练数据和测试数据，一般是 8:2 的比例。</p>
<p>为什么拆分数据呢？</p>
<p>当我们在构建一个机器学习模型上时，真正的目的是为了预测真是世界的数据，而机器学习模型是依靠算法学习训练数据学习Y 与 X 的关系，这种的关系的学习好坏的评判是要依靠没有参与学习模型的数据与预测数据之间的差距来评判的。</p>
<pre class="r"><code># Load the caret package
library(caret)

# Import dataset
orange &lt;- read.csv(&#39;../../data/orange_juice_withmissing.csv&#39;)
# Create the training and test datasets
set.seed(100)

# Step 1: Get row numbers for the training data
trainRowNumbers &lt;- createDataPartition(orange$Purchase, p=0.8, list=FALSE)

# Step 2: Create the training  dataset
trainData &lt;- orange[trainRowNumbers,]

# Step 3: Create the test dataset
testData &lt;- orange[-trainRowNumbers,]

# Store X and Y for later use.
x = trainData[, 2:18]
y = trainData$Purchase</code></pre>
<p><code>createDataPartition</code>：输入 Y 和 P 比率（训练数据的比率） 输出 训练数据的行索引。</p>
<div id="save-the-image-for-next-blog" class="section level1">
<h1>save the image for next blog</h1>
<pre class="r"><code>save.image(file = &quot;../../data/caret.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[2 machine learning: load dataset]]></title>
    <link href="/2018/03/machine-learning-2-load-dataset/"/>
    <id>/2018/03/machine-learning-2-load-dataset/</id>
    <published>2018-03-22T00:00:00+00:00</published>
    <updated>2018-03-22T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-the-package-and-dataset" class="section level1">
<h1>Load the package and dataset</h1>
<p>我们将使用 ISLR 包中的 <a href="&#39;https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv&#39;">Orange Juice Data</a>.</p>
<p>目标： 预测顾客会购买哪两种🍊汁</p>
<p>数据集不是很大，我们的重点是构建模型的过程，而非真正搭建一个有用的模型。</p>
<p>👌，let’go.</p>
<pre class="r"><code># Load the caret package
library(caret)

# Import dataset
orange &lt;- read.csv(&#39;../../data/orange_juice_withmissing.csv&#39;)

# Structure of the dataframe
str(orange)</code></pre>
<pre><code>## &#39;data.frame&#39;:    1070 obs. of  18 variables:
##  $ Purchase      : Factor w/ 2 levels &quot;CH&quot;,&quot;MM&quot;: 1 1 1 2 1 1 1 1 1 1 ...
##  $ WeekofPurchase: int  237 239 245 227 228 230 232 234 235 238 ...
##  $ StoreID       : int  1 1 1 1 7 7 7 7 7 7 ...
##  $ PriceCH       : num  1.75 1.75 1.86 1.69 1.69 1.69 1.69 1.75 1.75 1.75 ...
##  $ PriceMM       : num  1.99 1.99 2.09 1.69 1.69 1.99 1.99 1.99 1.99 1.99 ...
##  $ DiscCH        : num  0 0 0.17 0 0 0 0 0 0 0 ...
##  $ DiscMM        : num  0 0.3 0 0 0 0 0.4 0.4 0.4 0.4 ...
##  $ SpecialCH     : int  0 0 0 0 0 0 1 1 0 0 ...
##  $ SpecialMM     : int  0 1 0 0 0 1 1 0 0 0 ...
##  $ LoyalCH       : num  0.5 0.6 0.68 0.4 0.957 ...
##  $ SalePriceMM   : num  1.99 1.69 2.09 1.69 1.69 1.99 1.59 1.59 1.59 1.59 ...
##  $ SalePriceCH   : num  1.75 1.75 1.69 1.69 1.69 1.69 1.69 1.75 1.75 1.75 ...
##  $ PriceDiff     : num  0.24 -0.06 0.4 0 0 0.3 -0.1 -0.16 -0.16 -0.16 ...
##  $ Store7        : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 2 2 2 2 2 2 ...
##  $ PctDiscMM     : num  0 0.151 0 0 0 ...
##  $ PctDiscCH     : num  0 0 0.0914 0 0 ...
##  $ ListPriceDiff : num  0.24 0.24 0.23 0 0 0.3 0.3 0.24 0.24 0.24 ...
##  $ STORE         : int  1 1 1 1 0 0 0 0 0 0 ...</code></pre>
<pre class="r"><code># See top 6 rows and 10 columns
head(orange[, 1:10])</code></pre>
<pre><code>##   Purchase WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH
## 1       CH            237       1    1.75    1.99   0.00    0.0         0
## 2       CH            239       1    1.75    1.99   0.00    0.3         0
## 3       CH            245       1    1.86    2.09   0.17    0.0         0
## 4       MM            227       1    1.69    1.69   0.00    0.0         0
## 5       CH            228       7    1.69    1.69   0.00    0.0         0
## 6       CH            230       7    1.69    1.99   0.00    0.0         0
##   SpecialMM  LoyalCH
## 1         0 0.500000
## 2         1 0.600000
## 3         0 0.680000
## 4         0 0.400000
## 5         0 0.956535
## 6         1 0.965228</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Machine Learning in R 1：introdution]]></title>
    <link href="/2018/03/machine-learning-in-r-introdution/"/>
    <id>/2018/03/machine-learning-in-r-introdution/</id>
    <published>2018-03-21T00:00:00+00:00</published>
    <updated>2018-03-21T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Caret Package是一个用于在R中构建机器学习模型的综合框架。在此博文中，我会解释 caret 包的几乎所有核心功能，并引导完成构建预测模型的分步过程。</p>
<p>Caret 是 Classification And REgression Training 的简称。</p>
<p>Caret 包很好地让所有与机器学习模型开发相关的步骤集成到一个简化的工作流程中，几乎每个主流的ML算法都可以在R中实现。</p>
<p>由于R具有如此多的机器学习算法实现包，因此选则在哪个包中实现某种算法是非常头疼的问题。多数时候，实现算法的语法和方法在不同包中有所不同。 结合数据预处理，查阅超参数的帮助页面（定义算法如何学习的参数）并努力寻找最佳模型，可以使构建预测模型成为一项相关任务。</p>
<p>在本教程后面的部分，我将介绍如何查看所有caret 支持的ML算法（这是一个很长的列表）以及可以调整哪些超参数。</p>
<p>此外，我们不会止步于 caret 包，我们将看看如何巧妙地集成来自多个最佳模型的预测，并可能使用 <code>caretEnsemble</code> 来产生更好的预测。</p>
<p>这个教程总共包括5部分，分别是： 1. 数据准备和清理 2. 可视化重要变量 3. 特征选择 4. 训练模型和调节模型 5. 预测</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[HOW TO USE THE NCBI&#39;S NEW API KEYS]]></title>
    <link href="/2018/03/how-to-use-the-ncbis-new-api-keys/"/>
    <id>/2018/03/how-to-use-the-ncbis-new-api-keys/</id>
    <published>2018-03-19T00:00:00+00:00</published>
    <updated>2018-03-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><a href="https://www.ncbi.nlm.nih.gov/">The NCBI</a> is one of the most important sources of biological data. The centre provides access to information on 28 million scholarly articles through PubMed and 250 million DNA sequences through GenBank. More importantly, records in the [50 public databases] (<a href="https://www.ncbi.nlm.nih.gov/guide/all/#databases" class="uri">https://www.ncbi.nlm.nih.gov/guide/all/#databases</a>) maintained by the NCBI are strongly cross-referenced. As a result, it is possible to pinpoint searches using almost 2 million taxonomic names or a <a href="https://www.nlm.nih.gov/mesh/">controlled vocabulary with 270,000 terms</a>.</p>
<p><strong>Rentrez has been designed to make it easy to search for and download NCBI records and download them from within an R session.</strong></p>
<p>I though it might be fun to use this post to find out where papers describing R packages are published these days</p>
<p>Here we use the <code>entrez_search</code> and <code>entrez_summary</code> functions to get some information on all of the papers published in 2017 with the term ‘R package’ in their title:</p>
<pre class="r"><code>if (!require(&quot;rentrez&quot;)) install.packages(&quot;rentrez&quot;)
library(rentrez)

pkg_search &lt;- entrez_search(db=&quot;pubmed&quot;, 
                            term=&quot;(R Package[TITLE]) AND (2018[PDAT])&quot;, 
                            use_history=TRUE)
pkg_summs &lt;- entrez_summary(db=&quot;pubmed&quot;, web_history=pkg_search$web_history)
pkg_summs</code></pre>
<pre><code>## List of  31 esummary records. First record:
## 
##  $`29554216`
## esummary result with 42 items:
##  [1] uid               pubdate           epubdate         
##  [4] source            authors           lastauthor       
##  [7] title             sorttitle         volume           
## [10] issue             pages             lang             
## [13] nlmuniqueid       issn              essn             
## [16] pubtype           recordstatus      pubstatus        
## [19] articleids        history           references       
## [22] attributes        pmcrefcount       fulljournalname  
## [25] elocationid       doctype           srccontriblist   
## [28] booktitle         medium            edition          
## [31] publisherlocation publishername     srcdate          
## [34] reportnumber      availablefromurl  locationlabel    
## [37] doccontriblist    docdate           bookname         
## [40] chapter           sortpubdate       sortfirstauthor</code></pre>
<p>we are interested in the journals in which these papers appear. We can use the helper function <code>extract_from_esummary</code> to isolate the <em>source</em> of each paper, then use <code>table</code> to count up the frequency of each journal.</p>
<pre class="r"><code>library(ggplot2)
library(ggpomological)
#scales::show_col(ggpomological:::pomological_palette)

journals &lt;- extract_from_esummary(pkg_summs, &quot;source&quot;)
journal_freq &lt;- as.data.frame(table(journals, dnn=&quot;journal&quot;), responseName=&quot;n.papers&quot;)
pkg_journal &lt;- ggplot(journal_freq, aes(reorder(journal, n.papers), n.papers)) + 
    geom_point(size=2) + 
    coord_flip() + 
    scale_y_continuous(&quot;Number of papers&quot;) +
    scale_x_discrete(&quot;Journal&quot;) +
    theme_bw() +
    ggtitle(&quot;Venues for papers describing R Packages in 2018&quot;)

pkg_journal + ggpomological::theme_pomological()</code></pre>
<p><img src="/post/2018-03-19-how_to_use_NCBI_API_keys_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>So, it looks like <em>Bioinformatics</em>, <em>Plos One</em> and <em>Comput Methods Progams Biomed</em> Resources are popular destinations for papers describing R packages, but these appear in journals all the way across the biological sciences.</p>
<p>The NCBI now gives users the opportunity to <a href="https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/">register for an access key</a> that will allow them to make up to 10 requests per second (non-registered users are limited to 3 requests per second per IP address).For one-off cases, this is as simple as adding the api_key argument to a given function call.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[A function for ggplot2: input strings for aes()]]></title>
    <link href="/2018/03/a-function-for-ggplot2-input-strings-for-aes/"/>
    <id>/2018/03/a-function-for-ggplot2-input-strings-for-aes/</id>
    <published>2018-03-18T00:00:00+00:00</published>
    <updated>2018-03-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>在我写文章画图时经常遇到的一个问题是：ggplot2 坐标轴的输入不支持输入数据框的变量名，通常会报错找不到对象</p>
<div class="section level1">
<h1>🌰：问题描述</h1>
<p>data: <a href="https://github.com/fivethirtyeight/data/tree/master/early-senate-polls">early senate poll</a></p>
<pre class="r"><code>library(tidyverse) # general tasks
library(broom) # tidy model output
library(ggthemes) # style the plots

poll_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/early-senate-polls/early-senate-polls.csv&quot;)

glimpse(poll_data)</code></pre>
<pre><code>## Observations: 107
## Variables: 4
## $ year                  &lt;int&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006...
## $ election_result       &lt;int&gt; -39, -10, -9, -16, 40, 10, -2, -41, -31,...
## $ presidential_approval &lt;int&gt; 46, 33, 32, 33, 53, 44, 37, 39, 42, 33, ...
## $ poll_average          &lt;int&gt; -28, -10, -1, -15, 39, 14, 2, -22, -27, ...</code></pre>
<p>background: <strong>there is a strong correlation between polling numbers and the ultimate result of an election</strong></p>
<div id="-" class="section level2">
<h2>构建模型： 线性模型</h2>
<pre class="r"><code>poll_lm &lt;- lm(election_result ~ poll_average, data = poll_data)

summary(poll_lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = election_result ~ poll_average, data = poll_data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -29.4281  -5.0197   0.5601   6.1364  17.9357 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -0.89110    0.76969  -1.158     0.25    
## poll_average  1.04460    0.03777  27.659   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.93 on 105 degrees of freedom
## Multiple R-squared:  0.8793, Adjusted R-squared:  0.8782 
## F-statistic:   765 on 1 and 105 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="section level2">
<h2>写个函数画出因变量和自变量的关系</h2>
<p>结果出现了一个令我费解的报错</p>
<blockquote>
<p><strong>Error in FUN(X[[i]], …) : object ‘poll_average’ not found</strong></p>
</blockquote>
<p>我不断地检查我的拼写，直到我开始怀疑人生</p>
</div>
</div>
<div id="define-aesthetic-mappings-programatically" class="section level1">
<h1>解决办法：<a href="http://ggplot2.tidyverse.org/reference/aes_.html"><strong>Define aesthetic mappings programatically</strong></a></h1>
<pre class="r"><code>plot_model &lt;- function(mod, explanatory, response, .fitted = &quot;.fitted&quot;) {
  augment(mod) %&gt;%
  ggplot() +
    geom_point(aes_string(x = explanatory, y = response), color = &quot;#2CA58D&quot;) +
    geom_line(aes_string(x = explanatory, y = .fitted), color = &quot;#033F63&quot;) +
    theme_solarized() +
    theme(axis.title = element_text()) +
    labs(x = &quot;Poll average&quot;, y = &quot;Election results&quot;)
}

plot_model(poll_lm, &quot;poll_average&quot;, &quot;election_result&quot;)</code></pre>
<p><img src="/post/2018-03-18-a-function-for-ggplot2-input-strings-for-aes_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hello world]]></title>
    <link href="/2018/03/hello-world/"/>
    <id>/2018/03/hello-world/</id>
    <published>2018-03-15T00:00:00+00:00</published>
    <updated>2018-03-15T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Just say hello world!</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Conferences, webinars, podcasts and the likes]]></title>
    <link href="/page/conferences_podcasts_webinars/"/>
    <id>/page/conferences_podcasts_webinars/</id>
    <published>2018-02-01T16:06:06+02:00</published>
    <updated>2018-02-01T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p>Here, you can find a list of all the talks I gave at conferences, webinars, podcasts, workshops, and all the other places you can and could hear me talk. :-)</p>

<h2 id="workshops-i-am-giving">Workshops I am giving</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/11/deep_learning_keras_tensorflow/">Workshop on Deep Learning with Keras and TensorFlow in R</a></li>
</ul>

<blockquote>
<p>I offer a workshop on deep learning with Keras and TensorFlow using R.
Date and place depend on who and how many people are interested, so please contact me either directly or via the workshop page: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/</a> (the description is in German but I also offer to give the workshop in English).</p>
</blockquote>

<h2 id="upcoming-talks-webinars-podcasts-etc">Upcoming talks, webinars, podcasts, etc.</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/m3_2018/">Announcing my talk about explainability of machine learning models at Minds Mastering Machines Conference</a></li>
</ul>

<blockquote>
<p>On Wednesday, April 25th 2018 I am going to talk about explainability of machine learning models at the Minds Mastering Machines conference in Cologne.</p>
</blockquote>

<ul>
<li><a href="JAX 2018 talk announcement: Deep Learning - a Primer">JAX 2018 talk announcement: Deep Learning - a Primer</a></li>
</ul>

<blockquote>
<p>Deep Learning is one of the &ldquo;hot&rdquo; topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become &ldquo;Software 2.0&rdquo;, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>

<h2 id="past-talks-webinars-podcasts-etc">Past talks, webinars, podcasts, etc.</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/herr_mies_wills_wissen/">I talk about machine learning with Daniel Mies (Podcast in German, though)</a></li>
</ul>

<blockquote>
<p>In January 2018 I was interviewed for a tech podcast where I talked about machine learning, neural nets, why I love R and Rstudio and how I became a Data Scientist.</p>
</blockquote>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/">Explaining Predictions of Machine Learning Models with LIME - Münster Data Science Meetup</a></li>
</ul>

<blockquote>
<p>In December 2017 I talked about Explaining Predictions of Machine Learning Models with LIME at the Münster Data Science Meetup.</p>
</blockquote>

<ul>
<li><a href="https://shiring.github.io/blogging/2017/09/20/webinar_biology_to_data_science">From Biology to Industry. A Blogger’s Journey to Data Science</a></li>
</ul>

<blockquote>
<p>In September 2017 I gave a webinar for the Applied Epidemiology Didactic of the University of Wisconsin - Madison titled “From Biology to Industry. A Blogger’s Journey to Data Science.”
I talked about how blogging about R and Data Science helped me become a Data Scientist. I also gave a short introduction to Machine Learning, Big Data and Neural Networks.</p>
</blockquote>

<ul>
<li><a href="https://shiring.github.io/machine_learning/2017/03/31/webinar_code">Building meaningful machine learning models for disease prediction</a></li>
</ul>

<blockquote>
<p>In March 2017 I gave a webinar for the ISDS R Group about my work on building machine-learning models to predict the course of different diseases. I went over building a model, evaluating its performance, and answering or addressing different disease related questions using machine learning. My talk covered the theory of machine learning as it is applied using R.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Welcome to my page!]]></title>
    <link href="/page/about/"/>
    <id>/page/about/</id>
    <published>2017-09-12T16:06:06+02:00</published>
    <updated>2017-09-12T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p><img src="/img/Bewerbungsfoto_klein.jpg" alt="" /></p>

<p>I&rsquo;m Shirin, a biologist turned bioinformatician turned data scientist.</p>

<p>I&rsquo;m especially interested in machine learning and data visualization. While I am using R most every day at work, I wanted to have an incentive to regularly explore other types of analyses and other types of data that I don&rsquo;t normally work with. I have also very often benefited from other people&rsquo;s published code in that it gave me ideas for my own work; and I hope that sharing my own analyses will inspire others as much as I often am by what can be be done with data.  It&rsquo;s amazing to me what can be learned from analyzing and visualizing data!</p>

<p>My tool of choice for data analysis so far has been R. I also organize the <a href="https://shiring.github.io/r_users_group/2017/05/20/muenster_r_user_group">MünsteR R-users group on meetup.com</a>.</p>

<p><img src="http://res.cloudinary.com/shiring/image/upload/v1511852499/my_story_wml3zm.png" alt="My journey to Data Science" /></p>

<p>I love dancing and used to do competitive ballroom and latin dancing. Even though I don&rsquo;t have time for that anymore, I still enjoy teaching &ldquo;social dances&rdquo; once a week with the Hochschulsport (university sports courses).</p>

<p>I created the R package <a href="https://github.com/ShirinG/exprAnalysis">exprAnalysis</a>, designed to streamline my RNA-seq data analysis pipeline. It is available via Github. Instructions for installation and usage can be found <a href="https://shiring.github.io/rna-seq/microarray/2016/09/28/exprAnalysis">here</a>.</p>

<p>This blog will showcase some of the analyses I have been doing with different data sets (all freely available). I will also host teaching materials for students to access in conjunction with R courses I am giving.</p>

<hr />

<h2 id="contact-me">Contact me:</h2>

<ul>
<li><a href="https://www.codecentric.de/team/shirin-glander/">Codecentric AG</a></li>
<li><a href="mailto:shirin.glander@gmail.com">Email</a></li>
<li><a href="http://www.xing.com/profile/Shirin_Glander">Xing</a></li>
<li><a href="http://de.linkedin.com/in/shirin-glander-01120881">Linkedin</a></li>
<li><a href="http://twitter.com/ShirinGlander">Twitter</a></li>
</ul>

<hr />

<p>Also check out <a href="http://www.R-bloggers.com">R-bloggers</a> for lots of cool R stuff!</p>
]]></content>
  </entry>
</feed>