<feed xmlns="http://www.w3.org/2005/Atom">
  <title>student zero </title>
  <link href="/index.xml" rel="self"/>
  <link href="/"/>
  <updated>2018-03-21T00:00:00+00:00</updated>
  <id>/</id>
  <author>
    <name>Jixing Liu</name>
  </author>
  <generator>Hugo -- gohugo.io</generator>
  <entry>
    <title type="html"><![CDATA[Machine Learning in R 1：introdution]]></title>
    <link href="/2018/03/machine-learning-in-r-introdution/"/>
    <id>/2018/03/machine-learning-in-r-introdution/</id>
    <published>2018-03-21T00:00:00+00:00</published>
    <updated>2018-03-21T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Caret Package是一个用于在R中构建机器学习模型的综合框架。在此博文中，我会解释 caret 包的几乎所有核心功能，并引导完成构建预测模型的分步过程。</p>
<p>Caret 是 Classification And REgression Training 的简称。</p>
<p>Caret 包很好地让所有与机器学习模型开发相关的步骤集成到一个简化的工作流程中，几乎每个主流的ML算法都可以在R中实现。</p>
<p>由于R具有如此多的机器学习算法实现包，因此选则在哪个包中实现某种算法是非常头疼的问题。多数时候，实现算法的语法和方法在不同包中有所不同。 结合数据预处理，查阅超参数的帮助页面（定义算法如何学习的参数）并努力寻找最佳模型，可以使构建预测模型成为一项相关任务。</p>
<p>在本教程后面的部分，我将介绍如何查看所有caret 支持的ML算法（这是一个很长的列表）以及可以调整哪些超参数。</p>
<p>此外，我们不会止步于 caret 包，我们将看看如何巧妙地集成来自多个最佳模型的预测，并可能使用 <code>caretEnsemble</code> 来产生更好的预测。</p>
<p>这个教程总共包括5部分，分别是： 1. 数据准备和清理 2. 可视化重要变量 3. 特征选择 4. 训练模型和调节模型 5. 预测</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[HOW TO USE THE NCBI&#39;S NEW API KEYS]]></title>
    <link href="/2018/03/how-to-use-the-ncbis-new-api-keys/"/>
    <id>/2018/03/how-to-use-the-ncbis-new-api-keys/</id>
    <published>2018-03-19T00:00:00+00:00</published>
    <updated>2018-03-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><a href="https://www.ncbi.nlm.nih.gov/">The NCBI</a> is one of the most important sources of biological data. The centre provides access to information on 28 million scholarly articles through PubMed and 250 million DNA sequences through GenBank. More importantly, records in the [50 public databases] (<a href="https://www.ncbi.nlm.nih.gov/guide/all/#databases" class="uri">https://www.ncbi.nlm.nih.gov/guide/all/#databases</a>) maintained by the NCBI are strongly cross-referenced. As a result, it is possible to pinpoint searches using almost 2 million taxonomic names or a <a href="https://www.nlm.nih.gov/mesh/">controlled vocabulary with 270,000 terms</a>.</p>
<p><strong>Rentrez has been designed to make it easy to search for and download NCBI records and download them from within an R session.</strong></p>
<p>I though it might be fun to use this post to find out where papers describing R packages are published these days</p>
<p>Here we use the <code>entrez_search</code> and <code>entrez_summary</code> functions to get some information on all of the papers published in 2017 with the term ‘R package’ in their title:</p>
<pre class="r"><code>if (!require(&quot;rentrez&quot;)) install.packages(&quot;rentrez&quot;)
library(rentrez)

pkg_search &lt;- entrez_search(db=&quot;pubmed&quot;, 
                            term=&quot;(R Package[TITLE]) AND (2018[PDAT])&quot;, 
                            use_history=TRUE)
pkg_summs &lt;- entrez_summary(db=&quot;pubmed&quot;, web_history=pkg_search$web_history)
pkg_summs</code></pre>
<pre><code>## List of  31 esummary records. First record:
## 
##  $`29554216`
## esummary result with 42 items:
##  [1] uid               pubdate           epubdate         
##  [4] source            authors           lastauthor       
##  [7] title             sorttitle         volume           
## [10] issue             pages             lang             
## [13] nlmuniqueid       issn              essn             
## [16] pubtype           recordstatus      pubstatus        
## [19] articleids        history           references       
## [22] attributes        pmcrefcount       fulljournalname  
## [25] elocationid       doctype           srccontriblist   
## [28] booktitle         medium            edition          
## [31] publisherlocation publishername     srcdate          
## [34] reportnumber      availablefromurl  locationlabel    
## [37] doccontriblist    docdate           bookname         
## [40] chapter           sortpubdate       sortfirstauthor</code></pre>
<p>we are interested in the journals in which these papers appear. We can use the helper function <code>extract_from_esummary</code> to isolate the <em>source</em> of each paper, then use <code>table</code> to count up the frequency of each journal.</p>
<pre class="r"><code>library(ggplot2)
library(ggpomological)
#scales::show_col(ggpomological:::pomological_palette)

journals &lt;- extract_from_esummary(pkg_summs, &quot;source&quot;)
journal_freq &lt;- as.data.frame(table(journals, dnn=&quot;journal&quot;), responseName=&quot;n.papers&quot;)
pkg_journal &lt;- ggplot(journal_freq, aes(reorder(journal, n.papers), n.papers)) + 
    geom_point(size=2) + 
    coord_flip() + 
    scale_y_continuous(&quot;Number of papers&quot;) +
    scale_x_discrete(&quot;Journal&quot;) +
    theme_bw() +
    ggtitle(&quot;Venues for papers describing R Packages in 2018&quot;)

pkg_journal + ggpomological::theme_pomological()</code></pre>
<p><img src="/post/2018-03-19-how_to_use_NCBI_API_keys_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>So, it looks like <em>Bioinformatics</em>, <em>Plos One</em> and <em>Comput Methods Progams Biomed</em> Resources are popular destinations for papers describing R packages, but these appear in journals all the way across the biological sciences.</p>
<p>The NCBI now gives users the opportunity to <a href="https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/">register for an access key</a> that will allow them to make up to 10 requests per second (non-registered users are limited to 3 requests per second per IP address).For one-off cases, this is as simple as adding the api_key argument to a given function call.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[A function for ggplot2: input strings for aes()]]></title>
    <link href="/2018/03/a-function-for-ggplot2-input-strings-for-aes/"/>
    <id>/2018/03/a-function-for-ggplot2-input-strings-for-aes/</id>
    <published>2018-03-18T00:00:00+00:00</published>
    <updated>2018-03-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>在我写文章画图时经常遇到的一个问题是：ggplot2 坐标轴的输入不支持输入数据框的变量名，通常会报错找不到对象</p>
<div class="section level1">
<h1>🌰：问题描述</h1>
<p>data: <a href="https://github.com/fivethirtyeight/data/tree/master/early-senate-polls">early senate poll</a></p>
<pre class="r"><code>library(tidyverse) # general tasks
library(broom) # tidy model output
library(ggthemes) # style the plots

poll_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/early-senate-polls/early-senate-polls.csv&quot;)

glimpse(poll_data)</code></pre>
<pre><code>## Observations: 107
## Variables: 4
## $ year                  &lt;int&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006...
## $ election_result       &lt;int&gt; -39, -10, -9, -16, 40, 10, -2, -41, -31,...
## $ presidential_approval &lt;int&gt; 46, 33, 32, 33, 53, 44, 37, 39, 42, 33, ...
## $ poll_average          &lt;int&gt; -28, -10, -1, -15, 39, 14, 2, -22, -27, ...</code></pre>
<p>background: <strong>there is a strong correlation between polling numbers and the ultimate result of an election</strong></p>
<div id="-" class="section level2">
<h2>构建模型： 线性模型</h2>
<pre class="r"><code>poll_lm &lt;- lm(election_result ~ poll_average, data = poll_data)

summary(poll_lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = election_result ~ poll_average, data = poll_data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -29.4281  -5.0197   0.5601   6.1364  17.9357 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -0.89110    0.76969  -1.158     0.25    
## poll_average  1.04460    0.03777  27.659   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.93 on 105 degrees of freedom
## Multiple R-squared:  0.8793, Adjusted R-squared:  0.8782 
## F-statistic:   765 on 1 and 105 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="section level2">
<h2>写个函数画出因变量和自变量的关系</h2>
<p>结果出现了一个令我费解的报错</p>
<blockquote>
<p><strong>Error in FUN(X[[i]], …) : object ‘poll_average’ not found</strong></p>
</blockquote>
<p>我不断地检查我的拼写，直到我开始怀疑人生</p>
</div>
</div>
<div id="define-aesthetic-mappings-programatically" class="section level1">
<h1>解决办法：<a href="http://ggplot2.tidyverse.org/reference/aes_.html"><strong>Define aesthetic mappings programatically</strong></a></h1>
<pre class="r"><code>plot_model &lt;- function(mod, explanatory, response, .fitted = &quot;.fitted&quot;) {
  augment(mod) %&gt;%
  ggplot() +
    geom_point(aes_string(x = explanatory, y = response), color = &quot;#2CA58D&quot;) +
    geom_line(aes_string(x = explanatory, y = .fitted), color = &quot;#033F63&quot;) +
    theme_solarized() +
    theme(axis.title = element_text()) +
    labs(x = &quot;Poll average&quot;, y = &quot;Election results&quot;)
}

plot_model(poll_lm, &quot;poll_average&quot;, &quot;election_result&quot;)</code></pre>
<p><img src="/post/2018-03-18-a-function-for-ggplot2-input-strings-for-aes_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hello world]]></title>
    <link href="/2018/03/hello-world/"/>
    <id>/2018/03/hello-world/</id>
    <published>2018-03-15T00:00:00+00:00</published>
    <updated>2018-03-15T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Just say hello world!</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Why should I trust you 🤖?]]></title>
    <link href="/2018/02/why-should-i-trust-you/"/>
    <id>/2018/02/why-should-i-trust-you/</id>
    <published>2018-02-11T00:00:00+00:00</published>
    <updated>2018-02-11T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>传统的机器学习工作流程主要集中在模型训练和优化上; 最好的模型通常是通过像精度或者错误这样的性能度量来选择的，而且如果它通过了这些性能标准的某些阈值，我们倾向于假设一个模型是足够好的。在机器学习的许多应用中，用户会使用一个模型来帮助做决定， 例如：一位医生不会对病人进行手术，仅仅因为这个模型说不应该进行手术？</p>
<p>由于复杂的机器学习模型本质上是黑盒子，而且太复杂，机器学习模型做出的分类决定通常很难被人类的大脑所理解，但是能够理解和解释这些模型对于提高模型质量，提高信任度和透明度以及减少偏见非常重要，因为人类往往具有良好的直觉和因果推理，这些都是难以在数据评估指标中捕获。</p>
<p>因此，我们希望能够形象的理解它们的工作原理：为什么一个模型将具有特定标签的案例进行准确分类。eg: 为什么一个乳腺肿块样本被归类为“恶性”而不是“良性，仅仅因为它长得丑吗？</p>
<blockquote>
<p><a href="https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime">Local Interpretable Model-Agnostic Explanations (LIME)</a> is an attempt to make these complex models at least partly understandable. The method has been published in <a href="https://arxiv.org/pdf/1602.04938.pdf">“Why Should I Trust You?”</a> Explaining the Predictions of Any Classifier. By Marco Tulio Ribeiro, Sameer Singh and Carlos Guestrin from the University of Washington in Seattle</p>
</blockquote>
<div id="how-lime-works" class="section level2">
<h2><strong>How LIME works</strong></h2>
<blockquote>
<p>lime is able to explain all models for which we can obtain prediction probabilities (in R, that is every model that works with predict(type = “prob”)). It makes use of the fact that linear models are easy to explain because they are based on linear relationships between features and class labels: The complex model function is approximated by locally fitting linear models to permutations of the original training set.On each permutation, a linear model is being fit and weights are given so that incorrect classification of instances that are more similar to the original data are penalized (positive weights support a decision, negative weights contradict them). This will give an approximation of how much (and in which way) each feature contributed to a decision made by the model</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7fa56fd7333.jpg" width="30%" /></p>
<blockquote>
<p>We take the image on the left and divide it into interpretable components</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7faba0d302a.jpg" width="30%" /></p>
<blockquote>
<p>we then generate a data set of perturbed instances by turning some of the interpretable components “oﬀ”. For each perturbed instance, we get the probability that a tree frog is in the image according to the model.</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7fac3ca8787.jpg" width="30%" /></p>
<blockquote>
<p>We then learn a simple (linear) model on this data set, which is locally weighted—that is, we care more about making mistakes in perturbed instances that are more similar to the original image</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7fac8382a86.jpg" width="30%" /></p>
<blockquote>
<p>the end, we present the superpixels with highest positive weights as an explanation, graying out everything else</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7faf16f0320.jpg" width="30%" /></p>
<p>🤖预测这张图是个树蛙是因为这个部分,所以🤖预测结果是比较可信的。 <img src="https://i.loli.net/2018/02/11/5a7faebabe34e.jpg" style="display: block; margin: auto;" /></p>
<p>🤖预测这张图是台球是根据这些部分，所以🤖预测结果是不可信的。 <img src="https://i.loli.net/2018/02/11/5a7fb61ecb060.jpg" style="display: block; margin: auto;" /></p>
</div>
<div id="example-in-r" class="section level2">
<h2><strong>Example in R</strong></h2>
<div id="prepare-the-breast-cancer-data" class="section level3">
<h3>01.Prepare the breast cancer data</h3>
<p>This <strong>data</strong> of example comes from the book of <strong>R in action</strong></p>
<pre class="r"><code>loc &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/&quot;
ds  &lt;- &quot;breast-cancer-wisconsin/breast-cancer-wisconsin.data&quot;
url &lt;- paste(loc, ds, sep=&quot;&quot;)

breast &lt;- read.table(url, sep=&quot;,&quot;, header=FALSE, na.strings=&quot;?&quot;)
names(breast) &lt;- c(&quot;ID&quot;, &quot;clumpThickness&quot;, &quot;sizeUniformity&quot;,
                   &quot;shapeUniformity&quot;, &quot;maginalAdhesion&quot;, 
                   &quot;singleEpithelialCellSize&quot;, &quot;bareNuclei&quot;, 
                   &quot;blandChromatin&quot;, &quot;normalNucleoli&quot;, &quot;mitosis&quot;, &quot;class&quot;)

df &lt;- breast[-1]
df$class &lt;- factor(df$class, levels=c(2,4), 
                   labels=c(&quot;benign&quot;, &quot;malignant&quot;))

set.seed(1234)
train &lt;- sample(nrow(df), 0.7*nrow(df))
df.train &lt;- df[train,]
df.validate &lt;- df[-train,]
table(df.train$class)</code></pre>
<pre><code>## 
##    benign malignant 
##       329       160</code></pre>
<pre class="r"><code>table(df.validate$class)</code></pre>
<pre><code>## 
##    benign malignant 
##       129        81</code></pre>
</div>
<div id="create-decision-tree-model" class="section level3">
<h3>02 Create decision tree model</h3>
<pre class="r"><code>library(rpart)
set.seed(1234)
dtree &lt;- rpart(class ~ ., data=df.train, method=&quot;class&quot;,      
               parms=list(split=&quot;information&quot;))
dtree$cptable</code></pre>
<pre><code>##         CP nsplit rel error  xerror       xstd
## 1 0.800000      0   1.00000 1.00000 0.06484605
## 2 0.046875      1   0.20000 0.30625 0.04150018
## 3 0.012500      3   0.10625 0.20625 0.03467089
## 4 0.010000      4   0.09375 0.18125 0.03264401</code></pre>
<pre class="r"><code>plotcp(dtree)</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>dtree.pruned &lt;- prune(dtree, cp=.0125)</code></pre>
</div>
<div id="predict" class="section level3">
<h3>03 predict</h3>
<pre class="r"><code>dtree.pred &lt;- predict(dtree.pruned, df.validate, type=&quot;class&quot;)
(dtree.perf &lt;- table(df.validate$class, dtree.pred, 
                    dnn=c(&quot;Actual&quot;, &quot;Predicted&quot;)))</code></pre>
<pre><code>##            Predicted
## Actual      benign malignant
##   benign       122         7
##   malignant      2        79</code></pre>
</div>
<div id="plot-decision-tree" class="section level3">
<h3>04 plot decision tree</h3>
<pre class="r"><code>#plot01
library(rpart.plot)
prp(dtree.pruned, type = 2, extra = 104,  
    fallen.leaves = TRUE, main=&quot;Decision Tree&quot;)
#plot02
library(partykit)</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>library(dplyr)
dtree.pruned %&gt;% as.party() %&gt;% plot()</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
</div>
<div id="lime-why-should-i-trust-you" class="section level3">
<h3>LIME: why should I trust you 🤖?</h3>
<pre class="r"><code>library(lime)
explainer &lt;- lime(df.train, model = dtree.pruned)

# Explain new observation
#[model_type](https://github.com/thomasp85/lime/blob/master/R/models.R)
model_type.rpart &lt;- function(x, ...) &#39;classification&#39;#defined model_type method
test.data &lt;- 
  df.validate %&gt;% 
  dplyr::select(-class) %&gt;% 
  head(3)
explanation &lt;- lime::explain(test.data, explainer, n_labels = 1, n_features = 2)

# The output is provided in a consistent tabular format and includes the
# output from the model.
head(explanation)</code></pre>
<pre><code>##       model_type case     label label_prob  model_r2 model_intercept
## 1 classification    3    benign  0.9933993 0.1754620      0.05039863
## 2 classification    3    benign  0.9933993 0.1754620      0.05039863
## 3 classification    4 malignant  0.9507042 0.2006329      0.53850056
## 4 classification    4 malignant  0.9507042 0.2006329      0.53850056
## 5 classification    5    benign  0.9933993 0.1871606      0.05372340
## 6 classification    5    benign  0.9933993 0.1871606      0.05372340
##   model_prediction                  feature feature_value feature_weight
## 1        0.4588963           blandChromatin             3   -0.001710722
## 2        0.4588963           sizeUniformity             1    0.410208409
## 3        0.9529770 singleEpithelialCellSize             3    0.003570030
## 4        0.9529770           sizeUniformity             8    0.410906399
## 5        0.4660418          maginalAdhesion             3   -0.004419457
## 6        0.4660418           sizeUniformity             1    0.416737849
##                        feature_desc                      data
## 1           2 &lt; blandChromatin &lt;= 3 3, 1, 1, 1, 2, 2, 3, 1, 1
## 2               sizeUniformity &lt;= 4 3, 1, 1, 1, 2, 2, 3, 1, 1
## 3 2 &lt; singleEpithelialCellSize &lt;= 4 6, 8, 8, 1, 3, 4, 3, 7, 1
## 4                4 &lt; sizeUniformity 6, 8, 8, 1, 3, 4, 3, 7, 1
## 5              maginalAdhesion &lt;= 3 4, 1, 1, 3, 2, 1, 3, 1, 1
## 6               sizeUniformity &lt;= 4 4, 1, 1, 3, 2, 1, 3, 1, 1
##               prediction
## 1 0.99339934, 0.00660066
## 2 0.99339934, 0.00660066
## 3 0.04929577, 0.95070423
## 4 0.04929577, 0.95070423
## 5 0.99339934, 0.00660066
## 6 0.99339934, 0.00660066</code></pre>
<pre class="r"><code># And can be visualised directly
plot_features(explanation)</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
</div>
<div id="links" class="section level2">
<h2><strong>Links</strong></h2>
<ul>
<li><a href="http://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease">CKD data set</a></li>
<li><a href="https://github.com/marcotcr/lime">open-source Python code for LIME</a></li>
<li><a href="https://github.com/thomasp85/lime">R package for LIME</a></li>
</ul>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Conferences, webinars, podcasts and the likes]]></title>
    <link href="/page/conferences_podcasts_webinars/"/>
    <id>/page/conferences_podcasts_webinars/</id>
    <published>2018-02-01T16:06:06+02:00</published>
    <updated>2018-02-01T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p>Here, you can find a list of all the talks I gave at conferences, webinars, podcasts, workshops, and all the other places you can and could hear me talk. :-)</p>

<h2 id="workshops-i-am-giving">Workshops I am giving</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/11/deep_learning_keras_tensorflow/">Workshop on Deep Learning with Keras and TensorFlow in R</a></li>
</ul>

<blockquote>
<p>I offer a workshop on deep learning with Keras and TensorFlow using R.
Date and place depend on who and how many people are interested, so please contact me either directly or via the workshop page: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/</a> (the description is in German but I also offer to give the workshop in English).</p>
</blockquote>

<h2 id="upcoming-talks-webinars-podcasts-etc">Upcoming talks, webinars, podcasts, etc.</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/m3_2018/">Announcing my talk about explainability of machine learning models at Minds Mastering Machines Conference</a></li>
</ul>

<blockquote>
<p>On Wednesday, April 25th 2018 I am going to talk about explainability of machine learning models at the Minds Mastering Machines conference in Cologne.</p>
</blockquote>

<ul>
<li><a href="JAX 2018 talk announcement: Deep Learning - a Primer">JAX 2018 talk announcement: Deep Learning - a Primer</a></li>
</ul>

<blockquote>
<p>Deep Learning is one of the &ldquo;hot&rdquo; topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become &ldquo;Software 2.0&rdquo;, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>

<h2 id="past-talks-webinars-podcasts-etc">Past talks, webinars, podcasts, etc.</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/herr_mies_wills_wissen/">I talk about machine learning with Daniel Mies (Podcast in German, though)</a></li>
</ul>

<blockquote>
<p>In January 2018 I was interviewed for a tech podcast where I talked about machine learning, neural nets, why I love R and Rstudio and how I became a Data Scientist.</p>
</blockquote>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/">Explaining Predictions of Machine Learning Models with LIME - Münster Data Science Meetup</a></li>
</ul>

<blockquote>
<p>In December 2017 I talked about Explaining Predictions of Machine Learning Models with LIME at the Münster Data Science Meetup.</p>
</blockquote>

<ul>
<li><a href="https://shiring.github.io/blogging/2017/09/20/webinar_biology_to_data_science">From Biology to Industry. A Blogger’s Journey to Data Science</a></li>
</ul>

<blockquote>
<p>In September 2017 I gave a webinar for the Applied Epidemiology Didactic of the University of Wisconsin - Madison titled “From Biology to Industry. A Blogger’s Journey to Data Science.”
I talked about how blogging about R and Data Science helped me become a Data Scientist. I also gave a short introduction to Machine Learning, Big Data and Neural Networks.</p>
</blockquote>

<ul>
<li><a href="https://shiring.github.io/machine_learning/2017/03/31/webinar_code">Building meaningful machine learning models for disease prediction</a></li>
</ul>

<blockquote>
<p>In March 2017 I gave a webinar for the ISDS R Group about my work on building machine-learning models to predict the course of different diseases. I went over building a model, evaluating its performance, and answering or addressing different disease related questions using machine learning. My talk covered the theory of machine learning as it is applied using R.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Welcome to my page!]]></title>
    <link href="/page/about/"/>
    <id>/page/about/</id>
    <published>2017-09-12T16:06:06+02:00</published>
    <updated>2017-09-12T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p><img src="/img/Bewerbungsfoto_klein.jpg" alt="" /></p>

<p>I&rsquo;m Shirin, a biologist turned bioinformatician turned data scientist.</p>

<p>I&rsquo;m especially interested in machine learning and data visualization. While I am using R most every day at work, I wanted to have an incentive to regularly explore other types of analyses and other types of data that I don&rsquo;t normally work with. I have also very often benefited from other people&rsquo;s published code in that it gave me ideas for my own work; and I hope that sharing my own analyses will inspire others as much as I often am by what can be be done with data.  It&rsquo;s amazing to me what can be learned from analyzing and visualizing data!</p>

<p>My tool of choice for data analysis so far has been R. I also organize the <a href="https://shiring.github.io/r_users_group/2017/05/20/muenster_r_user_group">MünsteR R-users group on meetup.com</a>.</p>

<p><img src="http://res.cloudinary.com/shiring/image/upload/v1511852499/my_story_wml3zm.png" alt="My journey to Data Science" /></p>

<p>I love dancing and used to do competitive ballroom and latin dancing. Even though I don&rsquo;t have time for that anymore, I still enjoy teaching &ldquo;social dances&rdquo; once a week with the Hochschulsport (university sports courses).</p>

<p>I created the R package <a href="https://github.com/ShirinG/exprAnalysis">exprAnalysis</a>, designed to streamline my RNA-seq data analysis pipeline. It is available via Github. Instructions for installation and usage can be found <a href="https://shiring.github.io/rna-seq/microarray/2016/09/28/exprAnalysis">here</a>.</p>

<p>This blog will showcase some of the analyses I have been doing with different data sets (all freely available). I will also host teaching materials for students to access in conjunction with R courses I am giving.</p>

<hr />

<h2 id="contact-me">Contact me:</h2>

<ul>
<li><a href="https://www.codecentric.de/team/shirin-glander/">Codecentric AG</a></li>
<li><a href="mailto:shirin.glander@gmail.com">Email</a></li>
<li><a href="http://www.xing.com/profile/Shirin_Glander">Xing</a></li>
<li><a href="http://de.linkedin.com/in/shirin-glander-01120881">Linkedin</a></li>
<li><a href="http://twitter.com/ShirinGlander">Twitter</a></li>
</ul>

<hr />

<p>Also check out <a href="http://www.R-bloggers.com">R-bloggers</a> for lots of cool R stuff!</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Find all my other posts on my old website!]]></title>
    <link href="/2017/07/find-all-my-other-posts-on-my-old-website/"/>
    <id>/2017/07/find-all-my-other-posts-on-my-old-website/</id>
    <published>2017-07-01T00:00:00+00:00</published>
    <updated>2017-07-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>For all my other posts, see my old website:
<a href="https://shiring.github.io">shiring.github.io</a></p>
]]></content>
  </entry>
</feed>