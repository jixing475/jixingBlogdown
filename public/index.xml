<feed xmlns="http://www.w3.org/2005/Atom">
  <title>student zero </title>
  <link href="/index.xml" rel="self"/>
  <link href="/"/>
  <updated>2018-05-09T00:00:00+00:00</updated>
  <id>/</id>
  <author>
    <name>Jixing Liu</name>
  </author>
  <generator>Hugo -- gohugo.io</generator>
  <entry>
    <title type="html"><![CDATA[LELTS speaking: constructing your answers]]></title>
    <link href="/2018/05/lelts-speaking-constructing-your-answers/"/>
    <id>/2018/05/lelts-speaking-constructing-your-answers/</id>
    <published>2018-05-09T00:00:00+00:00</published>
    <updated>2018-05-09T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><center><h2>  <strong style="color: darkred;"> å››ä¸ªè¦æ³¨æ„çš„é—®é¢˜</strong> </h2></center></p>

<ol>
<li>æµç•…æ€§</li>
<li>ä¸°å¯Œå¤šå˜çš„è¯æ±‡</li>
<li>è¯­æ³•å’Œå‡†ç¡®</li>
<li>å‘éŸ³</li>
</ol>

<p><center><h2>  <strong style="color: darkred;">ç­”æ¡ˆçš„åŸºæœ¬ç»“æ„</strong> </h2></center></p>

<p>ç›´æ¥å›ç­” + è¿æ¥è¯ + <strong style="color: darkred;">æ‰©å±•å¥å­</strong> ï¼š</p>

<ol>
<li>åŸå› </li>
<li>ç»†èŠ‚</li>
<li>ä¾‹å­</li>
<li>äº‹æƒ…çš„ä¸¤é¢æ€§</li>
</ol>

<p><center><h2>  <strong style="color: darkred;">å¸¸è§è¯è¯­çš„æ›¿ä»£</strong> </h2></center></p>

<ol>
<li><p>And</p>

<ul>
<li>in addition to what i have just mentioned earlier</li>
<li>on the top of that</li>
<li>as well as this</li>
<li>second to that</li>
<li>among other things</li>
<li>as a further matter
<br /></li>
</ul></li>

<li><p>So</p>

<ul>
<li>this is the reason why</li>
<li>this wxplains why</li>
<li>and the result of this is
<br /></li>
</ul></li>

<li><p>But</p>

<ul>
<li>however</li>
<li>meanwhile</li>
<li>on the other hand</li>
<li>in the meantime
<br /></li>
</ul></li>

<li><p>I think</p>

<ul>
<li>i believe</li>
<li>i reckon</li>
<li>i&rsquo;m convinced to say that</li>
<li>i&rsquo;m inclined to believe that</li>
<li>my view on this matter is
<br /></li>
</ul></li>

<li><p>In my opinion</p>

<ul>
<li>as far as i&rsquo;m concerned</li>
<li>from what i can see</li>
<li>come to think of it</li>
<li>the way i see it
<br /></li>
</ul></li>

<li><p>Because</p>

<ul>
<li>and this is probably because</li>
<li>i guess the reason for that is</li>
<li>the reason i feel this way is</li>
<li>it is mainly due to the fact that
<br /></li>
</ul></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[R vs python]]></title>
    <link href="/2018/05/r-vs-python-representing-data-in-r-python-equivalent/"/>
    <id>/2018/05/r-vs-python-representing-data-in-r-python-equivalent/</id>
    <published>2018-05-03T00:00:00+00:00</published>
    <updated>2018-05-03T00:00:00+00:00</updated>
    <content type="html"><![CDATA[

<h2 id="strong-style-color-darkred-representing-data-in-r-python-equivalent-strong"><strong style="color: darkred;">Representing Data in R &ndash; Python equivalent</strong></h2>

<pre><code class="language-python">import pandas as pd
import numpy as np
</code></pre>

<pre><code class="language-python"># 'characters' is equivalent to string
firstName = 'jeff'
print((type(firstName), firstName))
</code></pre>

<pre><code>&lt;type 'str'&gt; jeff
</code></pre>

<pre><code class="language-python"># 'numeric' is equivalent to float
heightCM = 188.2
print((type(heightCM), heightCM))
</code></pre>

<pre><code>&lt;type 'float'&gt; 188.2
</code></pre>

<pre><code class="language-python"># integer is equivalent to integer
numberSons = 1
print((type(numberSons), numberSons))
</code></pre>

<pre><code>&lt;type 'int'&gt; 1
</code></pre>

<pre><code class="language-python"># 'logical' is equivalent to Boolean
teachingCoursera = True
print((type(teachingCoursera), teachingCoursera))
</code></pre>

<pre><code>&lt;type 'bool'&gt; True
</code></pre>

<pre><code class="language-python"># 'vectors' is equivalent to numpy array or Python list (I will use array everywhere for consistency)
heights = np.array([188.2, 181.3, 193.4])
print(heights)

firstNames = np.array(['jeff', 'roger', 'andrew', 'brian'])
print(firstNames)
</code></pre>

<pre><code>[ 188.2  181.3  193.4]
['jeff' 'roger' 'andrew' 'brian']
</code></pre>

<pre><code class="language-python"># 'list' is equivalent to dictionary in Python
vector1 = np.array([188.2, 181.3, 193.4])
vector2 = np.array(['jeff', 'roger', 'andrew', 'brian'])
myList = dict(heights = vector1, firstNames = vector2)
print(myList)

print((myList['heights']))
print((myList['firstNames']))
</code></pre>

<pre><code>{'firstNames': array(['jeff', 'roger', 'andrew', 'brian'], 
      dtype='|S6'), 'heights': array([ 188.2,  181.3,  193.4])}
[ 188.2  181.3  193.4]
['jeff' 'roger' 'andrew' 'brian']
</code></pre>

<pre><code class="language-python"># 'matrices' is equivalent to two-dimensional numpy array
myMatrix = np.array([[1, 2], [3, 4]])
print(myMatrix)
</code></pre>

<pre><code>[[1 2]
 [3 4]]
</code></pre>

<pre><code class="language-python"># data frame is equivalent to Pandas DataFrame
# this example doesn't work because the input array lengths are not the same
vector1 = np.array([188.2, 181.3, 193.4])
vector2 = np.array(['jeff', 'roger', 'andrew', 'brian'])

# ValueError: arrays must all be same length
# 
myDataFrame = pd.DataFrame(dict(heights = vector1, firstNames = vector2))
</code></pre>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)

&lt;ipython-input-10-58e1535d1fac&gt; in &lt;module&gt;()
      6 # ValueError: arrays must all be same length
      7 #
----&gt; 8 myDataFrame = pd.DataFrame(dict(heights = vector1, firstNames = vector2))


/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/frame.pyc in __init__(self, data, index, columns, dtype, copy)
    383             mgr = self._init_mgr(data, index, columns, dtype=dtype, copy=copy)
    384         elif isinstance(data, dict):
--&gt; 385             mgr = self._init_dict(data, index, columns, dtype=dtype)
    386         elif isinstance(data, ma.MaskedArray):
    387             mask = ma.getmaskarray(data)


/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/frame.pyc in _init_dict(self, data, index, columns, dtype)
    515 
    516         return _arrays_to_mgr(arrays, data_names, index, columns,
--&gt; 517                               dtype=dtype)
    518 
    519     def _init_ndarray(self, values, index, columns, dtype=None,


/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/frame.pyc in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)
   5343     # figure out the index, if necessary
   5344     if index is None:
-&gt; 5345         index = extract_index(arrays)
   5346     else:
   5347         index = _ensure_index(index)


/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/frame.pyc in extract_index(data)
   5395             lengths = list(set(raw_lengths))
   5396             if len(lengths) &gt; 1:
-&gt; 5397                 raise ValueError('arrays must all be same length')
   5398 
   5399             if have_dicts:


ValueError: arrays must all be same length
</code></pre>

<pre><code class="language-python"># data frame -- fixed
vector1 = np.array([188.2, 181.3, 193.4, 192.3])
vector2 = np.array(['jeff', 'roger', 'andrew', 'brian'])

myDataFrame = pd.DataFrame(dict(heights = vector1, firstNames = vector2))
myDataFrame
</code></pre>

<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>firstNames</th>
      <th>heights</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>   jeff</td>
      <td> 188.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>  roger</td>
      <td> 181.3</td>
    </tr>
    <tr>
      <th>2</th>
      <td> andrew</td>
      <td> 193.4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>  brian</td>
      <td> 192.3</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># factors is equivalent to pandas Categorical
smoker = np.array(['yes', 'no', 'yes', 'yes'])
smokerFactor = pd.Categorical.from_array(smoker)
smokerFactor
</code></pre>

<pre><code>Categorical: 
array(['yes', 'no', 'yes', 'yes'], dtype=object)
Levels (2): Index(['no', 'yes'], dtype=object)
</code></pre>

<pre><code class="language-python"># R's NA missing values is equivalent to NaN
vector1 = np.array([188.2, 181.3, 193.4, NaN])
print(vector1)
print((isnan(vector1)))
</code></pre>

<pre><code>[ 188.2  181.3  193.4    nan]
[False False False  True]
</code></pre>

<pre><code class="language-python"># subsetting
vector1 = np.array([188.2, 181.3, 193.4, 192.3])
vector2 = np.array(['jeff', 'roger', 'andrew', 'brian'])

myDataFrame = pd.DataFrame(dict(heights = vector1, firstNames = vector2))

print('------------------')
print((vector1[0]))
print('------------------')
print((vector1[[0, 1, 3]]))
print('------------------')
print((myDataFrame.ix[0, 0:2])) # appears transposed as compared to R
print('------------------')
print((myDataFrame['firstNames'])) # there's no 'Levels' as in R
print('------------------')
print((myDataFrame[myDataFrame['firstNames'] == 'jeff']))
print('------------------')
print((myDataFrame[myDataFrame['heights'] &lt; 190]))
</code></pre>

<pre><code>------------------
188.2
------------------
[ 188.2  181.3  192.3]
------------------
firstNames     jeff
heights       188.2
Name: 0
------------------
0      jeff
1     roger
2    andrew
3     brian
Name: firstNames
------------------
  firstNames  heights
0       jeff    188.2
------------------
  firstNames  heights
0       jeff    188.2
1      roger    181.3
</code></pre>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[How To Get Genomewide Position-Specific Scores]]></title>
    <link href="/2018/04/how-to-get-genomewide-position-specific-scores/"/>
    <id>/2018/04/how-to-get-genomewide-position-specific-scores/</id>
    <published>2018-04-27T00:00:00+00:00</published>
    <updated>2018-04-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-packages" class="section level2">
<h2>load packages</h2>
<pre class="r"><code>#source(&quot;http://bioconductor.org/biocLite.R&quot;)
#biocLite(&quot;GenomicScores&quot;)
library(GenomicScores)</code></pre>
</div>
<div id="retrieval-of-genomic-scores-through-annotation-packages" class="section level2">
<h2>Retrieval of genomic scores through annotation packages</h2>
<p>There are currently four different annotation packages that store genomic scores and can be accessed using theÂ <em><a href="http://bioconductor.org/packages/GenomicScores">GenomicScores</a></em>Â package</p>
<p><strong style="color: darkred;">Annotation packages </strong> <strong style="color: darkred;">Description</strong> 1. <em><a href="http://bioconductor.org/packages/phastCons100way.UCSC.hg19">phastCons100way.UCSC.hg19</a></em> phastCons scores derived from the alignment of the human genome (hg19) to other 99 vertebrate species. 2. <em><a href="http://bioconductor.org/packages/phastCons100way.UCSC.hg38">phastCons100way.UCSC.hg38</a></em> phastCons scores derived from the alignment of the human genome (hg38) to other 99 vertebrate species. 3. <em><a href="http://bioconductor.org/packages/phastCons7way.UCSC.hg38">phastCons7way.UCSC.hg38</a></em> phastCons scores derived from the alignment of the human genome (hg38) to other 6 mammal species. 4. <em><a href="http://bioconductor.org/packages/fitCons.UCSC.hg19">fitCons.UCSC.hg19</a></em> fitCons scores: fitness consequences of functional annotation for the human genome (hg19).</p>
<div id="to-retrieve-genomic-scores-for-specific-positions-we-should-use-the-function-scores" class="section level3">
<h3><strong style="color: darkred;">To retrieve genomic scores for specific positions we should use the function scores()</strong></h3>
<pre class="r"><code>library(phastCons100way.UCSC.hg19)
gsco &lt;- phastCons100way.UCSC.hg19
class(gsco)</code></pre>
<pre><code>## [1] &quot;GScores&quot;
## attr(,&quot;package&quot;)
## [1] &quot;GenomicScores&quot;</code></pre>
<pre class="r"><code>scores(gsco, GRanges(seqnames=&quot;chr22&quot;,
                     IRanges(start=50967020:50967021, width=1)))</code></pre>
<pre><code>## GRanges object with 2 ranges and 1 metadata column:
##       seqnames               ranges strand |    scores
##          &lt;Rle&gt;            &lt;IRanges&gt;  &lt;Rle&gt; | &lt;numeric&gt;
##   [1]    chr22 [50967020, 50967020]      * |         1
##   [2]    chr22 [50967021, 50967021]      * |         1
##   -------
##   seqinfo: 1 sequence from an unspecified genome; no seqlengths</code></pre>
<pre class="r"><code>gsco</code></pre>
<pre><code>## GScores object 
## # organism: Homo sapiens (UCSC)
## # provider: UCSC
## # provider version: 09Feb2014
## # download date: Mar 17, 2017
## # loaded sequences: chr19_gl000208_random, chr22
## # maximum abs. error: 0.05</code></pre>
<pre class="r"><code>#citation(gsco) now cann&#39;t run
provider(gsco)</code></pre>
<pre><code>## [1] &quot;UCSC&quot;</code></pre>
<pre class="r"><code>providerVersion(gsco)</code></pre>
<pre><code>## [1] &quot;09Feb2014&quot;</code></pre>
<pre class="r"><code>organism(gsco)</code></pre>
<pre><code>## [1] &quot;Homo sapiens&quot;</code></pre>
<pre class="r"><code>seqlevelsStyle(gsco)</code></pre>
<pre><code>## [1] &quot;UCSC&quot;</code></pre>
</div>
</div>
<div id="retrieval-of-genomic-scores-through-annotationhub-resources" class="section level2">
<h2>Retrieval of genomic scores through <strong style="color: darkred;">AnnotationHub resources</strong></h2>
<p>Another way to retrieve genomic scores is by using theÂ <em><a href="http://bioconductor.org/packages/AnnotationHub">AnnotationHub</a></em>, which is a web resource that provides a central location where genomic files (e.g., VCF, bed, wig) and other resources from standard (e.g., UCSC, Ensembl) and distributed sites, can be found. A BioconductorÂ <em><a href="http://bioconductor.org/packages/AnnotationHub">AnnotationHub</a></em>Â web resource creates and manages a local cache of files retrieved by the user, helping with quick and reproducible access.</p>
<p><img src="/post/2018-04-27-how-to-get-genomewide-position-specific-scores_files/Screen Shot 2018-04-27 at 15.35.21.png" width="30%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>gsco &lt;- getGScores(&quot;phastCons7way.UCSC.hg38&quot;)
scores(gsco, GRanges(seqnames=&quot;chr22&quot;,IRanges(start=20967020:20967025, width=1)))</code></pre>
</div>
<div id="retrieval-of-multiple-scores-per-genomic-position" class="section level2">
<h2>Retrieval of <strong style="color: darkred;">multiple scores</strong> per genomic position</h2>
</div>
<div id="summarization-of-genomic-scores" class="section level2">
<h2>Summarization of genomic scores</h2>
<pre class="r"><code>library(dplyr)
gsco &lt;- phastCons100way.UCSC.hg19
gr &lt;- GRanges(seqnames=&quot;chr22&quot;, IRanges(start=50967020:50967025, width=1))

#mean
scores(gsco, gr) %&gt;% 
  .$scores %&gt;% 
  mean()</code></pre>
<pre><code>## [1] 0.8</code></pre>
<pre class="r"><code>#median
scores(gsco, gr) %&gt;% 
  .$scores %&gt;% 
  median()</code></pre>
<pre><code>## [1] 1</code></pre>
<pre class="r"><code>#min
scores(gsco, gr) %&gt;% 
  .$scores %&gt;% 
  min()</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>scores(gsco, gr) %&gt;% 
  .$scores %&gt;% 
  hist()</code></pre>
<p><img src="/post/2018-04-27-how-to-get-genomewide-position-specific-scores_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="annotating-variants-with-genomic-scores" class="section level2">
<h2><strong style="color: darkred;">Annotating variants with genomic scores</strong></h2>
<p>A typical use case of theÂ <em><a href="http://bioconductor.org/packages/GenomicScores">GenomicScores</a></em>Â package is in the context of annotating variants with genomic scores, such as phastCons conservation scores. For this purpose, we load theÂ <em><a href="http://bioconductor.org/packages/VariantAnnotaiton">VariantAnnotaiton</a></em>Â andÂ <em><a href="http://bioconductor.org/packages/TxDb.Hsapiens.UCSC.hg19.knownGene">TxDb.Hsapiens.UCSC.hg19.knownGene</a></em>Â packages. The former will allow us to read a VCF file and annotate it, and the latter contains the gene annotations from UCSC that will be used in this process.</p>
<p>Letâ€™s load one of the sample VCF files that form part of theÂ <em><a href="http://bioconductor.org/packages/VariantAnnotation">VariantAnnotation</a></em>Â package.</p>
<pre class="r"><code>library(VariantAnnotation)
library(TxDb.Hsapiens.UCSC.hg19.knownGene)
fl &lt;- system.file(&quot;extdata&quot;, &quot;chr22.vcf.gz&quot;, package=&quot;VariantAnnotation&quot;)
vcf &lt;- readVcf(fl, &quot;hg19&quot;)
seqlevelsStyle(vcf)</code></pre>
<pre><code>## [1] &quot;NCBI&quot;    &quot;Ensembl&quot;</code></pre>
<pre class="r"><code>txdb &lt;- TxDb.Hsapiens.UCSC.hg19.knownGene
seqlevelsStyle(txdb)</code></pre>
<pre><code>## [1] &quot;UCSC&quot;</code></pre>
<p>Because the chromosome nomenclature from the VCF file (NCBI) is different from the one with the gene annotations (UCSC) we use the <strong style="color: darkred;">seqlevelsStyle()</strong> function to force our variants having the chromosome nomenclature of the gene annotations.</p>
<pre class="r"><code>seqlevelsStyle(vcf) &lt;- seqlevelsStyle(txdb)</code></pre>
<p>We <strong style="color: darkred;">annotate the location of variants</strong> using the function <strong style="color: darkred;">locateVariants()</strong> from the VariantAnnotation package.</p>
<pre class="r"><code>loc &lt;- locateVariants(vcf, txdb, AllVariants())
loc[1:3]</code></pre>
<pre><code>## GRanges object with 3 ranges and 9 metadata columns:
##    seqnames               ranges strand | LOCATION  LOCSTART    LOCEND
##       &lt;Rle&gt;            &lt;IRanges&gt;  &lt;Rle&gt; | &lt;factor&gt; &lt;integer&gt; &lt;integer&gt;
##       chr22 [50300078, 50300078]      - |   intron     10763     10763
##       chr22 [50300086, 50300086]      - |   intron     10755     10755
##       chr22 [50300101, 50300101]      - |   intron     10740     10740
##      QUERYID        TXID         CDSID      GENEID       PRECEDEID
##    &lt;integer&gt; &lt;character&gt; &lt;IntegerList&gt; &lt;character&gt; &lt;CharacterList&gt;
##            1       75253                     79087                
##            2       75253                     79087                
##            3       75253                     79087                
##           FOLLOWID
##    &lt;CharacterList&gt;
##                   
##                   
##                   
##   -------
##   seqinfo: 1 sequence from hg19 genome; no seqlengths</code></pre>
<pre class="r"><code>table(loc$LOCATION)</code></pre>
<pre><code>## 
## spliceSite     intron    fiveUTR   threeUTR     coding intergenic 
##         11      22572        309       1368       2822       2867 
##   promoter 
##       2864</code></pre>
</div>
<div id="annotate-phastcons-conservation-scores" class="section level2">
<h2><strong style="color: darkred;">Annotate phastCons conservation scores</strong></h2>
<p>on the variants and store those annotations as an additional metadata column of the GRanges object. For this specific purpose we should the argument scores.only=TRUE that makes the scores() method to return the genomic scores as a numeric vector instead as a metadata column in the input ranges object.</p>
<pre class="r"><code>loc$PHASTCONS &lt;- scores(gsco, loc, scores.only=TRUE)
loc[1:3]</code></pre>
<pre><code>## GRanges object with 3 ranges and 10 metadata columns:
##    seqnames               ranges strand | LOCATION  LOCSTART    LOCEND
##       &lt;Rle&gt;            &lt;IRanges&gt;  &lt;Rle&gt; | &lt;factor&gt; &lt;integer&gt; &lt;integer&gt;
##       chr22 [50300078, 50300078]      - |   intron     10763     10763
##       chr22 [50300086, 50300086]      - |   intron     10755     10755
##       chr22 [50300101, 50300101]      - |   intron     10740     10740
##      QUERYID        TXID         CDSID      GENEID       PRECEDEID
##    &lt;integer&gt; &lt;character&gt; &lt;IntegerList&gt; &lt;character&gt; &lt;CharacterList&gt;
##            1       75253                     79087                
##            2       75253                     79087                
##            3       75253                     79087                
##           FOLLOWID PHASTCONS
##    &lt;CharacterList&gt; &lt;numeric&gt;
##                          0.0
##                          0.1
##                          0.0
##   -------
##   seqinfo: 1 sequence from hg19 genome; no seqlengths</code></pre>
<p>Using the following code we can examine <strong style="color: darkred;">the distribution of phastCons conservation scores</strong> of variants across the different annotated regions</p>
<pre class="r"><code>x &lt;- split(loc$PHASTCONS, loc$LOCATION)
mask &lt;- elementNROWS(x) &gt; 0
boxplot(x[mask], ylab=&quot;phastCons score&quot;, las=1, cex.axis=1.2, cex.lab=1.5, col=&quot;gray&quot;)
points(1:length(x[mask])+0.25, 
       sapply(x[mask], mean, na.rm=TRUE),
       pch=23, bg=&quot;red&quot;)</code></pre>
<p><img src="/post/2018-04-27-how-to-get-genomewide-position-specific-scores_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="links" class="section level2">
<h2><strong>Links</strong></h2>
<ul>
<li><a href="http://bioconductor.org/packages/release/bioc/vignettes/GenomicScores/inst/doc/GenomicScores.html">An introduction to the GenomicScores package</a></li>
</ul>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[purrr: adverb]]></title>
    <link href="/2018/04/purrr-adverb/"/>
    <id>/2018/04/purrr-adverb/</id>
    <published>2018-04-20T00:00:00+00:00</published>
    <updated>2018-04-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="what-is-an-adverb" class="section level2">
<h2><strong style="color: darkred;">What is an adverb</strong></h2>
<p>Read carefully theÂ <a href="http://purrr.tidyverse.org/reference/index.html#section-adverbs">purrr documentation</a></p>
<p><img src="https://media.giphy.com/media/8UGoKT4boBkF9MGUb3/giphy.gif" width="30%" style="display: block; margin: auto;" /></p>
<blockquote>
<p>Adverbs <strong style="color: darkred;">modify the action of a function</strong> ; taking a function as input and <strong style="color: darkred;">returning a function with modified action as output.</strong></p>
</blockquote>
<p>In other words, adverbs take a function, and return this function modified. Yes, just an adverb modifies a verb.</p>
<pre class="r"><code>library(purrr)
safe_log &lt;- safely(log)#high-order functions
safe_log(&quot;a&quot;)</code></pre>
<pre><code>## $result
## NULL
## 
## $error
## &lt;simpleError in log(x = x, base = base): non-numeric argument to mathematical function&gt;</code></pre>
<pre class="r"><code># have a result and error</code></pre>
</div>
<div id="how-to-write-your-own" class="section level1">
<h1><strong style="color: darkred;">how to write your own?</strong></h1>
<pre class="r"><code>library(attempt)

# Silently only return the errors, and nothing if the function succeeds
silent_log &lt;- silently(log)
silent_log(1)
# Surely make a function always work, without stopping the process
sure_log &lt;- surely(log)
sure_log(1)</code></pre>
<pre><code>## [1] 0</code></pre>
<div id="with_message-and-with_warning" class="section level2">
<h2><strong style="color: darkred;">with_message and with_warning</strong></h2>
<pre class="r"><code>as_num_msg &lt;- with_message(as.numeric, msg = &quot;We&#39;re performing a numeric conversion&quot;)
as_num_warn &lt;- with_warning(as.numeric, msg = &quot;We&#39;re performing a numeric conversion&quot;)
as_num_msg(&quot;1&quot;)</code></pre>
<pre><code>## [1] 1</code></pre>
</div>
</div>
<div id="how-to-implement-this-kind-of-behavior" class="section level1">
<h1>how to implement this kind of behavior?</h1>
<p>Letâ€™s take a simple example with sleepy:</p>
<pre class="r"><code>sleepy &lt;- function(fun, sleep){
  function(...){
    Sys.sleep(sleep)
    fun(...)
  }
}

sleep_print &lt;- sleepy(Sys.time, 5)
class(sleep_print)</code></pre>
<pre><code>## [1] &quot;function&quot;</code></pre>
<pre class="r"><code>sleep_print()</code></pre>
<pre><code>## [1] &quot;2018-04-20 09:15:36 CST&quot;</code></pre>
</div>
<div id="how" class="section level1">
<h1><strong style="color: darkred;">how?</strong></h1>
<p>First of all, the adverb functon should return another function, so we need to start with?</p>
<pre class="r"><code>talky &lt;- function(fun){
  function(...){
    fun(...)
  }
}</code></pre>
<p>secondly, with R referential transparency, you can <strong style="color: darkred;">create a variable that is a function:</strong></p>
<pre class="r"><code>plop &lt;- mean
plop(1:10)</code></pre>
<pre><code>## [1] 5.5</code></pre>
<pre class="r"><code>sys_time &lt;- talky(Sys.time)
sys_time()</code></pre>
<pre><code>## [1] &quot;2018-04-20 09:15:36 CST&quot;</code></pre>
<div id="the-template" class="section level2">
<h2><strong style="color: darkred;">the template</strong></h2>
<pre class="r"><code>talky &lt;- function(fun, mess){
  function(...){
    #add some command
    message(mess)#è¿™é‡Œå¯ä»¥æ·»åŠ å‚æ•°
    print(Sys.time())
    
    print(&quot;you can add anything&quot;)
    
    
    fun(...)#the function you want modify
  }
}

talky_sqrt&lt;- talky(fun = sqrt, mess = &quot;Hey there! You Rock!&quot;)#åˆ›å»ºè¢«ä¿®é¥°å‡½æ•°
talky_sqrt(4)#4 æ˜¯ä¼ é€’ç»™è¢«ä¿®é¥°çš„å‡½æ•°çš„</code></pre>
<pre><code>## [1] &quot;2018-04-20 09:15:36 CST&quot;
## [1] &quot;you can add anything&quot;</code></pre>
<pre><code>## [1] 2</code></pre>
<div id="run-it-or-not" class="section level3">
<h3>Run it or not ?</h3>
<pre class="r"><code>maybe &lt;- function(fun){
  function(...){
    num &lt;- sample(1:100, 1)
    if (num &gt; 50) {
      fun(...)
    }
  }
}
maybe_sqrt &lt;- maybe(fun = sqrt)
maybe_sqrt(1)</code></pre>
<pre><code>## [1] 1</code></pre>
<pre class="r"><code>maybe_sqrt(1)</code></pre>
</div>
<div id="create-a-log-file-of-a-function" class="section level3">
<h3>Create a log file of a function</h3>
<pre class="r"><code>log_calls &lt;- function(fun, file){
  function(...){
    write(as.character(Sys.time()), file, append = TRUE, sep = &quot;\n&quot;)
    fun(...)
  }
}
log_sqrt &lt;- log_calls(sqrt, file = &quot;logs&quot;)
log_sqrt(10)</code></pre>
<pre><code>## [1] 3.162278</code></pre>
<p><strong>refrence</strong> - <a href="https://colinfay.me/purrr-adverb-tidyverse/">colin Fay</a></p>
</div>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[ä¸ºä»€ä¹ˆæˆ‘çš„é”»ç‚¼åˆå¤±è´¥äº†]]></title>
    <link href="/2018/04/why-did-my-plan-fail-again-2018-04-20/"/>
    <id>/2018/04/why-did-my-plan-fail-again-2018-04-20/</id>
    <published>2018-04-20T00:00:00+00:00</published>
    <updated>2018-04-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<center>
<h2>
<strong style="color: darkred;"> å¦ˆè›‹ï¼</strong>
</h2>
</center>
<p><strong style="color: darkred;">å¥åº·é¥®é£Ÿ</strong></p>
<p><strong style="color: darkred;">åšæŒé”»ç‚¼ </strong></p>
<p><strong style="color: darkred;">å¥½å¤„å¤šå¤š</strong></p>
<p><strong style="color: darkred;">äººå°½çš†çŸ¥</strong></p>
<p>ä½†è¦çœŸæ­£åšåˆ°è¿™ä¸¤ç‚¹ çœŸå¿ƒä¸å®¹æ˜“</p>
<p>ä»Šå¤©å°±å¥½å¥½åæ€ä¸€ä¸‹ æˆ‘ä¸ºä»€ä¹ˆä¸èƒ½åšæŒé”»ç‚¼</p>
<div class="section level2">
<h2><strong style="color: darkred;">æ”¾å¼ƒçš„åŸå› :</strong></h2>
<pre><code>. æŒ‡å®šçš„è®¡åˆ’éš¾åº¦å¤ªé«˜
. çŸ­æœŸæ²¡çœ‹åˆ°æ•ˆæœå°±æ˜¯å»äº†åŠ¨åŠ›
. è¢«å„ç§çäº‹å¹²æ‰°ï¼Œæ— æ³•æ­£å¸¸æ‰§è¡Œ
. åªè¦ä¸­é€”æ–­äº†å‡ æ¬¡ï¼Œå°±ä¼šæ”¾å¼ƒé”»ç‚¼çš„ä¹ æƒ¯</code></pre>
<blockquote>
<p><strong style="color: darkred;">èƒ½è®©äººåœ¨çŸ­æœŸå†…å–å¾—æ˜¾è‘—æˆæ•ˆçš„è®¡åˆ’ï¼Œä½ éƒ½è¦ç‰¹åˆ«å°å¿ƒã€‚å› ä¸ºå¾€å¾€è¿™ç§è®¡åˆ’å¤ªè¿‡æ¿€çƒˆï¼Œæ²¡äººèƒ½é•¿æœŸåšæŒï¼Œå¾ˆå¿«ä½ å°±ä¼šæ”¾å¼ƒï¼Œå–å¾—çš„æˆæ•ˆä¹Ÿä¼šéšå®ƒè€Œå»ï¼Œç”šè‡³è¿˜æœ‰åå¼¹æ•ˆæœ</strong></p>
</blockquote>
</div>
<div id="-" class="section level2">
<h2><strong style="color: darkred;">è§£å†³åŠæ³•:</strong> æŠŠé”»ç‚¼å˜æˆåƒåˆ·ç‰™ä¸€æ ·çš„å¸¸è§„æ´»åŠ¨ï¼Œé›†ä¸­ç²¾åŠ›æƒ³å°½ä¸€åˆ‡åŠæ³•æŠŠå®ƒè¾¹åšå¸¸è§„æ´»åŠ¨</h2>
<pre><code>. ä»å°çš„è¿åŠ¨å¼€å§‹, å°½é‡ä»å°çš„å¼€å§‹ç›´åˆ°ä½ å­¦ä¼šäº†åšæŒã€‚å¯ä»¥æ…¢æ…¢å¢åŠ è¿åŠ¨é‡ï¼Œå“ªæ€•ä½ ä¸€å¼€å§‹èƒ½å¤Ÿå¤šåšä¸€äº›ï¼Œä¹Ÿä¸€å®šè¦å¿ä½ï¼Œ
. å®‰æ’å›ºå®šçš„é”»ç‚¼æ—¶é—´ï¼Œè¿™æ®µæ—¶é—´æœ€å¥½èƒ½ä¸å—åˆ°å…¶ä»–äº‹æƒ…çš„å¹²æ‰°
. å…»æˆä¹ æƒ¯ä¸­æœ€é‡è¦çš„ä¸€ç‚¹å°±æ˜¯åšæŒä¸æ‡ˆï¼Œåªè¦é”™è¿‡å‡ æ¬¡ï¼Œä½ å°±ä¼šå¼€å§‹çŠ¯æ‡’ã€‚å¤šå°‘æ¬¡éƒ½æ˜¯å› ä¸ºè¿™ä¸ªè´¥ä¸‹é˜µæ¥ï¼ŒğŸ˜”
. æœ€å¥½æ‰¾ä¸ªæ­æ¡£ï¼Œå’Œåˆ«äººçº¦å®šä¸€èµ·ï¼Œä¼šå¸®åŠ©ä½ è‡ªå¾‹</code></pre>
<p><img src="https://media.giphy.com/media/8UGoKT4boBkF9MGUb3/giphy.gif" width="30%" style="display: block; margin: auto;" /></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[algorithms: quick sort]]></title>
    <link href="/2018/04/algorithms-quick-sort/"/>
    <id>/2018/04/algorithms-quick-sort/</id>
    <published>2018-04-18T00:00:00+00:00</published>
    <updated>2018-04-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="--dcdivde-and-conquer" class="section level1">
<h1>å¿«é€Ÿæ’åº å’Œ åˆ†è€Œæ²»ä¹‹ï¼ˆD&amp;Cï¼‰divde and conquer</h1>
<ul>
<li>D&amp;C å°†é—®é¢˜é€æ­¥åˆ†è§£ï¼Œæ‰¾åˆ°æœ€åŸºæœ¬çš„æƒ…å†µä¸ºæ­¢</li>
<li>å¿«é€Ÿæ’åºï¼Œéšæœºçš„é€‰æ‹©åŸºå‡†å€¼ï¼Œå¹³å‡è¿è¡Œæ—¶é—´ä¸º**O(n*logn)**</li>
</ul>
<p><img src="https://i.loli.net/2018/04/18/5ad680ee22950.jpg" width="30%" style="display: block; margin: auto;" /></p>
<pre class="python"><code>def quicksort(array):
  if len(array) &lt; 2:# base case, arrays with 0 or 1 element are already &quot;sorted&quot;
    return array
  else:# recursive case
    pivot = array[0]# sub-array of all the elements less than the pivot
    less = [i for i in array[1:] if i &lt;= pivot]# sub-array of all the elements greater than the pivot
    greater = [i for i in array[1:] if i &gt; pivot]
    return quicksort(less) + [pivot] + quicksort(greater)
print(quicksort([10, 5, 2, 3]))
    </code></pre>
<pre><code>## [2, 3, 5, 10]</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[algorithms: recursive]]></title>
    <link href="/2018/04/algorithms-recursive/"/>
    <id>/2018/04/algorithms-recursive/</id>
    <published>2018-04-18T00:00:00+00:00</published>
    <updated>2018-04-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="section level1">
<h1>é€’å½’</h1>
<ul>
<li><strong style="color: darkred;">é€’å½’æŒ‡çš„æ˜¯è°ƒç”¨è‡ªå·±çš„å‡½æ•°</strong></li>
<li>æ¯ä¸ªé€’å½’éƒ½æœ‰ä¸¤ä¸ªæ¡ä»¶
<ul>
<li><strong style="color: darkred;">base case</strong> : ç»ˆæ­¢æ¡ä»¶</li>
<li><strong style="color: darkred;">recursive case</strong> : é€’å½’æ¡ä»¶</li>
</ul></li>
<li>æ‰€æœ‰çš„å‡½æ•°çš„è°ƒç”¨éƒ½éµå¾ª call stack</li>
<li>é€’å½’è¯­æ³•çš„æ¸…æ™°æ˜¯ä»¥å†…å­˜ä¸ºä»£ä»·çš„</li>
</ul>
<p><img src="https://i.loli.net/2018/04/18/5ad6782dbd45a.jpg" width="30%" style="display: block; margin: auto;" /></p>
</div>
<div class="section level1">
<h1>é€šè¿‡é€’å½’æ±‚é˜¶ä¹˜</h1>
<pre class="python"><code>def fact(n):
  if n == 1: #base case(ç»ˆæ­¢æ¡ä»¶)
    return 1
  else:
    return n * fact(n-1)#recursive case
    
print(fact(5))</code></pre>
<pre><code>## 120</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[workflow: Organising projects for reproducibility]]></title>
    <link href="/2018/04/workflow-organising-projects-for-reproducibility/"/>
    <id>/2018/04/workflow-organising-projects-for-reproducibility/</id>
    <published>2018-04-18T00:00:00+00:00</published>
    <updated>2018-04-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="why-reproducible" class="section level1">
<h1><strong style="color: darkred;">why reproducible</strong></h1>
<blockquote>
<p>The fundamental idea behind a <strong style="color: darkred;"><strong>robust, reproducible analysis</strong></strong> is a clean, repeatable script-based workflow (i.e.Â the sequence of tasks from the start to the end of a project) that links <strong>raw data</strong> through to <strong>clean data</strong> and to <strong>final analysis outputs</strong>.</p>
</blockquote>
<div id="principles-of-a-good-analysis-workflow" class="section level2">
<h2><strong style="color: darkred;">principles of a good analysis workflow</strong></h2>
<ol style="list-style-type: decimal">
<li>Any cleaning, merging, transforming, etc. of data <strong>should be done in scripts</strong>, not manually.</li>
<li><strong>Split your workflow (scripts) into logical thematic units</strong>. For example, you might separate your code into scripts that
<ul>
<li><ol style="list-style-type: lower-roman">
<li>load, merge and clean data</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-roman">
<li>analyse data</li>
</ol></li>
<li><ol start="3" style="list-style-type: lower-roman">
<li>produce outputs like figures and tables</li>
</ol></li>
</ul></li>
<li><p>Eliminate code duplication by <strong>packaging up useful code into custom functions</strong> (Programming: write a function). Make sure to comment your functions thoroughly, explaining their expected inputs and outputs, and what they are doing and why.</p></li>
<li><p><strong>Document your code and data as comments in your scripts</strong> or by producing separate documentation (see Programming and Reproducible reports).</p></li>
<li><p>Any intermediary outputs generated by your workflow should be kept separate from raw data. <strong>ç»“æœè¾“å‡ºåº”è¯¥å’ŒåŸå§‹æ•°æ®åˆ†å¼€</strong></p></li>
</ol>
</div>
</div>
<div id="file-system-structure" class="section level1">
<h1><strong style="color: darkred;">file system structure</strong></h1>
<ol style="list-style-type: decimal">
<li>The <code>data</code> folder contains all input data (and metadata) used in the analysis.</li>
<li>The <code>docs</code> folder contains the manuscript.</li>
<li>The <code>figs</code> directory contains figures generated by the analysis.</li>
<li>The <code>output_data</code> folder contains any type of intermediate or output files (e.g.Â simulation outputs, models, processed datasets, etc.). You might separate this and also have a cleaned-data folder.</li>
<li>The <code>scripts</code> contains R scripts with function definitions.</li>
<li>The <code>rmd</code> folder contains RMarkdown and reports files that document the analysis or report on results.</li>
</ol>
</div>
<div id="good-name-principle" class="section level1">
<h1><strong style="color: darkred;">good name principle</strong></h1>
<ol style="list-style-type: decimal">
<li>machine readable</li>
</ol>
<ul>
<li>Use delimiters to separate and make important metadata information</li>
<li>Avoid spaces, punctuation, accented characters and case sensitivity.</li>
<li>â€œ_â€ to separate metadata to be extracted as strings later on</li>
<li>â€œ-â€ instead of spaces or vice versa but do not mix</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>human readable</li>
</ol>
<ul>
<li>Ensure file names also include informative description of file contents</li>
<li>Adapt the concept of the slug to <strong>link outputs</strong> with <strong>the scripts in which they are generated</strong></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>easy to order by default</li>
<li><strong>Starting file names with a number</strong> helps.</li>
<li>For data, this might be a date allowing chronological ordering.</li>
<li>Make sure to use ISO 8601 format <strong>(YYYY-MM-DD)</strong> to avoid confusion between differing local dating conventions.</li>
<li>For scripts, you could use <strong>a number indicating the position of the scripts</strong> <strong>in the analysis sequence</strong> e.g.Â 01_download-data.R</li>
</ol>
<p><img src="https://media.giphy.com/media/QLuMqcnfCVTDIcOwG8/giphy.gif" width="30%" style="display: block; margin: auto;" /></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[algorithms: selection sort]]></title>
    <link href="/2018/04/algorithms-selection-sort/"/>
    <id>/2018/04/algorithms-selection-sort/</id>
    <published>2018-04-17T00:00:00+00:00</published>
    <updated>2018-04-17T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="section level2">
<h2>é€‰æ‹©æ’åº</h2>
<ul>
<li>è®¡ç®—æœºå†…å­˜å°±åƒä¸€å¤§å †çš„æŠ½å±‰ï¼Œ æ¯ä¸ªæŠ½å±‰éƒ½æœ‰åœ°å€</li>
<li>å­˜å‚¨å¤šä¸ªå…ƒç´ å¯ä»¥ä½¿ç”¨ï¼šæ•°ç»„å’Œé“¾è¡¨</li>
<li>æ•°ç»„çš„å…ƒç´ éƒ½åœ¨ä¸€èµ·</li>
<li>é“¾è¡¨çš„å…ƒç´ æ˜¯åˆ†å¼€çš„ï¼Œæ¯ä¸ªå…ƒç´ éƒ½å­˜ç€ä¸‹ä¸€ä¸ªå…ƒç´ çš„åœ°å€</li>
<li>æ•°ç»„çš„è¯»å–é€Ÿåº¦å¾ˆå¿«</li>
<li>é“¾è¡¨çš„æ’å…¥å’Œåˆ é™¤é€Ÿåº¦å¾ˆå¿«</li>
<li>åœ¨åŒä¸€ä¸ªæ•°ç»„ä¸­ï¼Œæ‰€æœ‰çš„å…ƒç´ ç±»å‹å¿…é¡»ä¸€è‡´ <img src="/Users/zero/Documents/_14_tmp/jixingBlogdown/public/post/2018-04-17-algorithms-selection-sort_files/%E5%86%85%E5%AD%98.png" width="30%" style="display: block; margin: auto;" /></li>
</ul>
<p><img src="/Users/zero/Documents/_14_tmp/jixingBlogdown/public/post/2018-04-17-algorithms-selection-sort_files/Screen%20Shot%202018-04-17%20at%2007.29.00.png" width="30%" style="display: block; margin: auto;" /></p>
<pre class="python"><code>def findSmallest(arr):
    smallest = arr[0] #åˆå§‹åŒ–ï¼šå­˜å‚¨æœ€å°çš„å€¼
    smallest_index=0 #åˆå§‹åŒ–: å­˜å‚¨æœ€å°å€¼çš„ç´¢å¼•
    for i in range(1, len(arr)):
        if arr[i] &lt; smallest:
            smallest=arr[i]
            smallest_index=i
    return smallest_index
def selectionSort(arr): #å¯¹æ•°æ®è¿›è¡Œæ’åº
    newArr=[]
    for i in range(len(arr)):
        smallest=findSmallest(arr)#æ‰¾å‡ºæ•°æ®ä¸­æœ€å°çš„å…ƒç´ ï¼Œå¹¶å°†å…¶åŠ åˆ°æ–°çš„æ•°ç»„ä¸­å»
        newArr.append(arr.pop(smallest))
    return newArr
print(selectionSort([5,3,6,2]))</code></pre>
<pre><code>## [2, 3, 5, 6]</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hierarchical Clustering]]></title>
    <link href="/2018/04/hierarchical-clustering/"/>
    <id>/2018/04/hierarchical-clustering/</id>
    <published>2018-04-16T00:00:00+00:00</published>
    <updated>2018-04-16T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="what-is-clustering" class="section level1">
<h1>What Is Clustering</h1>
<blockquote>
<p><strong>Clustering</strong> refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. Of course, to make this concrete, we must define what it means for two or more observations to be similar or different. Indeed, this is often a domain-specific consideration that must be made based on knowledge of the data being studied.</p>
</blockquote>
</div>
<div id="why-hierarachical-clustering" class="section level1">
<h1>Why Hierarachical Clustering</h1>
<blockquote>
<p>For instance, suppose that we have a set of n observations, each with p features. The n observations could correspond to tissue samples for patients with breast cancer, and the p features could correspond to measurements collected for each tissue sample; these could be clinical measurements, such as tumor stage or grade, or they could be gene expression measurements. We may have a reason to believe that there is some heterogeneity among the n tissue samples; for instance, perhaps there are a few different unknown subtypes of breast cancer. Clustering could be used to find these subgroups. This is an unsupervised problem because we are trying to discover structure in this case, distinct clusters on the basis of a data set. The goal in supervised problems, on the other hand, is to try to predict some outcome vector such as survival time or response to drug treatment</p>
</blockquote>
</div>
<div id="how-do-hierarachical-clustering-bottom-up" class="section level1">
<h1>How do hierarachical clustering: bottom-up</h1>
<div id="key-operation-repeatedly-combine-two-nearest-clusters" class="section level2">
<h2>Key operation: repeatedly combine two <strong>nearest clusters</strong></h2>
</div>
<div id="three-important-question" class="section level2">
<h2>three important question</h2>
<div id="how-do-you-represent-a-cluster-of-more-than-one-point" class="section level3">
<h3>1. how do you represent a cluster of more than one point?</h3>
<ul>
<li>how do you represent the location of each cluster, to tell which pair of cluster is closest?</li>
<li>represent each cluster by its centroid=average of its point</li>
</ul>
</div>
<div id="how-do-you-determine-the-nearness-of-clusters" class="section level3">
<h3>2. how do you determine the â€œnearnessâ€ of clusters?</h3>
<ul>
<li>measure cluster distances by distances of centroids</li>
</ul>
</div>
<div id="when-to-stop-combining-clusters" class="section level3">
<h3>3. when to stop combining clusters?</h3>
<ul>
<li>pick a number k upfront, and stop when have k clusters</li>
<li>stop when the next merge would creat a cluster with low â€œcohesionâ€</li>
</ul>
</div>
</div>
</div>
<div id="eg-stat-question" class="section level1">
<h1>Eg: stat question</h1>
</div>
<div id="resuilt" class="section level1">
<h1>resuilt</h1>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Row Combine Many Table In R ]]></title>
    <link href="/2018/04/row-combine-many-table-in-r/"/>
    <id>/2018/04/row-combine-many-table-in-r/</id>
    <published>2018-04-11T00:00:00+00:00</published>
    <updated>2018-04-11T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>Q: I has many separate tables that need to be combined into a single file?</strong></p>
<p>google search â€œR read many datasets or tablesâ€</p>
<div id="three-steps" class="section level1">
<h1><strong>Three steps</strong>:</h1>
<ol style="list-style-type: decimal">
<li>Getting a list of files path to read</li>
<li>Write a function to read a file</li>
<li>Then loop it <br></li>
</ol>
<div id="step01-list-all-files-path" class="section level2">
<h2>step01: list all files path</h2>
<pre class="r"><code>library(here) 
allfiles = list.files(path = here(&quot;data&quot;), #Use the â­here package to indicate the directory the files are in relative to the root directory
                        pattern = &quot;AB.csv|ab.csv&quot;,#tell R which file paths should be listed
                        full.names = TRUE,
                        recursive = TRUE) #indicate whether or not child folders in the parent directory should be searched for files to list or not</code></pre>
</div>
<div id="step02-write-a-function-to-read-a-single-file" class="section level2">
<h2>step02: write a function to read a single file</h2>
<pre class="r"><code>library(stringr)

#eg_path: data/Block1/Siteone/SIT1_17_12_21_5.2_AB.csv
read_fun = function(path) {
     test = read.csv(path, 
                skip = 6,
                header = FALSE,
                col.names = c(&quot;date&quot;, &quot;temperature&quot;) )
     allnames = str_split( path, pattern = &quot;/&quot;, simplify = TRUE)
     test$block = allnames[, ncol(allnames) - 2] 
     test$site = allnames[, ncol(allnames) - 1] #The information on the physical units of the study, â€œBlocksâ€ and â€œSitesâ€
     test$plot = str_extract(allnames[, ncol(allnames)], pattern = &quot;[0-9](?=\\.)&quot;)
     test$logloc = toupper( str_sub(allnames[, ncol(allnames)], start = -6, end = -5) )
     test
}</code></pre>
<p>step03: read all files</p>
<p>If using either of for or lapply, the final concatenation step can be done via rbind in do.call</p>
<p><code>map_dfr</code> looks like, looping through each element of allfiles to read and modify the datasets with the read_fun function and then stacking everything together into a final combined dataset</p>
<pre class="r"><code>library(furrr)
library(future)
plan(multiprocess)
library(tictoc)
tic()
( combined_dat = future_map_dfr(allfiles, read_fun, .progress = TRUE) )</code></pre>
<pre><code>##    date temperature  block    site plot logloc
## 1    15           9 Block1 Siteone    5     AB
## 2    16           8 Block1 Siteone    5     AB
## 3    17          15 Block1 Siteone    5     AB
## 4    18           9 Block1 Siteone    5     AB
## 5    19          10 Block1 Siteone    5     AB
## 6     1          12 Block1 Siteone    2     AB
## 7     2          15 Block1 Siteone    2     AB
## 8     3          21 Block1 Siteone    2     AB
## 9     4          20 Block1 Siteone    2     AB
## 10    5          20 Block1 Siteone    2     AB
## 11    6          13 Block1 Siteone    2     AB
## 12    1          10 Block1 Siteone    5     AB
## 13    2          19 Block1 Siteone    5     AB
## 14    3          17 Block1 Siteone    5     AB
## 15    4           6 Block1 Siteone    5     AB
## 16    5           5 Block1 Siteone    5     AB
## 17    6          10 Block1 Siteone    5     AB
## 18    7          15 Block1 Siteone    5     AB
## 19    8          16 Block1 Siteone    5     AB
## 20    9          10 Block1 Siteone    5     AB
## 21    1           9 Block2 Sitenew    3     AB
## 22    2           8 Block2 Sitenew    3     AB
## 23    3          15 Block2 Sitenew    3     AB
## 24    5          10 Block2 Sitenew    3     AB
## 25    6           9 Block2 Sitenew    3     AB
## 26    7          10 Block2 Sitenew    3     AB
## 27    8           8 Block2 Sitenew    3     AB
## 28    1          11 Block2 Sitenew    5     AB
## 29    2          12 Block2 Sitenew    5     AB
## 30    3          13 Block2 Sitenew    5     AB
## 31    4          18 Block2 Sitenew    5     AB
## 32    5          19 Block2 Sitenew    5     AB
## 33    6          18 Block2 Sitenew    5     AB
## 34    8          19 Block2 Sitenew    5     AB
## 35    7          18 Block2 Sitenew    5     AB
## 36    9          19 Block2 Sitenew    5     AB
## 37   10          10 Block2 Sitenew    5     AB</code></pre>
<pre class="r"><code>toc() </code></pre>
<pre><code>## 2.921 sec elapsed</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Build Webapp In R Using Shiny]]></title>
    <link href="/2018/04/build-webapp-in-r-using-shiny/"/>
    <id>/2018/04/build-webapp-in-r-using-shiny/</id>
    <published>2018-04-09T00:00:00+00:00</published>
    <updated>2018-04-09T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>How to build shiny app from scratch in R</strong>?</p>
<p>You can build web page (online reporting tool) without knowing any web programming languages such as Javascript / PHP / CSS.</p>
<p>The best part about shiny package is that you can easily integrate R with webpage.</p>
<p>Suppose you want your web page run machine learning algorithms like random forest, SVM etc and display summary of the model with the flexibility of selecting inputs from user.</p>
<p>Shiny can do it?</p>
<div id="shinys-prominent-features" class="section level2">
<h2><strong>Shinyâ€™s prominent features</strong></h2>
<ol style="list-style-type: decimal">
<li>Customizable widgets like sliders, drop down lists, numeric inputs and many more.</li>
<li><strong>Downloading</strong> datasets, graphs and tables in various formats.</li>
<li><strong>Uploading</strong> files.</li>
<li>Provides utility to <strong>create brilliant plots</strong>.</li>
<li>In-built functions for viewing data or printing the text or summaries.</li>
<li>Reactive programs which makes data handling easier.</li>
<li>Conditional Panels for only when a particular condition is present.</li>
<li>Works in any R environment (Console R, RGUI for Windows or Mac,Â  RStudio, etc)</li>
<li>No need to learn another software for online dashboarding</li>
<li><strong>Can style your app with CSS</strong> / HTML (Optional)</li>
</ol>
<p>Two must things in shiny 1. UI: user interface which is controlled by ui script 2. Sever: It contains the instructions that your computer needs when the user interacts with the app</p>
</div>
<div id="basic-layout-of-ui" class="section level2">
<h2><strong>Basic layout of UI</strong></h2>
<p><code>User Interface:Â </code>A simple shiny UI consists of aÂ <strong>fluidpageÂ </strong>which contains various panels. We can divide the display in two parts namedÂ <strong>sidebdarPanel( )Â </strong>Â andÂ <strong>mainPanel( ).</strong>Â Both of the panels can be accessed usingÂ <strong>sidebarLayout( ).</strong></p>
<ol style="list-style-type: decimal">
<li><strong>Title panelÂ </strong>is a place where the title of the app is displayed.</li>
<li><strong>Sidebar panel</strong>Â is where special instructions or widgets (drop down / slider/ checkbox) are displayed to the app user.Â The sidebar panel appears on the left side of your app by default. You can move it to the right side by changing the position argument in the sidebar layout.</li>
<li><strong>Main panel</strong>Â is the area where all the outputs are generally placed.</li>
</ol>
<p><img src="https://i.loli.net/2018/04/09/5acac07887856.jpg" width="30%" style="display: block; margin: auto;" /></p>
</div>
<div id="the-first-simple-shiny-app-with-basic-layout" class="section level2">
<h2><strong>The first simple shiny app with basic layout</strong></h2>
<pre class="r"><code>library(shiny)
ui = fluidPage(sidebarLayout(sidebarPanel(&quot;Welcome to Shiny App&quot;),
                             mainPanel(&quot;This is main Panel&quot;)))
server = function(input, output) {  }
shinyApp(ui, server)</code></pre>
</div>
<div id="guidelines-for-beginners-to-run-a-shiny-app" class="section level2">
<h2><strong>Guidelines for beginners to run a shiny app</strong></h2>
<p><strong>Step 1 :Â shinyApp(ui,server)</strong>Â <strong>:Â </strong>It is an in-built function in shiny package to run the app with ui and server as the arguments. Select the code and run it. Once you do it successfully, you would find the textÂ **Listening on <a href="http://127.0.0.1:4692**" class="uri">http://127.0.0.1:4692**</a>Â on console.</p>
<p><strong>Step 2 :Â </strong>To create your app you need to save the code as anÂ <strong>app.R</strong>Â file and aÂ <strong>RunAppÂ </strong>icon will get displayed on your screen. Click on it and a new prompt window as your app will appear.</p>
</div>
<div id="using-html-tags-in-shiny" class="section level2">
<h2><strong>Using HTML tags in Shiny</strong></h2>
<p>Content can be added in various panels. To change theÂ <strong>appearance of the text by bolds, italics, images, changing the fonts and colors, adding heading</strong>Â etc. we can use various HTML functions in shiny. Some of them being the same in both of them are:</p>
<p><img src="https://i.loli.net/2018/04/09/5acac1e2659cd.jpg" width="30%" style="display: block; margin: auto;" /></p>
</div>
<div id="creating-a-hyperlink" class="section level2">
<h2><strong>Creating a hyperlink</strong></h2>
<p>A hyperlink can be created usingÂ <strong>a( )Â </strong>where the first argument is the text with which the link is attached.Â <strong>href</strong>Â contains the link for our website which we want to attach.</p>
<div id="modifying-the-text-presentation-using-html-tags." class="section level3">
<h3><strong>Modifying the text presentation using HTML tags.</strong></h3>
<p>We create an app containing the list of the favorite novels . You can refer to the above mentioned table of HTML and shiny functions.</p>
<p><img src="https://i.loli.net/2018/04/09/5acac3fc6bb2b.jpg" width="30%" style="display: block; margin: auto;" /></p>
</div>
<div id="introducing-widgets" class="section level3">
<h3><strong>Introducing widgets</strong></h3>
<p>Various widgets are used in shiny to select various outputs. These widgets can be inserted in the ui function (anywhere in the main panel and sidebar panel).<br />
The most commonly used widgets are:</p>
<p><img src="https://i.loli.net/2018/04/09/5acac49acf925.jpg" width="30%" style="display: block; margin: auto;" /><img src="https://i.loli.net/2018/04/09/5acac4b608308.jpg" width="30%" style="display: block; margin: auto;" /></p>
<ul>
<li>â€˜Buttonsâ€™ can be created using anÂ <strong>actionButtonÂ </strong>andÂ <strong>submitButtonÂ </strong>widgets</li>
<li>Single check box, multiple check box and date inputs are created usingÂ <strong>checkboxInput</strong>,Â <strong>checkboxGroupInputÂ </strong>andÂ <strong>dateInputÂ </strong>respectively.</li>
<li>Date range is created usingÂ <strong>dateRangeInput</strong>.</li>
</ul>
<p>More detail see: <a href="https://www.listendata.com/2018/02/shiny-tutorial-r.html">Most commonly used widgets</a></p>
</div>
<div id="sharing-the-app-with-others" class="section level3">
<h3><strong>Sharing the app with others</strong></h3>
<p><strong>Share your app as a web page:</strong>Â You need to create an account on<a href="http://shinyapps.io/"><strong>shinyapps.io</strong></a>Â and follow the instructions below to share your app.R file.</p>
</div>
<div id="deploying-shiny-app-on-shinyapps.io" class="section level3">
<h3><strong>Deploying shiny app on shinyapps.io</strong></h3>
<p>First you need to have an account on shinyapps.io.</p>
<p>Import libraryÂ <strong>rsconnectÂ </strong>by using</p>
<blockquote>
<p><code>library(rsconnect)</code>Â </p>
</blockquote>
<p>Then you need to configure theÂ <strong>rsconnectÂ </strong>package to your account using the code below -</p>
<blockquote>
<p><code>rsconnect::setAccountInfo(name=&quot;&lt;ACCOUNT&gt;&quot;, token=&quot;&lt;TOKEN&gt;&quot;, secret=&quot;&lt;SECRET&gt;&quot;)</code></p>
</blockquote>
<p>To deploy the app you can write:Â </p>
<blockquote>
<p><code>rsconnect::deployApp(' Folder path in which your app.R file is saved')Â </code></p>
</blockquote>
<p>Â As a result a new web page of your app link will be opened.Â </p>
</div>
</div>
<div id="links" class="section level2">
<h2><strong>Links</strong></h2>
<ul>
<li><a href="https://www.listendata.com/2018/02/shiny-tutorial-r.html">shiny</a></li>
</ul>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Run Python From R]]></title>
    <link href="/2018/04/run-python-from-r/"/>
    <id>/2018/04/run-python-from-r/</id>
    <published>2018-04-03T00:00:00+00:00</published>
    <updated>2018-04-03T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>R</strong> is mainly known for data analysis, statistical modeling and visualization. While <strong>python</strong> is popular for deep learning and natural language processing.</p>
<p>Python and R were ranked top 2 tools for data science and machine learning. If you really want to boost your career in data science world, these are the languages you need to focus on.</p>
<div id="how-to-call-or-run-python-from-r" class="section level1">
<h1><strong>How To Call Or Run Python From R?</strong></h1>
<p>RStudio developed a package called <strong>reticulate</strong> which provides a medium to run Python packages and functions from R.</p>
<pre class="r"><code># Load reticulate package
if (!require(&quot;reticulate&quot;)) install.packages(&quot;reticulate&quot;)</code></pre>
<pre><code>## Loading required package: reticulate</code></pre>
</div>
<div id="python-version-configuration" class="section level1">
<h1><a href="https://rstudio.github.io/reticulate/articles/versions.html">Python Version Configuration</a></h1>
<p>If the version of Python you want to use is located on the system PATH then it will be automatically discovered (via Sys.which) and used.</p>
<p>Alternatively, you can use one of the following functions to specify alternate versions of Python:</p>
<p>Function Description</p>
<ul>
<li><p><a href="https://rstudio.github.io/reticulate/reference/use_python.html">use_python</a> Specify the path a specific Python binary.</p></li>
<li><p><a href="https://rstudio.github.io/reticulate/reference/use_python.html">use_virtualenv</a> Specify the directory containing a Python virtualenv.</p></li>
<li><p><a href="https://rstudio.github.io/reticulate/reference/use_python.html">use_condaenv</a> Specify the name of a Conda environment.</p></li>
</ul>
</div>
<div id="import-a-python-module-within-r" class="section level1">
<h1>ğŸ”¥ import a python module within R</h1>
<pre class="r"><code>os &lt;- import(&quot;os&quot;)
pd &lt;- import(&quot;pandas&quot;)
numpy &lt;- import(&quot;numpy&quot;)
py_module_available(&quot;pandas&quot;)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>os$getcwd()</code></pre>
<pre><code>## [1] &quot;/Users/zero/myrepo/jixingBlogdown/content/post&quot;</code></pre>
<p><strong style="color: darkred;">Install Python package with conda in terminal</strong></p>
<div id="working-with-numpy" class="section level2">
<h2>working with <strong>numpy</strong></h2>
<pre class="r"><code>y &lt;- array(1:4, c(2, 2)) # create a matrix with R
x &lt;- numpy$array(y) # edit it with python
numpy$transpose(y) # transpose the above array</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1    2
## [2,]    3    4</code></pre>
<pre class="r"><code>numpy$linalg$eig(y) # Eigenvalues and eigen vectors</code></pre>
<pre><code>## [[1]]
## [1] -0.3722813  5.3722813
## 
## [[2]]
##            [,1]       [,2]
## [1,] -0.9093767 -0.5657675
## [2,]  0.4159736 -0.8245648</code></pre>
<pre class="r"><code>numpy$sqrt(x)</code></pre>
<pre><code>##          [,1]     [,2]
## [1,] 1.000000 1.732051
## [2,] 1.414214 2.000000</code></pre>
<pre class="r"><code>numpy$exp(x)</code></pre>
<pre><code>##          [,1]     [,2]
## [1,] 2.718282 20.08554
## [2,] 7.389056 54.59815</code></pre>
</div>
</div>
<div id="sourcing-python-scripts" class="section level1">
<h1>ğŸ”¥ <strong style="color: darkred;">Sourcing Python scripts</strong></h1>
<div id="call-the-python-fuction-in-r" class="section level2">
<h2><a href="%5B*Access%20objects%20created%20in%20python%20from%20R***:%5D(%5B*Access%20objects%20created%20in%20python%20from%20R**%5D(https://rviews.rstudio.com/2018/04/17/reticulated-shiny/))"><strong>Call the python fuction in R:</strong></a></h2>
<p>You can source any Python script just as you would source an R script using the source_python() function, then call the fuction in R.</p>
<pre class="r"><code>#use Python functions
source_python(&quot;../../scripts/add.py&quot;)# minus in action
add(1,1)</code></pre>
<pre><code>## [1] 0</code></pre>
</div>
<div id="access-objects-created-in-python-from-r" class="section level2">
<h2><a href="%5B*Access%20objects%20created%20in%20python%20from%20R**%5D(https://rviews.rstudio.com/2018/04/17/reticulated-shiny/)"><em>Access objects created in python from R</em>:</a></h2>
<p>If your Python file doesnâ€™t contain functions, but also creates objects, use <strong style="color: darkred;">py_run_file</strong> instead of <strong style="color: darkred;">source_python</strong></p>
</div>
</div>
<div id="access-objects-created-in-python-from-r-1" class="section level1">
<h1>â­<strong>Access objects created in python from R</strong></h1>
<p>with <strong style="color: darkred;">repl_python()</strong> <strong>note: Donâ€™t work with Rmd</strong></p>
<pre class="r"><code>repl_python()
#===== python console====

# Load Pandas package
import pandas as pd

# Importing Dataset
travel = pd.read_excel(&quot;data/AIR.xlsx&quot;)

# Number of rows and columns
travel.shape

# Select random no. of rows 
travel.sample(n = 10)

# Group By
travel.groupby(&quot;Year&quot;).AIR.mean()

# Filter
t = travel.loc[(travel.Month &gt;= 6) &amp; (travel.Year &gt;= 1955),:]

# Return to R
exit

#==== R console====
# Access objects created in python from R
summary(py$t)

# Line chart using ggplot2
library(ggplot2)
ggplot(py$t, aes(AIR, Year)) + geom_line()</code></pre>
</div>
<div id="access-objects-created-in-r-from-python" class="section level1">
<h1>â­<strong>Access objects created in R from Python</strong></h1>
<pre class="r"><code># Let&#39;s create a object in R
#===== R console====
mydata = head(cars, n=15)

#Use the R created object within Python REPL
repl_python()
#===== python console====
import pandas as pd
#Access objects created in R from Python: r.mydata
r.mydata.describe()
pd.isnull(r.mydata.speed)

exit</code></pre>
</div>
<div id="building-logistic-regression-model-using-sklearn-package" class="section level1">
<h1>â­Building Logistic Regression Model using sklearn package</h1>
<pre class="r"><code>repl_python()

# Load libraries
from sklearn import datasets
from sklearn.linear_model import LogisticRegression

# load the iris datasets
iris = datasets.load_iris()

# Developing logit model
model = LogisticRegression()
model.fit(iris.data, iris.target)

# Scoring
actual = iris.target
predicted = model.predict(iris.data)

# Performance Metrics
print(metrics.classification_report(actual, predicted))
print(metrics.confusion_matrix(actual, predicted))</code></pre>
<div id="links" class="section level2">
<h2><strong>Links</strong></h2>
<ul>
<li><p><a href="https://www.diycode.cc/projects/rstudio/reticulate">R Interface to Python</a></p></li>
<li><p><a href="https://blog.rstudio.com/2018/03/26/reticulate-r-interface-to-python/">reticulate: R interface to Python</a></p></li>
<li><p><a href="https://rviews.rstudio.com/2018/04/17/reticulated-shiny/"><strong style="color: darkred;">Reticulated Shiny</strong></a></p></li>
</ul>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[9 Ensembling The Predictions]]></title>
    <link href="/2018/04/9-ensembling-the-predictions/"/>
    <id>/2018/04/9-ensembling-the-predictions/</id>
    <published>2018-04-01T00:00:00+00:00</published>
    <updated>2018-04-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code>load(&quot;../../data/craet_8.Rdata&quot;)
library(tidyverse)
library(caret)
#Set Parallel Processing - Decrease computation time
if (!require(&quot;doMC&quot;)) install.packages(&quot;doMC&quot;)
library(doMC)
registerDoMC(cores = 4)</code></pre>
</div>
<div id="train-multiple-models" class="section level1">
<h1>Train Multiple Models</h1>
<p>So now we have predictions from multiple individual models.To do this we had to run the train() function once for each model, store the models and pass it to the res</p>
<pre class="r"><code>library(caretEnsemble)

# Stacking Algorithms - Run multiple algos in one call.
trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, 
                             number=10, 
                             repeats=3,
                             savePredictions=TRUE, 
                             classProbs=TRUE)

algorithmList &lt;- c(&#39;rf&#39;, &#39;adaboost&#39;, &#39;earth&#39;, &#39;svmRadial&#39;)

set.seed(100)
models &lt;- caretList(Purchase ~ ., data=trainData, trControl=trainControl, methodList=algorithmList) 

results &lt;- resamples(models)
summary(results)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = results)
## 
## Models: rf, adaboost, earth, svmRadial 
## Number of resamples: 30 
## 
## Accuracy 
##                Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## rf        0.7011494 0.7764706 0.7965116 0.8033148 0.8250684 0.9058824    0
## adaboost  0.6823529 0.7674419 0.7906977 0.7966532 0.8328659 0.8941176    0
## earth     0.7209302 0.7906977 0.8187415 0.8164175 0.8367305 0.8604651    0
## svmRadial 0.7558140 0.7948276 0.8304378 0.8261842 0.8588235 0.9058824    0
## 
## Kappa 
##                Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## rf        0.3518625 0.5184810 0.5504351 0.5737290 0.6253768 0.8040346    0
## adaboost  0.3349754 0.5046620 0.5686668 0.5711983 0.6423870 0.7831018    0
## earth     0.4102857 0.5609657 0.6148850 0.6095470 0.6580869 0.7147595    0
## svmRadial 0.4685109 0.5645744 0.6326120 0.6285652 0.6993397 0.7996464    0</code></pre>
<pre class="r"><code># Box plots to compare models
scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;))
bwplot(results, scales=scales)</code></pre>
<p><img src="/post/2018-04-01-9-ensembling-the-predictions_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="combine-the-predictions-of-multiple-models-to-form-a-final-prediction" class="section level1">
<h1>Combine The Predictions Of Multiple Models To Form A Final Prediction</h1>
<ul>
<li>One thought: Is it possible to combine these predicted values from multiple models somehow and make a new ensemble that predicts better?</li>
<li>another thought: using the caretStack(). <strong>You just need to make sure you donâ€™t use the same trainControl you used to build the models</strong></li>
</ul>
<pre class="r"><code># Create the trainControl
set.seed(101)
stackControl &lt;- trainControl(method=&quot;repeatedcv&quot;, 
                             number=10, 
                             repeats=3,
                             savePredictions=TRUE, 
                             classProbs=TRUE)

# Ensemble the predictions of `models` to form a new combined prediction based on glm
# åœ¨åŸæœ‰æ¨¡å‹çš„åŸºç¡€ä¸Šå åŠ  ä¸€èˆ¬çº¿æ€§æ¨¡å‹ ä½œä¸ºé¢„æµ‹
stack.glm &lt;- caretStack(models, method=&quot;glm&quot;, metric=&quot;Accuracy&quot;, trControl=stackControl)
print(stack.glm)</code></pre>
<pre><code>## A glm ensemble of 2 base models: rf, adaboost, earth, svmRadial
## 
## Ensemble results:
## Generalized Linear Model 
## 
## 2571 samples
##    4 predictor
##    2 classes: &#39;CH&#39;, &#39;MM&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 2314, 2314, 2314, 2314, 2313, 2313, ... 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.8321128  0.6419638</code></pre>
<pre class="r"><code># Predict on testData
stack_predicteds &lt;- predict(stack.glm, newdata=testData4)
head(stack_predicteds)</code></pre>
<pre><code>## [1] CH CH CH CH CH MM
## Levels: CH MM</code></pre>
<pre class="r"><code>save.image(&quot;../../data/craet_9.Rdata&quot;)</code></pre>
<p><strong>A point to consider: The ensembles tend to perform better if the predictions are less correlated with each other.</strong></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[8 How To Evaluate Performance Of Multiple Machine Learning Algorithms?]]></title>
    <link href="/2018/03/8-how-to-evaluate-performance-of-multiple-machine-learning-algorithms/"/>
    <id>/2018/03/8-how-to-evaluate-performance-of-multiple-machine-learning-algorithms/</id>
    <published>2018-03-31T00:00:00+00:00</published>
    <updated>2018-03-31T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code>load(&quot;../../data/craet_7.Rdata&quot;)
library(tidyverse)
library(caret)
#Set Parallel Processing - Decrease computation time
if (!require(&quot;doMC&quot;)) install.packages(&quot;doMC&quot;)
library(doMC)
registerDoMC(cores = 4)</code></pre>
</div>
<div id="caret-provides-the-resamples-function-where-you-can-provide-multiple-machine-learning-models-and-collectively-evaluate-them" class="section level1">
<h1>Caret provides the resamples() function where you can provide multiple machine learning models and collectively evaluate them</h1>
<div id="define-the-training-control" class="section level2">
<h2>Define the training control</h2>
<pre class="r"><code>fitControl &lt;- trainControl(
    method = &#39;cv&#39;,                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = &#39;final&#39;,       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) </code></pre>
</div>
<div id="train-models" class="section level2">
<h2>train models</h2>
<pre class="r"><code>set.seed(100)

# Training Adaboost using adaboost
model_adaboost = train(Purchase ~ ., data=trainData, method=&#39;adaboost&#39;, tuneLength=2, trControl = fitControl)

# Training Random Forest model using rf
model_rf = train(Purchase ~ ., data=trainData, method=&#39;rf&#39;, tuneLength=5, trControl = fitControl)

# Training xgBoost Dart
#model_xgbDART = train(Purchase ~ ., data=trainData, method=&#39;xgbDART&#39;, tuneLength=5, trControl = fitControl, verbose=F)

# Train SVM using MARS
model_svmRadial = train(Purchase ~ ., data=trainData, method=&#39;svmRadial&#39;, tuneLength=15, trControl = fitControl)</code></pre>
</div>
</div>
<div id="run-resamples-to-compare-the-models" class="section level1">
<h1>Run resamples() to compare the models</h1>
<pre class="r"><code># Compare model performances using resample()
models_compare &lt;- resamples(list(ADABOOST=model_adaboost, RF=model_rf, MARS=model_mars3, SVM=model_svmRadial))

# Summary of the models performances
summary(models_compare)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = models_compare)
## 
## Models: ADABOOST, RF, MARS, SVM 
## Number of resamples: 5 
## 
## ROC 
##               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## ADABOOST 0.8126510 0.8462687 0.8682549 0.8657598 0.8868515 0.9147727    0
## RF       0.8635394 0.8647908 0.8748565 0.8841388 0.9046198 0.9128875    0
## MARS     0.8520967 0.8660981 0.9091561 0.8953757 0.9118590 0.9376688    0
## SVM      0.8769723 0.8839375 0.8902597 0.8895479 0.8948048 0.9017652    0
## 
## Sens 
##               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## ADABOOST 0.7619048 0.7904762 0.7904762 0.8070330 0.8076923 0.8846154    0
## RF       0.7809524 0.8000000 0.8461538 0.8451832 0.8750000 0.9238095    0
## MARS     0.8190476 0.8476190 0.8857143 0.8739377 0.8942308 0.9230769    0
## SVM      0.8750000 0.8761905 0.8761905 0.8891209 0.9047619 0.9134615    0
## 
## Spec 
##               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## ADABOOST 0.7014925 0.7462687 0.7727273 0.7635007 0.7761194 0.8208955    0
## RF       0.6119403 0.6363636 0.7462687 0.7332429 0.8208955 0.8507463    0
## MARS     0.6567164 0.7164179 0.7313433 0.7454998 0.7424242 0.8805970    0
## SVM      0.6969697 0.7164179 0.7313433 0.7364089 0.7611940 0.7761194    0</code></pre>
<pre class="r"><code># Draw box plots to compare models
scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;))
bwplot(models_compare, scales=scales)</code></pre>
<p><img src="/post/2018-03-31-8-how-to-evaluate-performance-of-multiple-machine-learning-algorithms_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>save.image(&quot;../../data/craet_8.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[7 How To Do Hyperparameter Tuning ]]></title>
    <link href="/2018/03/7-how-to-do-hyperparameter-tuning/"/>
    <id>/2018/03/7-how-to-do-hyperparameter-tuning/</id>
    <published>2018-03-30T00:00:00+00:00</published>
    <updated>2018-03-30T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code>load(&quot;../../data/craet_6.Rdata&quot;)
library(tidyverse)
library(caret)
# Set Parallel Processing - Decrease computation time
if (!require(&quot;doMC&quot;)) install.packages(&quot;doMC&quot;)
library(doMC)
registerDoMC(cores = 4)</code></pre>
<div id="hyper-parameter-tuning-using-tunegrid" class="section level2">
<h2>Hyper parameter tuning using tuneGrid</h2>
<ol style="list-style-type: decimal">
<li><p>Model Tuning Parameter Set</p></li>
<li><p>Cross Validation Set</p>
<strong>Cross validationÂ <code>method</code>Â can be one amongst</strong>:
<ul>
<li>â€˜bootâ€™: Bootstrap sampling</li>
<li>â€˜boot632â€™: Bootstrap sampling with 63.2% bias correction applied</li>
<li>â€˜optimism_bootâ€™: The optimism bootstrap estimator</li>
<li>â€˜boot_allâ€™: All boot methods.</li>
<li>â€˜cvâ€™: k-Fold cross validation</li>
<li>â€˜repeatedcvâ€™: Repeated k-Fold cross validation</li>
<li>â€˜oobâ€™: Out of Bag cross validation</li>
<li>â€˜LOOCVâ€™: Leave one out cross validation</li>
<li>â€˜LGOCVâ€™: Leave group out cross validation</li>
</ul></li>
<li><p>Training And Tuning</p></li>
<li><p>Predict</p></li>
<li><p>Confusion Matrix</p></li>
</ol>
<pre class="r"><code># Step 1: Define the tuneGrid
marsGrid &lt;-  expand.grid(nprune = c(2, 4, 6, 8, 10), 
                        degree = c(1, 2, 3))

# Step 2: Define the training control
fitControl &lt;- trainControl(
    method = &#39;cv&#39;,                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = &#39;final&#39;,       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 

# Step 3: Training and Tuning hyper parameters by setting tuneGrid
set.seed(100)
model_mars3 = train(Purchase ~ ., data=trainData, method=&#39;earth&#39;, metric=&#39;ROC&#39;, tuneGrid = marsGrid, trControl = fitControl)
model_mars3</code></pre>
<pre><code>## Multivariate Adaptive Regression Spline 
## 
## 857 samples
##  18 predictor
##   2 classes: &#39;CH&#39;, &#39;MM&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 685, 685, 687, 686, 685 
## Resampling results across tuning parameters:
## 
##   degree  nprune  ROC        Sens       Spec     
##   1        2      0.8745398  0.8700916  0.7006784
##   1        4      0.8924657  0.8662454  0.7394844
##   1        6      0.8912361  0.8719414  0.7334238
##   1        8      0.8886974  0.8661722  0.7334238
##   1       10      0.8879988  0.8623626  0.7423790
##   2        2      0.8745398  0.8700916  0.7006784
##   2        4      0.8953757  0.8739377  0.7454998
##   2        6      0.8917824  0.8681868  0.7515152
##   2        8      0.8904559  0.8624359  0.7574401
##   2       10      0.8932377  0.8547436  0.7784261
##   3        2      0.8582783  0.8777106  0.6618725
##   3        4      0.8914544  0.8662454  0.7544550
##   3        6      0.8910605  0.8586264  0.7665310
##   3        8      0.8838647  0.8452015  0.7456355
##   3       10      0.8827056  0.8471062  0.7426504
## 
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were nprune = 4 and degree = 2.</code></pre>
<pre class="r"><code># Step 4: Predict on testData 
predicted3 &lt;- predict(model_mars3, testData4)

# Step 5: Compute the confusion matrix
confusionMatrix(reference = testData$Purchase, data = predicted3, mode=&#39;everything&#39;, positive=&#39;MM&#39;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  CH  MM
##         CH 117  21
##         MM  13  62
##                                           
##                Accuracy : 0.8404          
##                  95% CI : (0.7841, 0.8869)
##     No Information Rate : 0.6103          
##     P-Value [Acc &gt; NIR] : 2.164e-13       
##                                           
##                   Kappa : 0.6585          
##  Mcnemar&#39;s Test P-Value : 0.2299          
##                                           
##             Sensitivity : 0.7470          
##             Specificity : 0.9000          
##          Pos Pred Value : 0.8267          
##          Neg Pred Value : 0.8478          
##               Precision : 0.8267          
##                  Recall : 0.7470          
##                      F1 : 0.7848          
##              Prevalence : 0.3897          
##          Detection Rate : 0.2911          
##    Detection Prevalence : 0.3521          
##       Balanced Accuracy : 0.8235          
##                                           
##        &#39;Positive&#39; Class : MM              
## </code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  methods   stats     graphics  grDevices utils     datasets 
## [8] base     
## 
## other attached packages:
##  [1] earth_4.6.1        plotmo_3.3.5       TeachingDemos_2.10
##  [4] plotrix_3.7        doMC_1.3.5         iterators_1.0.9   
##  [7] foreach_1.4.4      caret_6.0-78       lattice_0.20-35   
## [10] forcats_0.3.0      stringr_1.3.0      dplyr_0.7.4       
## [13] purrr_0.2.4        readr_1.1.1        tidyr_0.8.0       
## [16] tibble_1.4.2       ggplot2_2.2.1      tidyverse_1.2.1   
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-131.1     lubridate_1.7.3    dimRed_0.1.0      
##  [4] httr_1.3.1         rprojroot_1.3-2    tools_3.4.3       
##  [7] backports_1.1.2    R6_2.2.2           rpart_4.1-13      
## [10] lazyeval_0.2.1     colorspace_1.3-2   nnet_7.3-12       
## [13] withr_2.1.1.9000   tidyselect_0.2.4   mnormt_1.5-5      
## [16] compiler_3.4.3     cli_1.0.0          rvest_0.3.2       
## [19] xml2_1.2.0         bookdown_0.7       scales_0.5.0.9000 
## [22] sfsmisc_1.1-2      DEoptimR_1.0-8     psych_1.7.8       
## [25] robustbase_0.92-8  digest_0.6.15      foreign_0.8-69    
## [28] rmarkdown_1.9      pkgconfig_2.0.1    htmltools_0.3.6   
## [31] rlang_0.2.0.9000   readxl_1.0.0       ddalpha_1.3.1.1   
## [34] bindr_0.1.1        jsonlite_1.5       ModelMetrics_1.1.0
## [37] magrittr_1.5       Matrix_1.2-12      Rcpp_0.12.16      
## [40] munsell_0.4.3      stringi_1.1.7      yaml_2.1.18       
## [43] MASS_7.3-49        plyr_1.8.4         recipes_0.1.2     
## [46] grid_3.4.3         crayon_1.3.4       haven_1.1.1       
## [49] splines_3.4.3      hms_0.4.2          knitr_1.20        
## [52] pillar_1.2.1       reshape2_1.4.3     codetools_0.2-15  
## [55] stats4_3.4.3       CVST_0.2-1         glue_1.2.0        
## [58] evaluate_0.10.1    blogdown_0.5       modelr_0.1.1      
## [61] cellranger_1.1.0   gtable_0.2.0       kernlab_0.9-25    
## [64] assertthat_0.2.0   DRR_0.0.3          xfun_0.1          
## [67] gower_0.1.2        prodlim_1.6.1      broom_0.4.3       
## [70] e1071_1.6-8        class_7.3-14       survival_2.41-3   
## [73] timeDate_3043.102  RcppRoll_0.2.2     bindrcpp_0.2      
## [76] lava_1.6           ipred_0.9-6</code></pre>
<pre class="r"><code>save.image(file = &quot;../../data/craet_7.Rdata&quot;)</code></pre>
</div>
<div id="links" class="section level2">
<h2><strong>Links</strong></h2>
<ul>
<li><a href="https://www.machinelearningplus.com/caret-package/#7howtodohyperparametertuningtooptimizethemodelforbetterperformance?">How to do hyperparameter tuning to optimize the model for better performance?</a></li>
</ul>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[6 Training and Tuning the model]]></title>
    <link href="/2018/03/6-training-and-tuning-the-model/"/>
    <id>/2018/03/6-training-and-tuning-the-model/</id>
    <published>2018-03-29T00:00:00+00:00</published>
    <updated>2018-03-29T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
</div>
<div id="training" class="section level1">
<h1>Training</h1>
<div id="how-to-train-the-model-and-interpret-the-results" class="section level2">
<h2>1. How to train the model and interpret the results?</h2>
<p>Once you have chosen an algorithm, building the model is fairly easy using the train() function</p>
<p><code>train()</code> does multiple other things like:</p>
<ol style="list-style-type: decimal">
<li><em>Cross validating the model</em></li>
<li><em>Tune the hyper parameters for optimal model performance</em></li>
<li><em>Choose the optimal model based on a given evaluation metric</em></li>
<li><em>Preprocess the predictors (what we did so far using preProcess())</em></li>
</ol>
</div>
<div id="how-to-compute-variable-importance" class="section level2">
<h2>2. How to compute variable importance?</h2>
<p>Which variables came out to be useful?</p>
</div>
</div>
<div id="tuning" class="section level1">
<h1>Tuning</h1>
<div id="preprocess-the-test-dataset-and-predict" class="section level2">
<h2>1. Preprocess the test dataset and predict</h2>
<p>The pre-processing in the following sequence:</p>
<p><strong>Missing Value imputation â€“&gt; One-Hot Encoding â€“&gt; Range Normalization</strong></p>
<p><strong>All the information required for pre-processing is stored in the respective preProcess model and dummyVar model.</strong></p>
<p>pass the testData through these models in the same sequence:</p>
<p><strong>preProcess_missingdata_model â€“&gt; dummies_model â€“&gt; preProcess_range_model</strong></p>
</div>
<div id="predict-on-testdata-and-confusion-matrix" class="section level2">
<h2>2. Predict on testData and Confusion Matrix</h2>
</div>
<div id="reference" class="section level2">
<h2><strong>Reference</strong></h2>
<p><a href="https://www.machinelearningplus.com/caret-package/">Traing and Tuning model</a></p>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[5 How to do feature selection using recursive feature elimination]]></title>
    <link href="/2018/03/5-how-to-do-feature-selection-using-recursive-feature-elimination/"/>
    <id>/2018/03/5-how-to-do-feature-selection-using-recursive-feature-elimination/</id>
    <published>2018-03-28T00:00:00+00:00</published>
    <updated>2018-03-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>You might need <em>a rigorous way to determine the important variables</em> first before feeding them to the ML algorithm. This is important.</p>
<p>A good choice of selecting the important features is the <em>recursive feature elimination (RFE)</em></p>
<p>RFE works in 3 broad steps:</p>
<p>Step 1: Build a ML model on a training dataset and estimate the feature importances on the test dataset.ï¼ˆåœ¨ç¡®å®šè‡ªç”±åº¦çš„æƒ…å†µä¸‹ï¼Œè¯„ä»·å˜é‡åœ¨æµ‹è¯•æ•°æ®é›†ä¸­çš„é‡è¦æ€§ï¼‰</p>
<p>Step 2: Keeping priority to the most important variables, iterate through by building models of given sizes. Ranking of the predictors is recalculated in each iteration.ï¼ˆæŠŠåˆšæ‰çš„è¿‡ç¨‹åœ¨ä¸åŒçš„è‡ªç”±åº¦ä¸‹è¿­ä»£æ‰§è¡Œï¼‰</p>
<p>Step 3: The model performances are compared across different subset sizes to arrive at the optimal number and list of final predictors.ï¼ˆæ¯”è¾ƒä¸åŒè‡ªç”±åº¦çš„æµ‹è¯•é”™è¯¯ç‡ï¼Œç»™å‡ºæœ€ä½³è‡ªç”±åº¦æ¨¡å‹é€‰æ‹©ï¼‰</p>
<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code># Load Package And Data
load(&quot;../../data/craet_4.Rdata&quot;)
library(tidyverse)
library(caret)
#Set Parallel Processing - Decrease computation time
if (!require(&quot;doMC&quot;)) install.packages(&quot;doMC&quot;)
library(doMC)
registerDoMC(cores = 4)</code></pre>
</div>
<div id="feature-select" class="section level1">
<h1>Feature select</h1>
<pre class="r"><code>set.seed(100)
options(warn=-1)

subsets &lt;- c(1:5, 10, 15, 18)

#Step 1: Build a ML model on a training dataset and estimate the feature importances on the test dataset.ï¼ˆåœ¨ç¡®å®šè‡ªç”±åº¦çš„æƒ…å†µä¸‹ï¼Œè¯„ä»·å˜é‡åœ¨æµ‹è¯•æ•°æ®é›†ä¸­çš„é‡è¦æ€§ï¼‰
ctrl &lt;- rfeControl(functions = rfFuncs,
                   method = &quot;repeatedcv&quot;,#repeated K-fold cross-validation
                   number = 10,#10-fold cross-validations
                   repeats = 5, #five separate 10-fold cross-validations are used
                   verbose = FALSE)
#Step 2: Keeping priority to the most important variables, iterate through by building models of given sizes. Ranking of the predictors is recalculated in each iteration.ï¼ˆæŠŠåˆšæ‰çš„è¿‡ç¨‹åœ¨ä¸åŒçš„è‡ªç”±åº¦ä¸‹è¿­ä»£æ‰§è¡Œ
lmProfile &lt;- rfe(x=trainData[, 1:18], y=trainData$Purchase,
                 sizes = subsets,
                 rfeControl = ctrl)

#Step 3: The model performances are compared across different subset sizes to arrive at the optimal number and list of final predictors.ï¼ˆæ¯”è¾ƒä¸åŒè‡ªç”±åº¦çš„æµ‹è¯•é”™è¯¯ç‡ï¼Œç»™å‡ºæœ€ä½³è‡ªç”±åº¦æ¨¡å‹é€‰æ‹©
lmProfile</code></pre>
<pre><code>## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (10 fold, repeated 5 times) 
## 
## Resampling performance over subset size:
## 
##  Variables Accuracy  Kappa AccuracySD KappaSD Selected
##          1   0.7442 0.4569    0.04125 0.08753         
##          2   0.8124 0.6031    0.04002 0.08505         
##          3   0.8182 0.6136    0.04170 0.08790        *
##          4   0.8047 0.5879    0.04314 0.08993         
##          5   0.8000 0.5770    0.04215 0.08861         
##         10   0.8035 0.5826    0.04112 0.08815         
##         15   0.8089 0.5918    0.04209 0.09076         
##         18   0.8084 0.5918    0.04118 0.08894         
## 
## The top 3 variables (out of 3):
##    LoyalCH, PriceDiff, StoreID</code></pre>
<div id="input" class="section level2">
<h2>input</h2>
<ul>
<li><p>Size: sizes determines what all model sizes (the number of most important features) the rfe should consider</p></li>
<li>rfeControl():
<ul>
<li>functions: what type of algorithm should be used <strong>rfFuncs:: random forest based</strong>
<ul>
<li>methods: repeated K-fold cross-validation</li>
<li>number: 10-fold cross-validations</li>
<li>repeats: five separate 10-fold cross-validations are used</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="output" class="section level2">
<h2>output</h2>
<p>The Output Shows: - accuracy<br />
- kappa (and their standard deviation) for the different model sizes we provided - The final selected model subset size is marked with a * in the rightmost Selected column.</p>
<pre class="r"><code>save.image(&quot;../../data/craet_5.Rdata&quot;)
sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  methods   stats     graphics  grDevices utils     datasets 
## [8] base     
## 
## other attached packages:
##  [1] doMC_1.3.5      iterators_1.0.9 foreach_1.4.4   caret_6.0-78   
##  [5] lattice_0.20-35 forcats_0.3.0   stringr_1.3.0   dplyr_0.7.4    
##  [9] purrr_0.2.4     readr_1.1.1     tidyr_0.8.0     tibble_1.4.2   
## [13] ggplot2_2.2.1   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##  [1] httr_1.3.1          ddalpha_1.3.1.1     sfsmisc_1.1-2      
##  [4] jsonlite_1.5        splines_3.4.3       prodlim_1.6.1      
##  [7] modelr_0.1.1        assertthat_0.2.0    stats4_3.4.3       
## [10] DRR_0.0.3           cellranger_1.1.0    yaml_2.1.18        
## [13] robustbase_0.92-8   ipred_0.9-6         pillar_1.2.1       
## [16] backports_1.1.2     glue_1.2.0          digest_0.6.15      
## [19] randomForest_4.6-12 rvest_0.3.2         colorspace_1.3-2   
## [22] recipes_0.1.2       htmltools_0.3.6     Matrix_1.2-12      
## [25] plyr_1.8.4          psych_1.7.8         timeDate_3043.102  
## [28] pkgconfig_2.0.1     CVST_0.2-1          broom_0.4.3        
## [31] haven_1.1.1         bookdown_0.7        scales_0.5.0.9000  
## [34] gower_0.1.2         lava_1.6            withr_2.1.1.9000   
## [37] nnet_7.3-12         lazyeval_0.2.1      cli_1.0.0          
## [40] mnormt_1.5-5        survival_2.41-3     magrittr_1.5       
## [43] crayon_1.3.4        readxl_1.0.0        evaluate_0.10.1    
## [46] nlme_3.1-131.1      MASS_7.3-49         xml2_1.2.0         
## [49] dimRed_0.1.0        foreign_0.8-69      class_7.3-14       
## [52] blogdown_0.5        tools_3.4.3         hms_0.4.2          
## [55] kernlab_0.9-25      munsell_0.4.3       bindrcpp_0.2       
## [58] e1071_1.6-8         compiler_3.4.3      RcppRoll_0.2.2     
## [61] rlang_0.2.0.9000    grid_3.4.3          rmarkdown_1.9      
## [64] gtable_0.2.0        ModelMetrics_1.1.0  codetools_0.2-15   
## [67] reshape2_1.4.3      R6_2.2.2            lubridate_1.7.3    
## [70] knitr_1.20          bindr_0.1.1         rprojroot_1.3-2    
## [73] stringi_1.1.7       Rcpp_0.12.16        rpart_4.1-13       
## [76] tidyselect_0.2.4    DEoptimR_1.0-8      xfun_0.1</code></pre>
</div>
<div id="reference" class="section level2">
<h2><strong>Reference</strong></h2>
<ul>
<li><a href="https://www.machinelearningplus.com/caret-package/">How to do feature selection using recursive feature elimination (rfe)?</a></li>
</ul>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[4 How To Visualize The Importance Of Variables Using featurePlot]]></title>
    <link href="/2018/03/4-how-to-visualize-the-importance-of-variables-using-featureplot/"/>
    <id>/2018/03/4-how-to-visualize-the-importance-of-variables-using-featureplot/</id>
    <published>2018-03-27T00:00:00+00:00</published>
    <updated>2018-03-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code>load(&quot;../../data/craet_3-3.Rdata&quot;)
library(tidyverse)
library(caret)</code></pre>
</div>
<div id="q-how-the-predictors-influence-the-y" class="section level1">
<h1>Q: How The Predictors Influence The Y</h1>
<p>é€‰æ‹©é‡è¦çš„å˜é‡: é€šè¿‡è§‚å¯Ÿåœ¨Yçš„åˆ†ç»„ä¸‹å„ä¸ªå˜é‡çš„åˆ†å¸ƒæƒ…å†µ</p>
<p>ä¸€èˆ¬æœ‰ ç®±çº¿å›¾ å’Œ å¯†åº¦å›¾</p>
</div>
<div id="box-plot" class="section level1">
<h1>box-plot</h1>
<pre class="r"><code>featurePlot(x = trainData[, 1:18], 
            y = trainData$Purchase, 
            plot = &quot;box&quot;,#&quot;density&quot;
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation=&quot;free&quot;), 
                          y = list(relation=&quot;free&quot;)))</code></pre>
<p><img src="/post/2018-03-27-4-how-to-visualize-the-importance-of-variables-using-featureplot_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="density" class="section level1">
<h1>Density</h1>
<pre class="r"><code>featurePlot(x = trainData[, 1:18], 
            y = trainData$Purchase, 
            plot = &quot;density&quot;,
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation=&quot;free&quot;), 
                          y = list(relation=&quot;free&quot;)))</code></pre>
<p><img src="/post/2018-03-27-4-how-to-visualize-the-importance-of-variables-using-featureplot_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>save.image(&quot;../../data/craet_4.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Scraping and Wranging Tables from Research Articles]]></title>
    <link href="/2018/03/scraping-and-wranging-tables-from-research-articles/"/>
    <id>/2018/03/scraping-and-wranging-tables-from-research-articles/</id>
    <published>2018-03-27T00:00:00+00:00</published>
    <updated>2018-03-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>What do you do when you want to use results from the literature to anchor your own analysis? weâ€™ll go through a practical scenario on scraping an html table from a Nature Genetics article into R and wrangling the data into a useful format.</p>
<div id="scraping-a-html-table-from-a-webpage" class="section level1">
<h1>01. Scraping a html table from a webpage</h1>
<pre class="r"><code>#load packages
library(&quot;rvest&quot;)
library(&quot;knitr&quot;)
library(tidyverse)
#scraping web page
url &lt;- &quot;https://www.nature.com/articles/ng.2802/tables/2&quot;

#====ğŸ”¥find where is the table lives on this webpage====
table_path=&#39;//*[@id=&quot;content&quot;]/div/div/figure/div[1]/div/div[1]/table&#39;
#get the table
nature_genetics_table2 &lt;- url %&gt;%
  read_html() %&gt;%
  html_nodes(xpath=table_path) %&gt;%
  html_table(fill=T) %&gt;% .[[1]]
#the first few lines of table
kable(nature_genetics_table2[1:4,])</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">SNPa</th>
<th align="left">Chr.</th>
<th align="left">Positionb</th>
<th align="left">Closest genec</th>
<th align="left">Major/minor alleles</th>
<th align="left">MAFd</th>
<th align="left">Stage 1</th>
<th align="left">Stage 1</th>
<th align="left">Stage 2</th>
<th align="left">Stage 2</th>
<th align="left">Overall</th>
<th align="left">Overall</th>
<th align="left">Overall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">SNPa</td>
<td align="left">Chr.</td>
<td align="left">Positionb</td>
<td align="left">Closest genec</td>
<td align="left">Major/minor alleles</td>
<td align="left">MAFd</td>
<td align="left">OR (95% CI)e</td>
<td align="left">Meta P value</td>
<td align="left">OR (95% CI)e</td>
<td align="left">Meta P value</td>
<td align="left">OR (95% CI)e</td>
<td align="left">Meta P value</td>
<td align="left">I2 (%), P valuef</td>
</tr>
<tr class="even">
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="odd">
<td align="left">rs6656401</td>
<td align="left">1</td>
<td align="left">207692049</td>
<td align="left">CR1</td>
<td align="left">G/A</td>
<td align="left">0.197</td>
<td align="left">1.17 (1.12â€“1.22)</td>
<td align="left">7.7 Ã— 10âˆ’15</td>
<td align="left">1.21 (1.14â€“1.28)</td>
<td align="left">7.9 Ã— 10âˆ’11</td>
<td align="left">1.18 (1.14â€“1.22)</td>
<td align="left">5.7 Ã— 10âˆ’24</td>
<td align="left">0, 7.8 Ã— 10âˆ’1</td>
</tr>
<tr class="even">
<td align="left">rs6733839</td>
<td align="left">2</td>
<td align="left">127892810</td>
<td align="left">BIN1</td>
<td align="left">C/T</td>
<td align="left">0.409</td>
<td align="left">1.21 (1.17â€“1.25)</td>
<td align="left">1.7 Ã— 10âˆ’26</td>
<td align="left">1.24 (1.18â€“1.29)</td>
<td align="left">3.4 Ã— 10âˆ’19</td>
<td align="left">1.22 (1.18â€“1.25)</td>
<td align="left">6.9 Ã— 10âˆ’44</td>
<td align="left">28, 6.1 Ã— 10âˆ’2</td>
</tr>
</tbody>
</table>
</div>
<div id="making-messy-data-useful" class="section level1">
<h1>02 Making messy data useful</h1>
<div id="cleaning-up-the-rows" class="section level2">
<h2>Cleaning up the rows</h2>
<p>All The Elements Of These Rows Contain The Exact Same Text</p>
<pre class="r"><code>v=which(apply(nature_genetics_table2,1, function(x) length(unique(unlist(x))) )==1)
v</code></pre>
<pre><code>## [1]  2 12 18</code></pre>
</div>
<div id="split-table" class="section level2">
<h2>split table</h2>
<pre class="r"><code>nature_genetics_table2_list = split(nature_genetics_table2, cumsum(1:nrow(nature_genetics_table2) %in% v))
nature_genetics_table2_list = lapply(nature_genetics_table2_list[2:4], function(y) {
y$Description = unique(as.character(y[1, ]))
y[-1, ]
})

#rbind three table
nature_genetics_table2_clean = do.call(&quot;rbind&quot;, nature_genetics_table2_list)

kable(nature_genetics_table2_clean[1:3,])</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">SNPa</th>
<th align="left">Chr.</th>
<th align="left">Positionb</th>
<th align="left">Closest genec</th>
<th align="left">Major/minor alleles</th>
<th align="left">MAFd</th>
<th align="left">Stage 1</th>
<th align="left">Stage 1</th>
<th align="left">Stage 2</th>
<th align="left">Stage 2</th>
<th align="left">Overall</th>
<th align="left">Overall</th>
<th align="left">Overall</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1.3</td>
<td align="left">rs6656401</td>
<td align="left">1</td>
<td align="left">207692049</td>
<td align="left">CR1</td>
<td align="left">G/A</td>
<td align="left">0.197</td>
<td align="left">1.17 (1.12â€“1.22)</td>
<td align="left">7.7 Ã— 10âˆ’15</td>
<td align="left">1.21 (1.14â€“1.28)</td>
<td align="left">7.9 Ã— 10âˆ’11</td>
<td align="left">1.18 (1.14â€“1.22)</td>
<td align="left">5.7 Ã— 10âˆ’24</td>
<td align="left">0, 7.8 Ã— 10âˆ’1</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="even">
<td>1.4</td>
<td align="left">rs6733839</td>
<td align="left">2</td>
<td align="left">127892810</td>
<td align="left">BIN1</td>
<td align="left">C/T</td>
<td align="left">0.409</td>
<td align="left">1.21 (1.17â€“1.25)</td>
<td align="left">1.7 Ã— 10âˆ’26</td>
<td align="left">1.24 (1.18â€“1.29)</td>
<td align="left">3.4 Ã— 10âˆ’19</td>
<td align="left">1.22 (1.18â€“1.25)</td>
<td align="left">6.9 Ã— 10âˆ’44</td>
<td align="left">28, 6.1 Ã— 10âˆ’2</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="odd">
<td>1.5</td>
<td align="left">rs10948363</td>
<td align="left">6</td>
<td align="left">47487762</td>
<td align="left">CD2AP</td>
<td align="left">A/G</td>
<td align="left">0.266</td>
<td align="left">1.10 (1.07â€“1.14)</td>
<td align="left">3.1 Ã— 10âˆ’8</td>
<td align="left">1.09 (1.04â€“1.15)</td>
<td align="left">4.1 Ã— 10âˆ’4</td>
<td align="left">1.10 (1.07â€“1.13)</td>
<td align="left">5.2 Ã— 10âˆ’11</td>
<td align="left">0, 9 Ã— 10âˆ’1</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="fixing-column-names" class="section level1">
<h1>03. Fixing column names</h1>
<pre class="r"><code>colnames(nature_genetics_table2_clean) &lt;- c(&quot;SNP&quot;, &quot;Chr&quot;, &quot;Position&quot;, &quot;Closest gene&quot;, &quot;Major/minor alleles&quot;, &quot;MAF&quot;, &quot;Stage1_OR&quot;, &quot;Stage1_MetaP&quot;, &quot;Stage2_OR&quot;,&quot;Stage2_MetaP&quot;,    &quot;Overall_OR&quot;, &quot;Overall_MetaP&quot;, &quot;I2_Percent/P&quot;,&quot;Description&quot;)
colnames(nature_genetics_table2_clean)</code></pre>
<pre><code>##  [1] &quot;SNP&quot;                 &quot;Chr&quot;                 &quot;Position&quot;           
##  [4] &quot;Closest gene&quot;        &quot;Major/minor alleles&quot; &quot;MAF&quot;                
##  [7] &quot;Stage1_OR&quot;           &quot;Stage1_MetaP&quot;        &quot;Stage2_OR&quot;          
## [10] &quot;Stage2_MetaP&quot;        &quot;Overall_OR&quot;          &quot;Overall_MetaP&quot;      
## [13] &quot;I2_Percent/P&quot;        &quot;Description&quot;</code></pre>
</div>
<div id="making-a-character-variable-into-a-numeric-variable" class="section level1">
<h1>04. Making a character variable into a numeric variable</h1>
<pre class="r"><code># &quot; Ã— 10-&quot; -&gt; &quot;e-&quot;
nature_genetics_table2_clean$Stage1_MetaP &lt;- 
str_replace(nature_genetics_table2_clean$Stage1_MetaP,&quot; Ã— 10âˆ’&quot;,&quot;e-&quot;) %&gt;% as.numeric()
kable(nature_genetics_table2_clean[1:3,])</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">SNP</th>
<th align="left">Chr</th>
<th align="left">Position</th>
<th align="left">Closest gene</th>
<th align="left">Major/minor alleles</th>
<th align="left">MAF</th>
<th align="left">Stage1_OR</th>
<th align="right">Stage1_MetaP</th>
<th align="left">Stage2_OR</th>
<th align="left">Stage2_MetaP</th>
<th align="left">Overall_OR</th>
<th align="left">Overall_MetaP</th>
<th align="left">I2_Percent/P</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1.3</td>
<td align="left">rs6656401</td>
<td align="left">1</td>
<td align="left">207692049</td>
<td align="left">CR1</td>
<td align="left">G/A</td>
<td align="left">0.197</td>
<td align="left">1.17 (1.12â€“1.22)</td>
<td align="right">0</td>
<td align="left">1.21 (1.14â€“1.28)</td>
<td align="left">7.9 Ã— 10âˆ’11</td>
<td align="left">1.18 (1.14â€“1.22)</td>
<td align="left">5.7 Ã— 10âˆ’24</td>
<td align="left">0, 7.8 Ã— 10âˆ’1</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="even">
<td>1.4</td>
<td align="left">rs6733839</td>
<td align="left">2</td>
<td align="left">127892810</td>
<td align="left">BIN1</td>
<td align="left">C/T</td>
<td align="left">0.409</td>
<td align="left">1.21 (1.17â€“1.25)</td>
<td align="right">0</td>
<td align="left">1.24 (1.18â€“1.29)</td>
<td align="left">3.4 Ã— 10âˆ’19</td>
<td align="left">1.22 (1.18â€“1.25)</td>
<td align="left">6.9 Ã— 10âˆ’44</td>
<td align="left">28, 6.1 Ã— 10âˆ’2</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
<tr class="odd">
<td>1.5</td>
<td align="left">rs10948363</td>
<td align="left">6</td>
<td align="left">47487762</td>
<td align="left">CD2AP</td>
<td align="left">A/G</td>
<td align="left">0.266</td>
<td align="left">1.10 (1.07â€“1.14)</td>
<td align="right">0</td>
<td align="left">1.09 (1.04â€“1.15)</td>
<td align="left">4.1 Ã— 10âˆ’4</td>
<td align="left">1.10 (1.07â€“1.13)</td>
<td align="left">5.2 Ã— 10âˆ’11</td>
<td align="left">0, 9 Ã— 10âˆ’1</td>
<td align="left">Known GWAS-defined associated genes</td>
</tr>
</tbody>
</table>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
##  [1] forcats_0.3.0   stringr_1.3.0   dplyr_0.7.4     purrr_0.2.4    
##  [5] readr_1.1.1     tidyr_0.8.0     tibble_1.4.2    ggplot2_2.2.1  
##  [9] tidyverse_1.2.1 knitr_1.20      rvest_0.3.2     xml2_1.2.0     
## 
## loaded via a namespace (and not attached):
##  [1] xfun_0.1          reshape2_1.4.3    haven_1.1.1      
##  [4] lattice_0.20-35   colorspace_1.3-2  htmltools_0.3.6  
##  [7] yaml_2.1.18       rlang_0.2.0.9000  pillar_1.2.1     
## [10] foreign_0.8-69    glue_1.2.0        selectr_0.3-2    
## [13] modelr_0.1.1      readxl_1.0.0      bindrcpp_0.2     
## [16] bindr_0.1.1       plyr_1.8.4        munsell_0.4.3    
## [19] blogdown_0.5      gtable_0.2.0      cellranger_1.1.0 
## [22] psych_1.7.8       evaluate_0.10.1   parallel_3.4.3   
## [25] curl_3.1          highr_0.6         broom_0.4.3      
## [28] Rcpp_0.12.16      backports_1.1.2   scales_0.5.0.9000
## [31] jsonlite_1.5      mnormt_1.5-5      hms_0.4.2        
## [34] digest_0.6.15     stringi_1.1.7     bookdown_0.7     
## [37] grid_3.4.3        rprojroot_1.3-2   cli_1.0.0        
## [40] tools_3.4.3       magrittr_1.5      lazyeval_0.2.1   
## [43] crayon_1.3.4      pkgconfig_2.0.1   lubridate_1.7.3  
## [46] assertthat_0.2.0  rmarkdown_1.9     httr_1.3.1       
## [49] R6_2.2.2          nlme_3.1-131.1    compiler_3.4.3</code></pre>
</div>
<div id="reference" class="section level1">
<h1><strong>Reference</strong></h1>
<ul>
<li><a href="http://research.libd.org/rstatsclub/2018/03/19/introduction-to-scraping-and-wranging-tables-from-research-articles/">Introduction to Scraping and Wranging Tables from Research Articles</a></li>
</ul>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[3.3 How To Create Dummy Variables And Normalization]]></title>
    <link href="/2018/03/how-to-create-dummy-variables-and-normalization/"/>
    <id>/2018/03/how-to-create-dummy-variables-and-normalization/</id>
    <published>2018-03-26T00:00:00+00:00</published>
    <updated>2018-03-26T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-package-and-data" class="section level1">
<h1>Load Package And Data</h1>
<pre class="r"><code>load(&quot;../../data/craet_3-2.Rdata&quot;)
library(tidyverse)
library(caret)</code></pre>
</div>
<div id="why-dummy-variables" class="section level1">
<h1>Why Dummy Variables</h1>
<p>å¯¹äºå­—ç¬¦å‹çš„å› å­å˜é‡ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒè½¬å˜ä¸ºæœ‰åºçš„æ•°å€¼ï¼Œä¸€èˆ¬è½¬ä¸º 0ï¼Œ1 çš„äºŒå˜é‡ï¼Œ è¿™æ ·0 å°±ä»£è¡¨åŸºç¡€æ°´å¹³ï¼Œ 1ä»£è¡¨æ¯”è¾ƒç»„</p>
<p><img src="https://i.loli.net/2018/03/26/5ab847e351487.jpg" width="30%" style="display: block; margin: auto;" /></p>
</div>
<div id="how" class="section level1">
<h1>How</h1>
<pre class="r"><code># One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model &lt;- dummyVars(Purchase ~ ., data=trainData)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat &lt;- predict(dummies_model, newdata = trainData)

# # Convert to dataframe
trainData &lt;- data.frame(trainData_mat)

# # See the structure of the new dataset
str(trainData)</code></pre>
<pre><code>## &#39;data.frame&#39;:    857 obs. of  18 variables:
##  $ WeekofPurchase: num  -1.1 -1.74 -1.68 -1.29 -1.04 ...
##  $ StoreID       : num  -1.29 -1.29 1.33 1.33 1.33 ...
##  $ PriceCH       : num  -1.14 -1.73 -1.73 -1.14 -1.14 ...
##  $ PriceMM       : num  -0.688 -2.898 -2.898 -0.688 -0.688 ...
##  $ DiscCH        : num  -0.452 -0.452 -0.452 -0.452 -0.452 ...
##  $ DiscMM        : num  -0.582 -0.582 -0.582 1.341 1.341 ...
##  $ SpecialCH     : num  -0.429 -0.429 -0.429 2.329 -0.429 ...
##  $ SpecialMM     : num  -0.42 -0.42 -0.42 -0.42 -0.42 ...
##  $ LoyalCH       : num  -0.205 -0.525 1.256 1.324 1.35 ...
##  $ SalePriceMM   : num  0.113 -1.101 -1.101 -1.506 -1.506 ...
##  $ SalePriceCH   : num  -0.431 -0.844 -0.844 -0.431 -0.431 ...
##  $ PriceDiff     : num  0.341 -0.563 -0.563 -1.165 -1.165 ...
##  $ Store7.No     : num  1 1 0 0 0 0 0 0 0 1 ...
##  $ Store7.Yes    : num  0 0 1 1 1 1 1 1 1 0 ...
##  $ PctDiscMM     : num  -0.588 -0.588 -0.588 1.447 1.447 ...
##  $ PctDiscCH     : num  -0.448 -0.448 -0.448 -0.448 -0.448 ...
##  $ ListPriceDiff : num  0.211 -1.988 -1.988 0.211 0.211 ...
##  $ STORE         : num  -0.457 -0.457 -1.15 -1.15 -1.15 ...</code></pre>
</div>
<div id="why-normalization" class="section level1">
<h1>Why Normalization</h1>
<p>ä¸ºäº†æ¶ˆé™¤ä¸åŒå˜é‡ç”±äºå•ä½é€ æˆçš„æƒé‡å½±å“ï¼Œæˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–</p>
</div>
<div id="how-1" class="section level1">
<h1>How</h1>
<ol style="list-style-type: decimal">
<li><strong>range:</strong>Â Normalize values so it ranges between 0 and 1</li>
<li><strong>center:</strong>Â Subtract Mean</li>
<li><strong>scale:</strong>Â Divide by standard deviation</li>
<li><strong>BoxCox:</strong>Â Remove skewness leading to normality. Values must be &gt; 0</li>
<li><strong>YeoJohnson:</strong>Â Like BoxCox, but works for negative values.</li>
<li><strong>expoTrans:</strong>Â Exponential transformation, works for negative values.</li>
<li><strong>pca:</strong>Â Replace with principal components</li>
<li><strong>ica:</strong>Â Replace with independent components</li>
<li><strong>spatialSign:</strong>Â Project the data to a unit circle</li>
</ol>
<pre class="r"><code>preProcess_range_model &lt;- preProcess(trainData, method=&#39;range&#39;)
trainData &lt;- predict(preProcess_range_model, newdata = trainData)

# Append the Y variable
trainData$Purchase &lt;- y

apply(trainData[, 1:10], 2, FUN=function(x){c(&#39;min&#39;=min(x), &#39;max&#39;=max(x))})</code></pre>
<pre><code>##     WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH
## min              0       0       0       0      0      0         0
## max              1       1       1       1      1      1         1
##     SpecialMM LoyalCH SalePriceMM
## min         0       0           0
## max         1       1           1</code></pre>
<pre class="r"><code>save.image(file = &quot;../../data/craet_3-3.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Why should I trust you ğŸ¤–?]]></title>
    <link href="/2018/03/why-should-i-trust-you/"/>
    <id>/2018/03/why-should-i-trust-you/</id>
    <published>2018-03-26T00:00:00+00:00</published>
    <updated>2018-03-26T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ä¸»è¦é›†ä¸­åœ¨æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–ä¸Š; æœ€å¥½çš„æ¨¡å‹é€šå¸¸æ˜¯é€šè¿‡åƒç²¾åº¦æˆ–è€…é”™è¯¯è¿™æ ·çš„æ€§èƒ½åº¦é‡æ¥é€‰æ‹©çš„ï¼Œè€Œä¸”å¦‚æœå®ƒé€šè¿‡äº†è¿™äº›æ€§èƒ½æ ‡å‡†çš„æŸäº›é˜ˆå€¼ï¼Œæˆ‘ä»¬å€¾å‘äºå‡è®¾ä¸€ä¸ªæ¨¡å‹æ˜¯è¶³å¤Ÿå¥½çš„ã€‚åœ¨æœºå™¨å­¦ä¹ çš„è®¸å¤šåº”ç”¨ä¸­ï¼Œç”¨æˆ·ä¼šä½¿ç”¨ä¸€ä¸ªæ¨¡å‹æ¥å¸®åŠ©åšå†³å®šï¼Œ ä¾‹å¦‚ï¼šä¸€ä½åŒ»ç”Ÿä¸ä¼šå¯¹ç—…äººè¿›è¡Œæ‰‹æœ¯ï¼Œä»…ä»…å› ä¸ºè¿™ä¸ªæ¨¡å‹è¯´ä¸åº”è¯¥è¿›è¡Œæ‰‹æœ¯ï¼Ÿ</p>
<p>ç”±äºå¤æ‚çš„æœºå™¨å­¦ä¹ æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯é»‘ç›’å­ï¼Œè€Œä¸”å¤ªå¤æ‚ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹åšå‡ºçš„åˆ†ç±»å†³å®šé€šå¸¸å¾ˆéš¾è¢«äººç±»çš„å¤§è„‘æ‰€ç†è§£ï¼Œä½†æ˜¯èƒ½å¤Ÿç†è§£å’Œè§£é‡Šè¿™äº›æ¨¡å‹å¯¹äºæé«˜æ¨¡å‹è´¨é‡ï¼Œæé«˜ä¿¡ä»»åº¦å’Œé€æ˜åº¦ä»¥åŠå‡å°‘åè§éå¸¸é‡è¦ï¼Œå› ä¸ºäººç±»å¾€å¾€å…·æœ‰è‰¯å¥½çš„ç›´è§‰å’Œå› æœæ¨ç†ï¼Œè¿™äº›éƒ½æ˜¯éš¾ä»¥åœ¨æ•°æ®è¯„ä¼°æŒ‡æ ‡ä¸­æ•è·ã€‚</p>
<p>å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå½¢è±¡çš„ç†è§£å®ƒä»¬çš„å·¥ä½œåŸç†ï¼šä¸ºä»€ä¹ˆä¸€ä¸ªæ¨¡å‹å°†å…·æœ‰ç‰¹å®šæ ‡ç­¾çš„æ¡ˆä¾‹è¿›è¡Œå‡†ç¡®åˆ†ç±»ã€‚eg: ä¸ºä»€ä¹ˆä¸€ä¸ªä¹³è…ºè‚¿å—æ ·æœ¬è¢«å½’ç±»ä¸ºâ€œæ¶æ€§â€è€Œä¸æ˜¯â€œè‰¯æ€§ï¼Œä»…ä»…å› ä¸ºå®ƒé•¿å¾—ä¸‘å—ï¼Ÿ</p>
<blockquote>
<p><a href="https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime">Local Interpretable Model-Agnostic Explanations (LIME)</a> is an attempt to make these complex models at least partly understandable. The method has been published in <a href="https://arxiv.org/pdf/1602.04938.pdf">â€œWhy Should I Trust You?â€</a> Explaining the Predictions of Any Classifier. By Marco Tulio Ribeiro, Sameer Singh and Carlos Guestrin from the University of Washington in Seattle</p>
</blockquote>
<div id="how-lime-works" class="section level2">
<h2><strong>How LIME works</strong></h2>
<blockquote>
<p>lime is able to explain all models for which we can obtain prediction probabilities (in R, that is every model that works with predict(type = â€œprobâ€)). It makes use of the fact that linear models are easy to explain because they are based on linear relationships between features and class labels: The complex model function is approximated by locally fitting linear models to permutations of the original training set.On each permutation, a linear model is being fit and weights are given so that incorrect classification of instances that are more similar to the original data are penalized (positive weights support a decision, negative weights contradict them). This will give an approximation of how much (and in which way) each feature contributed to a decision made by the model</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7fa56fd7333.jpg" width="30%" /></p>
<blockquote>
<p>We take the image on the left and divide it into interpretable components</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7faba0d302a.jpg" width="30%" /></p>
<blockquote>
<p>we then generate a data set of perturbed instances by turning some of the interpretable components â€œoï¬€â€. For each perturbed instance, we get the probability that a tree frog is in the image according to the model.</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7fac3ca8787.jpg" width="30%" /></p>
<blockquote>
<p>We then learn a simple (linear) model on this data set, which is locally weightedâ€”that is, we care more about making mistakes in perturbed instances that are more similar to the original image</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7fac8382a86.jpg" width="30%" /></p>
<blockquote>
<p>the end, we present the superpixels with highest positive weights as an explanation, graying out everything else</p>
</blockquote>
<p><img src="https://i.loli.net/2018/02/11/5a7faf16f0320.jpg" width="30%" /></p>
<p>ğŸ¤–é¢„æµ‹è¿™å¼ å›¾æ˜¯ä¸ªæ ‘è›™æ˜¯å› ä¸ºè¿™ä¸ªéƒ¨åˆ†,æ‰€ä»¥ğŸ¤–é¢„æµ‹ç»“æœæ˜¯æ¯”è¾ƒå¯ä¿¡çš„ã€‚ <img src="https://i.loli.net/2018/02/11/5a7faebabe34e.jpg" style="display: block; margin: auto;" /></p>
<p>ğŸ¤–é¢„æµ‹è¿™å¼ å›¾æ˜¯å°çƒæ˜¯æ ¹æ®è¿™äº›éƒ¨åˆ†ï¼Œæ‰€ä»¥ğŸ¤–é¢„æµ‹ç»“æœæ˜¯ä¸å¯ä¿¡çš„ã€‚ <img src="https://i.loli.net/2018/02/11/5a7fb61ecb060.jpg" style="display: block; margin: auto;" /></p>
</div>
<div id="example-in-r" class="section level2">
<h2><strong>Example in R</strong></h2>
<div id="prepare-the-breast-cancer-data" class="section level3">
<h3>01.Prepare the breast cancer data</h3>
<p>This <strong>data</strong> of example comes from the book of <strong>R in action</strong></p>
<pre class="r"><code>loc &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/&quot;
ds  &lt;- &quot;breast-cancer-wisconsin/breast-cancer-wisconsin.data&quot;
url &lt;- paste(loc, ds, sep=&quot;&quot;)

breast &lt;- read.table(url, sep=&quot;,&quot;, header=FALSE, na.strings=&quot;?&quot;)
names(breast) &lt;- c(&quot;ID&quot;, &quot;clumpThickness&quot;, &quot;sizeUniformity&quot;,
                   &quot;shapeUniformity&quot;, &quot;maginalAdhesion&quot;, 
                   &quot;singleEpithelialCellSize&quot;, &quot;bareNuclei&quot;, 
                   &quot;blandChromatin&quot;, &quot;normalNucleoli&quot;, &quot;mitosis&quot;, &quot;class&quot;)

df &lt;- breast[-1]
df$class &lt;- factor(df$class, levels=c(2,4), 
                   labels=c(&quot;benign&quot;, &quot;malignant&quot;))

set.seed(1234)
train &lt;- sample(nrow(df), 0.7*nrow(df))
df.train &lt;- df[train,]
df.validate &lt;- df[-train,]
table(df.train$class)</code></pre>
<pre><code>## 
##    benign malignant 
##       329       160</code></pre>
<pre class="r"><code>table(df.validate$class)</code></pre>
<pre><code>## 
##    benign malignant 
##       129        81</code></pre>
</div>
<div id="create-decision-tree-model" class="section level3">
<h3>02 Create decision tree model</h3>
<pre class="r"><code>library(rpart)
set.seed(1234)
dtree &lt;- rpart(class ~ ., data=df.train, method=&quot;class&quot;,      
               parms=list(split=&quot;information&quot;))
dtree$cptable</code></pre>
<pre><code>##         CP nsplit rel error  xerror       xstd
## 1 0.800000      0   1.00000 1.00000 0.06484605
## 2 0.046875      1   0.20000 0.30625 0.04150018
## 3 0.012500      3   0.10625 0.20625 0.03467089
## 4 0.010000      4   0.09375 0.18125 0.03264401</code></pre>
<pre class="r"><code>plotcp(dtree)</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>dtree.pruned &lt;- prune(dtree, cp=.0125)</code></pre>
</div>
<div id="predict" class="section level3">
<h3>03 predict</h3>
<pre class="r"><code>dtree.pred &lt;- predict(dtree.pruned, df.validate, type=&quot;class&quot;)
(dtree.perf &lt;- table(df.validate$class, dtree.pred, 
                    dnn=c(&quot;Actual&quot;, &quot;Predicted&quot;)))</code></pre>
<pre><code>##            Predicted
## Actual      benign malignant
##   benign       122         7
##   malignant      2        79</code></pre>
</div>
<div id="plot-decision-tree" class="section level3">
<h3>04 plot decision tree</h3>
<pre class="r"><code>#plot01
library(rpart.plot)
prp(dtree.pruned, type = 2, extra = 104,  
    fallen.leaves = TRUE, main=&quot;Decision Tree&quot;)
#plot02
library(partykit)</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>library(dplyr)
dtree.pruned %&gt;% as.party() %&gt;% plot()</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
</div>
<div id="lime-why-should-i-trust-you" class="section level3">
<h3>LIME: why should I trust you ğŸ¤–?</h3>
<pre class="r"><code>library(lime)
explainer &lt;- lime(df.train, model = dtree.pruned)

# Explain new observation
#[model_type](https://github.com/thomasp85/lime/blob/master/R/models.R)
model_type.rpart &lt;- function(x, ...) &#39;classification&#39;#defined model_type method
test.data &lt;- 
  df.validate %&gt;% 
  dplyr::select(-class) %&gt;% 
  head(3)
explanation &lt;- lime::explain(test.data, explainer, n_labels = 1, n_features = 2)

# The output is provided in a consistent tabular format and includes the
# output from the model.
head(explanation)</code></pre>
<pre><code>##       model_type case     label label_prob  model_r2 model_intercept
## 1 classification    3    benign  0.9933993 0.1809444      0.05042991
## 2 classification    3    benign  0.9933993 0.1809444      0.05042991
## 3 classification    4 malignant  0.9507042 0.1918642      0.54743276
## 4 classification    4 malignant  0.9507042 0.1918642      0.54743276
## 5 classification    5    benign  0.9933993 0.1924691      0.05378321
## 6 classification    5    benign  0.9933993 0.1924691      0.05378321
##   model_prediction                  feature feature_value feature_weight
## 1        0.4637659           blandChromatin             3   -0.001977357
## 2        0.4637659           sizeUniformity             1    0.415313316
## 3        0.9524157 singleEpithelialCellSize             3    0.002885248
## 4        0.9524157           sizeUniformity             8    0.402097681
## 5        0.4712710          maginalAdhesion             3   -0.004533032
## 6        0.4712710           sizeUniformity             1    0.422020822
##                        feature_desc                      data
## 1           2 &lt; blandChromatin &lt;= 3 3, 1, 1, 1, 2, 2, 3, 1, 1
## 2               sizeUniformity &lt;= 4 3, 1, 1, 1, 2, 2, 3, 1, 1
## 3 2 &lt; singleEpithelialCellSize &lt;= 4 6, 8, 8, 1, 3, 4, 3, 7, 1
## 4                4 &lt; sizeUniformity 6, 8, 8, 1, 3, 4, 3, 7, 1
## 5              maginalAdhesion &lt;= 3 4, 1, 1, 3, 2, 1, 3, 1, 1
## 6               sizeUniformity &lt;= 4 4, 1, 1, 3, 2, 1, 3, 1, 1
##               prediction
## 1 0.99339934, 0.00660066
## 2 0.99339934, 0.00660066
## 3 0.04929577, 0.95070423
## 4 0.04929577, 0.95070423
## 5 0.99339934, 0.00660066
## 6 0.99339934, 0.00660066</code></pre>
<pre class="r"><code># And can be visualised directly
plot_features(explanation)</code></pre>
<p><img src="/post/2018-02-11-why-should-i-trust-you_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
</div>
<div id="links" class="section level2">
<h2><strong>Links</strong></h2>
<ul>
<li><a href="http://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease">CKD data set</a></li>
<li><a href="https://github.com/marcotcr/lime">open-source Python code for LIME</a></li>
<li><a href="https://github.com/thomasp85/lime">R package for LIME</a></li>
</ul>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[3.2 statistic description and impute missing value]]></title>
    <link href="/2018/03/3-2-statistic-description-and-impute-missing-value/"/>
    <id>/2018/03/3-2-statistic-description-and-impute-missing-value/</id>
    <published>2018-03-24T00:00:00+00:00</published>
    <updated>2018-03-24T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="section level1">
<h1>åŠ è½½æ•°æ®å’ŒåŒ…</h1>
<pre class="r"><code>load(&quot;../../data/caret.Rdata&quot;)
library(tidyverse)
library(caret)</code></pre>
<p>åœ¨è¿›è¡Œæ•°æ®æ•´ç†ä¹‹å‰ æˆ‘ä»¬å…ˆçœ‹çœ‹è®­ç»ƒæ•°æ®çš„ç»Ÿè®¡æè¿°</p>
<p><code>skimr</code>åŒ…å¯¹åˆ—çš„ç»Ÿè®¡æä¾›äº†æ–¹ä¾¿çš„å‡½æ•°</p>
<p><code>skimr::skim_to_wide()</code> è¾“å‡ºä¸€ä¸ªåŒ…å«åˆ—ç»Ÿè®¡æè¿°çš„æ•°æ®æ¡†</p>
<pre class="r"><code>library(skimr)
skimmed &lt;- skim_to_wide(trainData)
skimmed[, c(1:5, 9:11, 13, 15:16)]</code></pre>
<pre><code>## # A tibble: 18 x 11
##    type   variable  missing complete n     mean   sd    p0    median p100 
##    &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;
##  1 factor Purchase  0       857      857   &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt; 
##  2 factor Store7    0       857      857   &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt; 
##  3 integâ€¦ SpecialCH 2       855      857   &quot;  0.â€¦ &quot; 0.â€¦ 0     0      1    
##  4 integâ€¦ SpecialMM 4       853      857   &quot;  0.â€¦ &quot; 0.â€¦ 0     0      1    
##  5 integâ€¦ STORE     2       855      857   &quot;  1.â€¦ &quot; 1.â€¦ 0     2      4    
##  6 integâ€¦ StoreID   1       856      857   &quot;  3.â€¦ &quot; 2.â€¦ 1     3      7    
##  7 integâ€¦ WeekofPuâ€¦ 0       857      857   254.17 15.59 227   257    278  
##  8 numerâ€¦ DiscCH    2       855      857   0.054  &quot;0.1â€¦ &quot; 0 â€¦ &quot;0   &quot; &quot;0.5â€¦
##  9 numerâ€¦ DiscMM    3       854      857   &quot;0.12â€¦ &quot;0.2â€¦ &quot; 0 â€¦ &quot;0   &quot; &quot;0.8â€¦
## 10 numerâ€¦ ListPricâ€¦ 0       857      857   &quot;0.22â€¦ &quot;0.1â€¦ &quot; 0 â€¦ 0.24   0.44 
## 11 numerâ€¦ LoyalCH   5       852      857   &quot;0.56â€¦ &quot;0.3â€¦ &quot; 1.â€¦ &quot;0.6 &quot; &quot;1  â€¦
## 12 numerâ€¦ PctDiscCH 2       855      857   0.028  0.063 &quot; 0 â€¦ &quot;0   &quot; 0.25 
## 13 numerâ€¦ PctDiscMM 2       855      857   0.058  0.099 &quot; 0 â€¦ &quot;0   &quot; &quot;0.4â€¦
## 14 numerâ€¦ PriceCH   1       856      857   &quot;1.87â€¦ &quot;0.1â€¦ &quot; 1.â€¦ 1.86   2.09 
## 15 numerâ€¦ PriceDiff 1       856      857   &quot;0.15â€¦ &quot;0.2â€¦ &quot;-0.â€¦ 0.23   0.64 
## 16 numerâ€¦ PriceMM   1       856      857   &quot;2.08â€¦ &quot;0.1â€¦ &quot; 1.â€¦ 2.09   2.29 
## 17 numerâ€¦ SalePricâ€¦ 1       856      857   &quot;1.81â€¦ &quot;0.1â€¦ &quot; 1.â€¦ 1.86   2.09 
## 18 numerâ€¦ SalePricâ€¦ 3       854      857   &quot;1.96â€¦ &quot;0.2â€¦ &quot; 1.â€¦ 2.09   2.29 
## # ... with 1 more variable: hist &lt;chr&gt;</code></pre>
</div>
<div class="section level1">
<h1>æ’å…¥æ•°æ®</h1>
<p>Caret æä¾›äº†ä¸€ä¸ªå¾ˆæ–¹ä¾¿çš„å‡½æ•° <code>preProcess()</code></p>
<ul>
<li>è®¾ç½® <code>method=knnImpute</code> ç”Ÿæˆä¸€ä¸ªæ¨¡å‹</li>
<li>ä½¿ç”¨ <code>predict()</code> å¯¹æ•°æ®è¿›è¡Œæ’å…¥</li>
</ul>
<pre class="r"><code># Create the knn imputation model on the training data
preProcess_missingdata_model &lt;- preProcess(trainData, method=&#39;knnImpute&#39;)
preProcess_missingdata_model</code></pre>
<pre><code>## Created from 828 samples and 18 variables
## 
## Pre-processing:
##   - centered (16)
##   - ignored (2)
##   - 5 nearest neighbor imputation (16)
##   - scaled (16)</code></pre>
<pre class="r"><code># Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData &lt;- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)</code></pre>
<pre><code>## [1] FALSE</code></pre>
<pre class="r"><code>save.image(file = &quot;../../data/craet_3-2.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[3.1 How to split the dataset into training and validation?]]></title>
    <link href="/2018/03/3-1-how-to-split-the-dataset-into-training-and-validation/"/>
    <id>/2018/03/3-1-how-to-split-the-dataset-into-training-and-validation/</id>
    <published>2018-03-23T00:00:00+00:00</published>
    <updated>2018-03-23T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>æ•°æ®å‡†å¤‡å¥½äº†ä¹‹åçš„ç¬¬ä¸€æ­¥å°±æ˜¯æ‹†åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ï¼Œä¸€èˆ¬æ˜¯ 8:2 çš„æ¯”ä¾‹ã€‚</p>
<p>ä¸ºä»€ä¹ˆæ‹†åˆ†æ•°æ®å‘¢ï¼Ÿ</p>
<p>å½“æˆ‘ä»¬åœ¨æ„å»ºä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ä¸Šæ—¶ï¼ŒçœŸæ­£çš„ç›®çš„æ˜¯ä¸ºäº†é¢„æµ‹çœŸæ˜¯ä¸–ç•Œçš„æ•°æ®ï¼Œè€Œæœºå™¨å­¦ä¹ æ¨¡å‹æ˜¯ä¾é ç®—æ³•å­¦ä¹ è®­ç»ƒæ•°æ®å­¦ä¹ Y ä¸ X çš„å…³ç³»ï¼Œè¿™ç§çš„å…³ç³»çš„å­¦ä¹ å¥½åçš„è¯„åˆ¤æ˜¯è¦ä¾é æ²¡æœ‰å‚ä¸å­¦ä¹ æ¨¡å‹çš„æ•°æ®ä¸é¢„æµ‹æ•°æ®ä¹‹é—´çš„å·®è·æ¥è¯„åˆ¤çš„ã€‚</p>
<pre class="r"><code># Load the caret package
library(caret)

# Import dataset
orange &lt;- read.csv(&#39;../../data/orange_juice_withmissing.csv&#39;)
# Create the training and test datasets
set.seed(100)

# Step 1: Get row numbers for the training data
trainRowNumbers &lt;- createDataPartition(orange$Purchase, p=0.8, list=FALSE)

# Step 2: Create the training  dataset
trainData &lt;- orange[trainRowNumbers,]

# Step 3: Create the test dataset
testData &lt;- orange[-trainRowNumbers,]

# Store X and Y for later use.
x = trainData[, 2:18]
y = trainData$Purchase</code></pre>
<p><code>createDataPartition</code>ï¼šè¾“å…¥ Y å’Œ P æ¯”ç‡ï¼ˆè®­ç»ƒæ•°æ®çš„æ¯”ç‡ï¼‰ è¾“å‡º è®­ç»ƒæ•°æ®çš„è¡Œç´¢å¼•ã€‚</p>
<div id="save-the-image-for-next-blog" class="section level1">
<h1>save the image for next blog</h1>
<pre class="r"><code>save.image(file = &quot;../../data/caret.Rdata&quot;)</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[2 machine learning: load dataset]]></title>
    <link href="/2018/03/machine-learning-2-load-dataset/"/>
    <id>/2018/03/machine-learning-2-load-dataset/</id>
    <published>2018-03-22T00:00:00+00:00</published>
    <updated>2018-03-22T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="load-the-package-and-dataset" class="section level1">
<h1>Load the package and dataset</h1>
<p>æˆ‘ä»¬å°†ä½¿ç”¨ ISLR åŒ…ä¸­çš„ <a href="&#39;https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv&#39;">Orange Juice Data</a>.</p>
<p>ç›®æ ‡ï¼š é¢„æµ‹é¡¾å®¢ä¼šè´­ä¹°å“ªä¸¤ç§ğŸŠæ±</p>
<p>æ•°æ®é›†ä¸æ˜¯å¾ˆå¤§ï¼Œæˆ‘ä»¬çš„é‡ç‚¹æ˜¯æ„å»ºæ¨¡å‹çš„è¿‡ç¨‹ï¼Œè€ŒéçœŸæ­£æ­å»ºä¸€ä¸ªæœ‰ç”¨çš„æ¨¡å‹ã€‚</p>
<p>ğŸ‘Œï¼Œletâ€™go.</p>
<pre class="r"><code># Load the caret package
library(caret)

# Import dataset
orange &lt;- read.csv(&#39;../../data/orange_juice_withmissing.csv&#39;)

# Structure of the dataframe
str(orange)</code></pre>
<pre><code>## &#39;data.frame&#39;:    1070 obs. of  18 variables:
##  $ Purchase      : Factor w/ 2 levels &quot;CH&quot;,&quot;MM&quot;: 1 1 1 2 1 1 1 1 1 1 ...
##  $ WeekofPurchase: int  237 239 245 227 228 230 232 234 235 238 ...
##  $ StoreID       : int  1 1 1 1 7 7 7 7 7 7 ...
##  $ PriceCH       : num  1.75 1.75 1.86 1.69 1.69 1.69 1.69 1.75 1.75 1.75 ...
##  $ PriceMM       : num  1.99 1.99 2.09 1.69 1.69 1.99 1.99 1.99 1.99 1.99 ...
##  $ DiscCH        : num  0 0 0.17 0 0 0 0 0 0 0 ...
##  $ DiscMM        : num  0 0.3 0 0 0 0 0.4 0.4 0.4 0.4 ...
##  $ SpecialCH     : int  0 0 0 0 0 0 1 1 0 0 ...
##  $ SpecialMM     : int  0 1 0 0 0 1 1 0 0 0 ...
##  $ LoyalCH       : num  0.5 0.6 0.68 0.4 0.957 ...
##  $ SalePriceMM   : num  1.99 1.69 2.09 1.69 1.69 1.99 1.59 1.59 1.59 1.59 ...
##  $ SalePriceCH   : num  1.75 1.75 1.69 1.69 1.69 1.69 1.69 1.75 1.75 1.75 ...
##  $ PriceDiff     : num  0.24 -0.06 0.4 0 0 0.3 -0.1 -0.16 -0.16 -0.16 ...
##  $ Store7        : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 2 2 2 2 2 2 ...
##  $ PctDiscMM     : num  0 0.151 0 0 0 ...
##  $ PctDiscCH     : num  0 0 0.0914 0 0 ...
##  $ ListPriceDiff : num  0.24 0.24 0.23 0 0 0.3 0.3 0.24 0.24 0.24 ...
##  $ STORE         : int  1 1 1 1 0 0 0 0 0 0 ...</code></pre>
<pre class="r"><code># See top 6 rows and 10 columns
head(orange[, 1:10])</code></pre>
<pre><code>##   Purchase WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH
## 1       CH            237       1    1.75    1.99   0.00    0.0         0
## 2       CH            239       1    1.75    1.99   0.00    0.3         0
## 3       CH            245       1    1.86    2.09   0.17    0.0         0
## 4       MM            227       1    1.69    1.69   0.00    0.0         0
## 5       CH            228       7    1.69    1.69   0.00    0.0         0
## 6       CH            230       7    1.69    1.99   0.00    0.0         0
##   SpecialMM  LoyalCH
## 1         0 0.500000
## 2         1 0.600000
## 3         0 0.680000
## 4         0 0.400000
## 5         0 0.956535
## 6         1 0.965228</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Machine Learning in R 1ï¼šintrodution]]></title>
    <link href="/2018/03/machine-learning-in-r-introdution/"/>
    <id>/2018/03/machine-learning-in-r-introdution/</id>
    <published>2018-03-21T00:00:00+00:00</published>
    <updated>2018-03-21T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Caret Packageæ˜¯ä¸€ä¸ªç”¨äºåœ¨Rä¸­æ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹çš„ç»¼åˆæ¡†æ¶ã€‚åœ¨æ­¤åšæ–‡ä¸­ï¼Œæˆ‘ä¼šè§£é‡Š caret åŒ…çš„å‡ ä¹æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼Œå¹¶å¼•å¯¼å®Œæˆæ„å»ºé¢„æµ‹æ¨¡å‹çš„åˆ†æ­¥è¿‡ç¨‹ã€‚</p>
<p>Caret æ˜¯ Classification And REgression Training çš„ç®€ç§°ã€‚</p>
<p>Caret åŒ…å¾ˆå¥½åœ°è®©æ‰€æœ‰ä¸æœºå™¨å­¦ä¹ æ¨¡å‹å¼€å‘ç›¸å…³çš„æ­¥éª¤é›†æˆåˆ°ä¸€ä¸ªç®€åŒ–çš„å·¥ä½œæµç¨‹ä¸­ï¼Œå‡ ä¹æ¯ä¸ªä¸»æµçš„MLç®—æ³•éƒ½å¯ä»¥åœ¨Rä¸­å®ç°ã€‚</p>
<p>ç”±äºRå…·æœ‰å¦‚æ­¤å¤šçš„æœºå™¨å­¦ä¹ ç®—æ³•å®ç°åŒ…ï¼Œå› æ­¤é€‰åˆ™åœ¨å“ªä¸ªåŒ…ä¸­å®ç°æŸç§ç®—æ³•æ˜¯éå¸¸å¤´ç–¼çš„é—®é¢˜ã€‚å¤šæ•°æ—¶å€™ï¼Œå®ç°ç®—æ³•çš„è¯­æ³•å’Œæ–¹æ³•åœ¨ä¸åŒåŒ…ä¸­æœ‰æ‰€ä¸åŒã€‚ ç»“åˆæ•°æ®é¢„å¤„ç†ï¼ŒæŸ¥é˜…è¶…å‚æ•°çš„å¸®åŠ©é¡µé¢ï¼ˆå®šä¹‰ç®—æ³•å¦‚ä½•å­¦ä¹ çš„å‚æ•°ï¼‰å¹¶åŠªåŠ›å¯»æ‰¾æœ€ä½³æ¨¡å‹ï¼Œå¯ä»¥ä½¿æ„å»ºé¢„æµ‹æ¨¡å‹æˆä¸ºä¸€é¡¹ç›¸å…³ä»»åŠ¡ã€‚</p>
<p>åœ¨æœ¬æ•™ç¨‹åé¢çš„éƒ¨åˆ†ï¼Œæˆ‘å°†ä»‹ç»å¦‚ä½•æŸ¥çœ‹æ‰€æœ‰caret æ”¯æŒçš„MLç®—æ³•ï¼ˆè¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„åˆ—è¡¨ï¼‰ä»¥åŠå¯ä»¥è°ƒæ•´å“ªäº›è¶…å‚æ•°ã€‚</p>
<p>æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ä¼šæ­¢æ­¥äº caret åŒ…ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•å·§å¦™åœ°é›†æˆæ¥è‡ªå¤šä¸ªæœ€ä½³æ¨¡å‹çš„é¢„æµ‹ï¼Œå¹¶å¯èƒ½ä½¿ç”¨ <code>caretEnsemble</code> æ¥äº§ç”Ÿæ›´å¥½çš„é¢„æµ‹ã€‚</p>
<p>è¿™ä¸ªæ•™ç¨‹æ€»å…±åŒ…æ‹¬5éƒ¨åˆ†ï¼Œåˆ†åˆ«æ˜¯ï¼š 1. æ•°æ®å‡†å¤‡å’Œæ¸…ç† 2. å¯è§†åŒ–é‡è¦å˜é‡ 3. ç‰¹å¾é€‰æ‹© 4. è®­ç»ƒæ¨¡å‹å’Œè°ƒèŠ‚æ¨¡å‹ 5. é¢„æµ‹</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[HOW TO USE THE NCBI&#39;S NEW API KEYS]]></title>
    <link href="/2018/03/how-to-use-the-ncbis-new-api-keys/"/>
    <id>/2018/03/how-to-use-the-ncbis-new-api-keys/</id>
    <published>2018-03-19T00:00:00+00:00</published>
    <updated>2018-03-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><a href="https://www.ncbi.nlm.nih.gov/">The NCBI</a> is one of the most important sources of biological data. The centre provides access to information on 28 million scholarly articles through PubMed and 250 million DNA sequences through GenBank. More importantly, records in the [50 public databases] (<a href="https://www.ncbi.nlm.nih.gov/guide/all/#databases" class="uri">https://www.ncbi.nlm.nih.gov/guide/all/#databases</a>) maintained by the NCBI are strongly cross-referenced. As a result, it is possible to pinpoint searches using almost 2 million taxonomic names or a <a href="https://www.nlm.nih.gov/mesh/">controlled vocabulary with 270,000 terms</a>.</p>
<p><strong>Rentrez has been designed to make it easy to search for and download NCBI records and download them from within an R session.</strong></p>
<p>I though it might be fun to use this post to find out where papers describing R packages are published these days</p>
<p>Here we use the <code>entrez_search</code> and <code>entrez_summary</code> functions to get some information on all of the papers published in 2017 with the term â€˜R packageâ€™ in their title:</p>
<pre class="r"><code>if (!require(&quot;rentrez&quot;)) install.packages(&quot;rentrez&quot;)
library(rentrez)

pkg_search &lt;- entrez_search(db=&quot;pubmed&quot;, 
                            term=&quot;(R Package[TITLE]) AND (2018[PDAT])&quot;, 
                            use_history=TRUE)
pkg_summs &lt;- entrez_summary(db=&quot;pubmed&quot;, web_history=pkg_search$web_history)
pkg_summs</code></pre>
<pre><code>## List of  31 esummary records. First record:
## 
##  $`29554216`
## esummary result with 42 items:
##  [1] uid               pubdate           epubdate         
##  [4] source            authors           lastauthor       
##  [7] title             sorttitle         volume           
## [10] issue             pages             lang             
## [13] nlmuniqueid       issn              essn             
## [16] pubtype           recordstatus      pubstatus        
## [19] articleids        history           references       
## [22] attributes        pmcrefcount       fulljournalname  
## [25] elocationid       doctype           srccontriblist   
## [28] booktitle         medium            edition          
## [31] publisherlocation publishername     srcdate          
## [34] reportnumber      availablefromurl  locationlabel    
## [37] doccontriblist    docdate           bookname         
## [40] chapter           sortpubdate       sortfirstauthor</code></pre>
<p>we are interested in the journals in which these papers appear. We can use the helper functionÂ <code>extract_from_esummary</code>Â to isolate the <em>source</em> of each paper, then useÂ <code>table</code>Â to count up the frequency of each journal.</p>
<pre class="r"><code>library(ggplot2)
library(ggpomological)
#scales::show_col(ggpomological:::pomological_palette)

journals &lt;- extract_from_esummary(pkg_summs, &quot;source&quot;)
journal_freq &lt;- as.data.frame(table(journals, dnn=&quot;journal&quot;), responseName=&quot;n.papers&quot;)
pkg_journal &lt;- ggplot(journal_freq, aes(reorder(journal, n.papers), n.papers)) + 
    geom_point(size=2) + 
    coord_flip() + 
    scale_y_continuous(&quot;Number of papers&quot;) +
    scale_x_discrete(&quot;Journal&quot;) +
    theme_bw() +
    ggtitle(&quot;Venues for papers describing R Packages in 2018&quot;)

pkg_journal + ggpomological::theme_pomological()</code></pre>
<p><img src="/post/2018-03-19-how_to_use_NCBI_API_keys_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>So, it looks like <em>Bioinformatics</em>, <em>Plos One</em> and <em>Comput Methods Progams Biomed</em> Resources are popular destinations for papers describing R packages, but these appear in journals all the way across the biological sciences.</p>
<p>The NCBI now gives users the opportunity toÂ <a href="https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/">register for an access key</a>Â that will allow them to make up to 10 requests per second (non-registered users are limited to 3 requests per second per IP address).For one-off cases, this is as simple as adding the api_key argument to a given function call.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[A function for ggplot2: input strings for aes()]]></title>
    <link href="/2018/03/a-function-for-ggplot2-input-strings-for-aes/"/>
    <id>/2018/03/a-function-for-ggplot2-input-strings-for-aes/</id>
    <published>2018-03-18T00:00:00+00:00</published>
    <updated>2018-03-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>åœ¨æˆ‘å†™æ–‡ç« ç”»å›¾æ—¶ç»å¸¸é‡åˆ°çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼šggplot2 åæ ‡è½´çš„è¾“å…¥ä¸æ”¯æŒè¾“å…¥æ•°æ®æ¡†çš„å˜é‡åï¼Œé€šå¸¸ä¼šæŠ¥é”™æ‰¾ä¸åˆ°å¯¹è±¡</p>
<div class="section level1">
<h1>ğŸŒ°ï¼šé—®é¢˜æè¿°</h1>
<p>data: <a href="https://github.com/fivethirtyeight/data/tree/master/early-senate-polls">early senate poll</a></p>
<pre class="r"><code>library(tidyverse) # general tasks
library(broom) # tidy model output
library(ggthemes) # style the plots

poll_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/early-senate-polls/early-senate-polls.csv&quot;)

glimpse(poll_data)</code></pre>
<pre><code>## Observations: 107
## Variables: 4
## $ year                  &lt;int&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006...
## $ election_result       &lt;int&gt; -39, -10, -9, -16, 40, 10, -2, -41, -31,...
## $ presidential_approval &lt;int&gt; 46, 33, 32, 33, 53, 44, 37, 39, 42, 33, ...
## $ poll_average          &lt;int&gt; -28, -10, -1, -15, 39, 14, 2, -22, -27, ...</code></pre>
<p>background: <strong>there is a strong correlation between polling numbers and the ultimate result of an election</strong></p>
<div id="-" class="section level2">
<h2>æ„å»ºæ¨¡å‹ï¼š çº¿æ€§æ¨¡å‹</h2>
<pre class="r"><code>poll_lm &lt;- lm(election_result ~ poll_average, data = poll_data)

summary(poll_lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = election_result ~ poll_average, data = poll_data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -29.4281  -5.0197   0.5601   6.1364  17.9357 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -0.89110    0.76969  -1.158     0.25    
## poll_average  1.04460    0.03777  27.659   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.93 on 105 degrees of freedom
## Multiple R-squared:  0.8793, Adjusted R-squared:  0.8782 
## F-statistic:   765 on 1 and 105 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="section level2">
<h2>å†™ä¸ªå‡½æ•°ç”»å‡ºå› å˜é‡å’Œè‡ªå˜é‡çš„å…³ç³»</h2>
<p>ç»“æœå‡ºç°äº†ä¸€ä¸ªä»¤æˆ‘è´¹è§£çš„æŠ¥é”™</p>
<blockquote>
<p><strong>Error in FUN(X[[i]], â€¦) : object â€˜poll_averageâ€™ not found</strong></p>
</blockquote>
<p>æˆ‘ä¸æ–­åœ°æ£€æŸ¥æˆ‘çš„æ‹¼å†™ï¼Œç›´åˆ°æˆ‘å¼€å§‹æ€€ç–‘äººç”Ÿ</p>
</div>
</div>
<div id="define-aesthetic-mappings-programatically" class="section level1">
<h1>è§£å†³åŠæ³•ï¼š<a href="http://ggplot2.tidyverse.org/reference/aes_.html"><strong>Define aesthetic mappings programatically</strong></a></h1>
<pre class="r"><code>plot_model &lt;- function(mod, explanatory, response, .fitted = &quot;.fitted&quot;) {
  augment(mod) %&gt;%
  ggplot() +
    geom_point(aes_string(x = explanatory, y = response), color = &quot;#2CA58D&quot;) +
    geom_line(aes_string(x = explanatory, y = .fitted), color = &quot;#033F63&quot;) +
    theme_solarized() +
    theme(axis.title = element_text()) +
    labs(x = &quot;Poll average&quot;, y = &quot;Election results&quot;)
}

plot_model(poll_lm, &quot;poll_average&quot;, &quot;election_result&quot;)</code></pre>
<p><img src="/post/2018-03-18-a-function-for-ggplot2-input-strings-for-aes_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Hello world]]></title>
    <link href="/2018/03/hello-world/"/>
    <id>/2018/03/hello-world/</id>
    <published>2018-03-15T00:00:00+00:00</published>
    <updated>2018-03-15T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Just say hello world!</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Conferences, webinars, podcasts and the likes]]></title>
    <link href="/page/conferences_podcasts_webinars/"/>
    <id>/page/conferences_podcasts_webinars/</id>
    <published>2018-02-01T16:06:06+02:00</published>
    <updated>2018-02-01T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p>Here, you can find a list of all the talks I gave at conferences, webinars, podcasts, workshops, and all the other places you can and could hear me talk. :-)</p>

<h2 id="workshops-i-am-giving">Workshops I am giving</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/11/deep_learning_keras_tensorflow/">Workshop on Deep Learning with Keras and TensorFlow in R</a></li>
</ul>

<blockquote>
<p>I offer a workshop on deep learning with Keras and TensorFlow using R.
Date and place depend on who and how many people are interested, so please contact me either directly or via the workshop page: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/</a> (the description is in German but I also offer to give the workshop in English).</p>
</blockquote>

<h2 id="upcoming-talks-webinars-podcasts-etc">Upcoming talks, webinars, podcasts, etc.</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/m3_2018/">Announcing my talk about explainability of machine learning models at Minds Mastering Machines Conference</a></li>
</ul>

<blockquote>
<p>On Wednesday, April 25th 2018 I am going to talk about explainability of machine learning models at the Minds Mastering Machines conference in Cologne.</p>
</blockquote>

<ul>
<li><a href="JAX 2018 talk announcement: Deep Learning - a Primer">JAX 2018 talk announcement: Deep Learning - a Primer</a></li>
</ul>

<blockquote>
<p>Deep Learning is one of the &ldquo;hot&rdquo; topics in the AI area â€“ a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become &ldquo;Software 2.0&rdquo;, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>

<h2 id="past-talks-webinars-podcasts-etc">Past talks, webinars, podcasts, etc.</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/herr_mies_wills_wissen/">I talk about machine learning with Daniel Mies (Podcast in German, though)</a></li>
</ul>

<blockquote>
<p>In January 2018 I was interviewed for a tech podcast where I talked about machine learning, neural nets, why I love R and Rstudio and how I became a Data Scientist.</p>
</blockquote>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/">Explaining Predictions of Machine Learning Models with LIME - MÃ¼nster Data Science Meetup</a></li>
</ul>

<blockquote>
<p>In December 2017 I talked about Explaining Predictions of Machine Learning Models with LIME at the MÃ¼nster Data Science Meetup.</p>
</blockquote>

<ul>
<li><a href="https://shiring.github.io/blogging/2017/09/20/webinar_biology_to_data_science">From Biology to Industry. A Bloggerâ€™s Journey to Data Science</a></li>
</ul>

<blockquote>
<p>In September 2017 I gave a webinar for the Applied Epidemiology Didactic of the University of Wisconsin - Madison titled â€œFrom Biology to Industry. A Bloggerâ€™s Journey to Data Science.â€
I talked about how blogging about R and Data Science helped me become a Data Scientist. I also gave a short introduction to Machine Learning, Big Data and Neural Networks.</p>
</blockquote>

<ul>
<li><a href="https://shiring.github.io/machine_learning/2017/03/31/webinar_code">Building meaningful machine learning models for disease prediction</a></li>
</ul>

<blockquote>
<p>In March 2017 I gave a webinar for the ISDS R Group about my work on building machine-learning models to predict the course of different diseases. I went over building a model, evaluating its performance, and answering or addressing different disease related questions using machine learning. My talk covered the theory of machine learning as it is applied using R.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Welcome to my page!]]></title>
    <link href="/page/about/"/>
    <id>/page/about/</id>
    <published>2017-09-12T16:06:06+02:00</published>
    <updated>2017-09-12T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p><img src="/img/Bewerbungsfoto_klein.jpg" alt="" /></p>

<p>I&rsquo;m Shirin, a biologist turned bioinformatician turned data scientist.</p>

<p>I&rsquo;m especially interested in machine learning and data visualization. While I am using R most every day at work, I wanted to have an incentive to regularly explore other types of analyses and other types of data that I don&rsquo;t normally work with. I have also very often benefited from other people&rsquo;s published code in that it gave me ideas for my own work; and I hope that sharing my own analyses will inspire others as much as I often am by what can be be done with data.  It&rsquo;s amazing to me what can be learned from analyzing and visualizing data!</p>

<p>My tool of choice for data analysis so far has been R. I also organize the <a href="https://shiring.github.io/r_users_group/2017/05/20/muenster_r_user_group">MÃ¼nsteR R-users group on meetup.com</a>.</p>

<p><img src="http://res.cloudinary.com/shiring/image/upload/v1511852499/my_story_wml3zm.png" alt="My journey to Data Science" /></p>

<p>I love dancing and used to do competitive ballroom and latin dancing. Even though I don&rsquo;t have time for that anymore, I still enjoy teaching &ldquo;social dances&rdquo; once a week with the Hochschulsport (university sports courses).</p>

<p>I created the R package <a href="https://github.com/ShirinG/exprAnalysis">exprAnalysis</a>, designed to streamline my RNA-seq data analysis pipeline. It is available via Github. Instructions for installation and usage can be found <a href="https://shiring.github.io/rna-seq/microarray/2016/09/28/exprAnalysis">here</a>.</p>

<p>This blog will showcase some of the analyses I have been doing with different data sets (all freely available). I will also host teaching materials for students to access in conjunction with R courses I am giving.</p>

<hr />

<h2 id="contact-me">Contact me:</h2>

<ul>
<li><a href="https://www.codecentric.de/team/shirin-glander/">Codecentric AG</a></li>
<li><a href="mailto:shirin.glander@gmail.com">Email</a></li>
<li><a href="http://www.xing.com/profile/Shirin_Glander">Xing</a></li>
<li><a href="http://de.linkedin.com/in/shirin-glander-01120881">Linkedin</a></li>
<li><a href="http://twitter.com/ShirinGlander">Twitter</a></li>
</ul>

<hr />

<p>Also check out <a href="http://www.R-bloggers.com">R-bloggers</a> for lots of cool R stuff!</p>
]]></content>
  </entry>
</feed>